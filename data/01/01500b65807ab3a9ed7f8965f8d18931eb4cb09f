<p>Artificial intelligence is an amazing technology, but also one that seems to pose threats to human relations, to aspects of human flourishing, and to education. <span id="more-55641"></span></p>
<p>It&#8217;s not going to go away, but that doesn&#8217;t mean there&#8217;s nothing we can do about it. How AI affects us is in part a function of its infrastructure, and, if Eli Alshanetsky is correct in the following guest post, there&#8217;s a way to build that infrastructure that makes it less of a danger to trust and the institutions and practices that depend on trust.</p>
<p>In some contexts, individuals, like professors, are in a position to implement the &#8220;protocols&#8221; needed to do this, but generally they will need to be defined, adopted, and enforced by institutions, professions, and communities. The means of preserving trust in a world of AI may thus require a political movement calling for their implementation.</p>
<p><a href="https://liberalarts.temple.edu/directory/eli-alshanetsky" target="_blank" rel="noopener">Eli Alshanetsky</a> is assistant professor of philosophy at Temple University, where he heads up the <a href="https://sites.temple.edu/cilab/" target="_blank" rel="noopener">Cognitive Integrity Lab</a>.</p>
<hr />
<p><img loading="lazy" decoding="async" class="wp-image-55642 aligncenter" src="https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-1024x688.png" alt="" width="744" height="500" srcset="https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-1024x688.png 1024w, https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-300x201.png 300w, https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-768x516.png 768w, https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-1536x1032.png 1536w, https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-2048x1376.png 2048w, https://dailynous.com/wp-content/uploads/2025/11/handshake-links-2-400x269.png 400w" sizes="auto, (max-width: 744px) 100vw, 744px" /></p>
<h2 style="text-align: center;">Beyond Authorship Vibes: Preserving Judgment and Trust in the Age of AI<strong><br />
</strong><em>Why classrooms may be the proving ground for the next layer of infrastructure<br />
<strong>by Eli Alshanetsky</strong></em></h2>
<p>Imagine I started this post by saying, “I used AI to help write it&#8212;just for brainstorming and editing. Trust me, I was in charge. The thinking is all mine.”</p>
<p>Would you believe me?</p>
<p>You might use AI routinely&#8212;I do too&#8212;and we both know that. So if you comment below, I’ll face the same question: is this what you really think, or did you just go with what the model suggested?</p>
<p>The problem isn’t trust in AI. It’s trust between people when AI is in the loop. Even when no one’s lying, the line between our own reasoning and the model’s influence can blur.</p>
<p>That uncertainty now shadows exchanges where trust depends on knowing that human judgment guided the work. In law, medicine, and journalism, AI already helps with analysis, drafting, and diagnosis. Yet even when AI’s role is small, we often can’t be sure whether the professional drove the process.</p>
<p>The reason this matters isn’t some ineffable “human touch” (most of us in the U.S. don’t expect that <a href="https://theconversation.com/why-does-your-doctor-seem-so-rushed-and-dismissive-that-bedside-manner-may-be-the-result-of-the-health-care-system-261335" target="_blank" rel="noopener">from doctors anyway</a>). When outcomes aren’t directly inspectable&#8212;because they’re ambiguous, slow to reveal, or costly to verify&#8212;trust has to rest on how the work was done. A diagnosis can sound plausible but miss a crucial test; a legal strategy can look airtight yet conceal trade-offs only an experienced advocate would notice.</p>
<p>If something goes wrong, we’ll ask: “Why did you think this was right for <em>me</em>?”</p>
<p>“Because the model said so” isn’t an answer. It’s abandonment.</p>
<p>The natural follow-up is: “Then what are <em>you</em> for? Why did I put my body, my child’s life, my livelihood in your hands?”</p>
<p>What erodes isn’t just the authority of individual professionals, but the broader sense that institutions can still answer for what they certify.</p>
<h3>The Problem in Education</h3>
<p>The problem hits education hard because product and process are tied together in a way they aren’t in most other fields. Process isn’t just a way to validate the product; it’s part of the product’s value. An essay can be rough around the edges and still count as real progress if the student grappled with an idea. When the work no longer reveals the student’s thinking, the teacher’s feedback has nowhere to land. Learning breaks down, and the degree stops certifying anything we can rely on.</p>
<p>As models mimic the cues of reasoning, it’s tempting to fall back on crude heuristics: fluent means AI, clumsy means authentic. Across the professions, “authorship-vibes” are taking hold&#8212;stylized imperfections, idiosyncratic phrasing, and other performative cues meant to signal humanness. That advantages confident insiders and penalizes anxious or non-native writers, who risk being read as “AI-ish” no matter what they write.</p>
<p>Vibes aren’t bad; they’re part of how uptake works. In the classroom, they should support a learning environment where reasons are recognized and accessible. But when vibes displace shared standards, trust dissolves. Students feel judged on superficial signals; teachers are left guessing. And because vibes offer no reasons&#8212;you can’t argue with them or appeal them&#8212;institutions lose legitimacy.</p>
<p>In two recent pieces, I’ve proposed a way to keep assessment tied to students’ thinking, now that AI muddies the usual cues. In the <em><a href="https://link.springer.com/epdf/10.1007/s12115-025-01149-x?sharing_token=YkjdMt0CkiQif-U6HtPdife4RwlQNchNByi7wbcMAY54f3GCP9hNXeVPckmkP1XTfHsvI1t8FTAUY7P_SvkbZLRFYB8tc81Bnrk0YaYhQ4CORE99A6x7lrBnBlVOk_1mHURMbyYBwNPxrR1_jRzuFfuDU78VyH5fpwbCGcF8tbE1KRxOO8ElbeaPRVH_pHDA" target="_blank" rel="noopener">Society</a> </em>article, I argue for a shared, open framework that makes the conditions of AI use transparent without surveillance or forcing changes to how we teach; the <em><a href="https://theconversation.com/where-does-human-thinking-end-and-ai-begin-an-ai-authorship-protocol-aims-to-show-the-difference-266132" target="_blank" rel="noopener">Conversation</a> </em>piece gives a shorter, public-facing sketch.</p>
<p>It’s a bottom-up approach to governing AI: rather than trying to stop progress at the level of capability development, we secure how AI is allowed to enter and operate within human practices. If we can solve the problem in the classroom, education could offer a model for other fields where trust and accountability depend on process.</p>
<h3>The Interaction Layer</h3>
<p>When the surface features of reasoning become easy to fake, we need ways to make the real reasoning show up. Philosophers are no strangers to the task. When sophists mastered the performative aspects of argument&#8212;fluency, form, the tone of credibility&#8212;to make the weaker argument look stronger, philosophers didn’t respond with counter-vibes; they articulated frameworks like logic. A sophist could still slip in a false premise, but the breach became inspectable.</p>
<p>Today’s problem is recognizably similar, though the mechanism is very different. Like the sophists, language models can reproduce the surface features of good writing, but across countless styles, instantly, and at scale. And unlike in antiquity, the product may be perfectly good.</p>
<p>To respond, we need ways to structure AI-mediated work so that human and AI contributions remain distinguishable and adjustable. That requires a protocol. A protocol isn’t a tool or a piece of software but a shared set of rules that makes a kind of communication possible. Your browser can talk to a website only because both sides follow the same rules: IP, the Internet Protocol, specifies how data is addressed so it can be routed; HTTPS ensures the exchange hasn’t been tampered with. If either side breaks the rules, the communication won’t go through.</p>
<p>An interaction protocol for AI applies this same idea to any AI-mediated task&#8212;writing, translation, analysis, image or video generation, anything that now passes through a model. It defines the shared rules under which an AI-assisted exchange takes place. In my recent work I’ve been calling this architecture the interaction layer. We don’t have it yet. Without one, AI use is a bit like the early web: you see the output, but you can’t see what was done along the way.</p>
<h3>Implementing the Interaction Layer in Education</h3>
<p>If your syllabus includes an AI policy, you already have an informal interaction protocol: “Use AI only for X,” “If you do, disclose it.” The interaction layer lets you build those expectations into the submission process. Instead of trying to detect AI use afterward, the permitted modes are set upfront and become the only ones the system will accept.</p>
<p>When you post an assignment, you choose which modes students may use. Students select among them, or you can require one mode for a given task or stage. Once the modes are set, you don’t review logs or monitor draft histories. If a student tries to submit work in a mode you haven’t allowed, any system implementing the interaction layer will block it before it ever reaches you, much like a browser refuses a connection that doesn’t follow HTTPS.</p>
<p>There are many possible modes, but two sit at the poles: one with no AI at all, and one that allows full AI support paired with an authorship check.</p>
<p><strong>AI-Free</strong>. Students write in a simple window that blocks external model calls and prevents pasting text in or out. They can save and return later, but every sentence must be generated by them.</p>
<p><strong>Authorship Check. </strong>This is the main mode we’re piloting in our lab. Students may use AI freely while drafting, but before submitting they complete a short (5–10 minute), low-pressure interaction with an AI assistant. The assistant poses a handful of micro-revision prompts drawn from the student’s own draft&#8212;clarifying a thesis claim, varying an example, adjusting a transition, choosing between two formulations.</p>
<p>During the interaction, the system checks whether the student can work with their own text in real time. It doesn’t analyze style, try to “detect AI,” or test content knowledge. It verifies something much narrower: that the edits fit, the revisions hold together, and the student can actually inhabit and manipulate the prose they’re submitting rather than hand in a one-shot model output they never cognitively touched. If the interaction goes smoothly, the submission goes through; if not, the student tries again.</p>
<p>You never see the interaction&#8212;no keystroke logs, no screen recordings. The system doesn’t grade. Its role isn’t to replace teachers, but to keep your feedback connected to actual student work rather than to AI-generated text the student hasn’t processed. You can even return your own feedback under a mode, so students know whether&#8212;and how&#8212;you used AI as well.</p>
<h3>Can Students Still Game the System?</h3>
<p>Yes, but that’s not the point. The aim isn’t to make cheating impossible; it’s to recover something like the baseline of trust we had before AI.</p>
<p>To “game” Authorship Check, a student would have to feed their draft and the adaptive prompts into another model in real time, while having the live conversation with ours, and keep the edits coherent as the text shifts. For most students, that’s more work than just doing the assignment. If you want to close that loophole, you could run the check in class, though in practice I doubt it will be necessary.</p>
<p>In AI-Free mode, a student could still take screenshots or have a model whisper suggestions on another device while they type manually. That’s easier than gaming Authorship Check, but it’s still a step up from relying on a syllabus promise you can’t verify.</p>
<p>Even the basic modes give students a small but real power: a way to demonstrate authorship under a shared standard, without being monitored and without having to perform authenticity through vibes. Authorship Check is not only a verification step; it’s also a metacognitive one, helping students develop the habits of reflection needed when working with AI. If it works, it should help students (and me!) notice when we’re thinking and when we’re outsourcing, catching the moments when a model has slipped something in that we don’t actually stand behind.</p>
<p>These two modes are the endpoints. In between are AI-assisted modes where you set the boundaries: perhaps AI may help brainstorm but not draft, or draft but not revise; a Socratic mode may prompt reflection without generating prose. The system comes with defaults, and new ones can be shared across the teaching community. If you don’t like ours, you can adjust the settings, design your own, or even build your own platform that implements the same interaction layer. The layer adds no surveillance and almost no extra work. It’s there when you need it and invisible when you don’t.</p>
<p>Modes aren’t bans; they define assignments. Just as the rules of a game enable new forms of play, they give teachers and students handles on AI’s raw capacity&#8212;a way to structure tasks so that students stay in charge of their own thinking while navigating a cognitive environment that is, for a growing range of tasks, more powerful than they are.</p>
<h3>Why Not Go Back to Blue Books and Oral Exams?</h3>
<p>Blue-books and oral exams mostly reward performance under pressure rather than sustained reflection and raise accessibility concerns. They also can’t insulate themselves from the ways students’ writing habits shift outside of class. When smartphones and social media rewired how people read and focus, classroom expectations eventually shifted with them.</p>
<p>Speaking personally, I know I wouldn’t have gone into philosophy if courses had relied on them rather than essays. But even if those formats worked perfectly, they still wouldn’t be enough, because they don’t address the more serious problem AI forces on us. So let me say as plainly as I can what I take the real issue to be.</p>
<p>The immediate problem AI puts before us isn’t a superintelligence going rogue. It’s a communication problem and a political one.</p>
<p>AI is rapidly becoming a medium of thought and communication, a channel that sits between us. It refracts our thinking into forms that resemble a conversation. But we don’t communicate <em>with</em> AI; we communicate <em>through</em> it. It’s as if someone handed us a language, made the language intelligent, and unleashed it on us. The medium can respond, but we don’t control it. And the medium is owned.</p>
<p>We’ve always worried about people manipulating cues within a shared medium; that was the problem with sophists. But the more serious dangers in modern life come from distortions of the medium itself&#8212;the channels that decide what circulates, what becomes visible, and what forms of reasoning can register at all. Propaganda systems and algorithmic feeds that reward outrage don’t just mislead; they preset what shows up as possible before reasoning even begins. If sophists are like someone holding steam under a smoke detector, these are like rewiring the alarm.</p>
<p>AI is already becoming the medium through which teaching, writing, peer review, and professional judgment flow. The central question is no longer “Did someone use AI?” but “Who governs the channel we all rely on?” Blue books can’t answer that. Protocols can.</p>
<h3>Governance, Not Gadgets</h3>
<p>The interaction layer is not a product or a pedagogy but a governance architecture&#8212;a shared set of rules that lets educators and professions define the terms under which AI can enter their practices. In that sense, it plays a role for AI similar to the one open internet protocols play for digital communication: not gadgets or tools, but the underlying rules that make mediated exchanges reliable and governable. Without such an architecture, the defaults will be set for us by vendors, risk-management teams, or whatever authorship-vibes happen to be in fashion.</p>
<p>The way forward is not to ask tech companies to build “<a href="https://openai.com/index/chatgpt-study-mode/" target="_blank" rel="noopener">learning modes</a>” for us. If the rules for when an AI-assisted process counts as legitimate live inside a private product, institutions can’t ground their authority in them. It’s like asking the world to trust a bank transfer when only the sending bank has the record. Even banks, which once relied entirely on their own ledgers, eventually moved to shared networks because only common rails can sustain trust.</p>
<p>Vendors should provide powerful, safe models. But the standards for when AI assistance is acceptable in academic and professional work must live in the interaction layer, where public institutions and professional communities can define and enforce them.</p>
<p>And if those standards are meant to protect our agency, they must rest on a principled understanding of what it is for thinking to remain our own when the medium we think through is itself intelligent. That means articulating what cognitive agency requires, developing frameworks that preserve it in AI-mediated work, and ensuring those frameworks become the standards our institutions adopt. The standards we build now will shape thinking long after today’s tools are gone. If philosophers don’t help clarify them, someone else will. Our discipline is unusually well placed to lead that work.</p><p>The post <a href="https://dailynous.com/2025/11/18/beyond-authorship-vibes-preserving-judgment-and-trust-in-the-age-of-ai-guest-post/">Beyond Authorship Vibes: Preserving Judgment and Trust in the Age of AI (guest post)</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>