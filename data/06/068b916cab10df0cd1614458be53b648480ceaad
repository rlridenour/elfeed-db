A newly published Apple Machine Learning Research study has challenged the prevailing narrative around AI "reasoning" large-language models like OpenAI's o1 and Claude's thinking variants, revealing fundamental limitations that suggest these systems aren't truly reasoning at all. <br/>
<br/>
<img src="https://images.macrumors.com/article-new/2024/12/ml-research-apple.jpg" alt="" width="1920" height="1080" class="aligncenter size-full wp-image-982819" /><br/>
<a href="https://machinelearning.apple.com/research/illusion-of-thinking">For the study</a>, rather than using standard math benchmarks that are prone to data contamination, Apple researchers designed controllable puzzle environments including Tower of Hanoi and River Crossing. This allowed  a precise analysis of both the final answers and the internal reasoning traces across varying complexity levels, according to the researchers.<br/>
<br/>
The results are striking, to say the least. All tested reasoning models – including o3-mini, DeepSeek-R1, and Claude 3.7 Sonnet – experienced complete accuracy collapse beyond certain complexity thresholds, and dropped to zero success rates despite having adequate computational resources. Counterintuitively, the models actually reduce their thinking effort as problems become more complex, suggesting fundamental scaling limitations rather than resource constraints.<br/>
<br/>
Perhaps most damning, even when researchers provided complete solution algorithms, the models still failed at the same complexity points. Researchers say this indicates the limitation isn't in problem-solving strategy, but in basic logical step execution. <br/>
<br/>
Models also showed puzzling inconsistencies – succeeding on problems requiring 100+ moves while failing on simpler puzzles needing only 11 moves.<br/>
<br/>
The research highlights three distinct performance regimes: standard models surprisingly outperform reasoning models at low complexity, reasoning models show advantages at medium complexity, and both approaches fail completely at high complexity. The researchers' analysis of reasoning traces showed inefficient "overthinking" patterns, where models found correct solutions early but wasted computational budget exploring incorrect alternatives.<br/>
<br/>
The take-home of Apple's findings is that current "reasoning" models rely on sophisticated pattern matching rather than genuine reasoning capabilities. It suggests that LLMs don't scale reasoning like humans do, overthinking easy problems and thinking less for harder ones.<br/>
<br/>
The timing of the publication is notable, having emerged just days before WWDC 2025, where Apple is expected to limit its focus on AI in favor of new software designs and features, according to <em><a href="https://www.macrumors.com/2025/03/10/ios-19-macos-16-major-design-update/">Bloomberg</a></em>.<div class="linkback">Tag: <a href="https://www.macrumors.com/guide/apple-research/">Apple Research</a></div><br/>This article, &quot;<a href="https://www.macrumors.com/2025/06/09/apple-research-questions-ai-reasoning-models/">Apple Research Questions AI Reasoning Models Just Days Before WWDC</a>&quot; first appeared on <a href="https://www.macrumors.com">MacRumors.com</a><br/><br/><a href="https://forums.macrumors.com/threads/apple-research-questions-ai-reasoning-models-just-days-before-wwdc.2458415/">Discuss this article</a> in our forums<br/><br/>