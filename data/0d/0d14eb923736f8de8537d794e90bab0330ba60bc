<p id="p1">OpenAI found itself in the middle of another controversy earlier this week, only this time it wasn&rsquo;t about <a href="https://www.nytimes.com/2025/04/24/business/media/ziff-davis-openai-lawsuit.html" rel="noopener noreferrer">publishers</a> or <a href="https://www.theverge.com/2023/5/5/23709833/openai-chatgpt-gdpr-ai-regulation-europe-eu-italy" rel="noopener noreferrer">regulation</a>, but about its core product &ndash; ChatGPT. Specifically, after rolling out an update to the default 4o model with <a href="https://x.com/sama/status/1915902652703248679" rel="noopener noreferrer">improved</a> personality, users started noticing that ChatGPT was adopting highly sycophantic behavior: it weirdly agreed with users on all kinds of prompts, even about topics that would typically warrant some justified pushback from a digital assistant. (Simon Willison and Ethan Mollick have a good <a href="https://simonwillison.net/2025/Apr/30/sycophancy-in-gpt-4o/" rel="noopener noreferrer">roundup</a> of the <a href="https://x.com/emollick/status/1916504460253135140" rel="noopener noreferrer">examples</a> as well as the <a href="https://x.com/simonw/status/1917021036350214589" rel="noopener noreferrer">change</a> in the system prompt that may have caused this.) OpenAI <a href="https://openai.com/index/sycophancy-in-gpt-4o/" rel="noopener noreferrer">had to roll back the update and explain what happened</a> on the company&rsquo;s blog:</p>
<blockquote id="blockquote2"><p>
  We have rolled back last week&rsquo;s GPT&#8209;4o update in ChatGPT so people are now using an earlier version with more balanced behavior. The update we removed was overly flattering or agreeable&mdash;often described as sycophantic.</p>
<p>  We are actively testing new fixes to address the issue. We&rsquo;re revising how we collect and incorporate feedback to heavily weight long-term user satisfaction and we&rsquo;re introducing more personalization features, giving users greater control over how ChatGPT behaves.
</p></blockquote>
<p id="p3">And:</p>
<blockquote id="blockquote4"><p>
  We also believe users should have more control over how ChatGPT behaves and, to the extent that it is safe and feasible, make adjustments if they don&rsquo;t agree with the default behavior.</p>
<p>  Today, users can give the model specific instructions to shape its behavior with features like custom instructions. We&rsquo;re also building new, easier ways for users to do this. For example, users will be able to give real-time feedback to directly influence their interactions and choose from multiple default personalities.
</p></blockquote>
<p id="p5">&ldquo;Easier ways&rdquo; for users to adjust ChatGPT&rsquo;s behavior sound to me like a user-friendly toggle or slider to adjust ChatGPT&rsquo;s personality (Grok has something similar, albeit <a href="https://www.techradar.com/computing/artificial-intelligence/grok-3s-voice-mode-is-unhinged-and-thats-the-point" rel="noopener noreferrer">unhinged</a>), which I think would be a reasonable addition to the product. I&rsquo;ve long <a href="https://www.macstories.net/ios/s-gpt-1-0-2-brings-date-and-time-awareness-integration-with-macos-services-menu-passthrough-mode-better-homepod-support-and-more/" rel="noopener noreferrer">argued</a> that Siri should come with an adjustable personality similar to <a href="https://www.meetcarrot.com/weather/" rel="noopener noreferrer">CARROT Weather</a>, which lets you tweak whether you want the app to be &ldquo;evil&rdquo; or &ldquo;professional&rdquo; with a slider. I increasingly feel like that sort of option would make a lot of sense for modern LLMs, too.</p>
<p>â†’ Source: <a href='https://openai.com/index/sycophancy-in-gpt-4o/' target='_blank'>openai.com</a></p>