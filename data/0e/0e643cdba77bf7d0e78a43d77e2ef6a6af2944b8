<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0803-1763640592596.jpeg" alt="The M5 iPad Pro."><p class="image-caption">The M5 iPad Pro.</p></div>
<p id="p2">The best kind of follow-up article isn&rsquo;t one that clarifies a topic that someone got wrong (although I do love that, especially when that &ldquo;someone&rdquo; isn&rsquo;t me); it&rsquo;s one that provides more context to a story that was incomplete. My <a href="https://www.macstories.net/stories/m5-ipad-pro-review/" rel="noopener noreferrer">M5 iPad Pro review</a> was an incomplete narrative. As you may recall, I was unable to test Apple&rsquo;s promised claims of 3.5&times; improvements for local AI processing thanks to the new Neural Accelerators built into the M5&rsquo;s GPU. It&rsquo;s not that I didn&rsquo;t believe Apple&rsquo;s numbers. I simply couldn&rsquo;t test them myself due to the early nature of the software and the timing of my embargo.</p>
<p id="p3"><em>Well</em>, I was finally able to test local AI performance with a pre-release version of <a href="https://opensource.apple.com/projects/mlx/" rel="noopener noreferrer">MLX</a> optimized for M5, and let me tell you: not only is the hype real, but the numbers I got from my extensive tests over the past two weeks actually <em>exceed</em> Apple&rsquo;s claims.</p>
<p id="p4"><!--more--></p>
<p id="p5">This article won&rsquo;t be long, and I&rsquo;ll let the charts I recreated for the occasion speak for themselves. Essentially, as I suggested in my iPad Pro review, the M5&rsquo;s improvements for local AI performance largely apply to prompt processing (the prefill stage, when an LLM <a href="https://medium.com/%40sailakkshmiallada/understanding-the-two-key-stages-of-llm-inference-prefill-and-decode-29ec2b468114" rel="noopener noreferrer">needs to ingest the user&rsquo;s prompt</a> and load it into its context) and result in much, much shorter time to first token (TTFT) numbers. Since prompt processing with neural acceleration scales better with long prompts (where you can more easily measure the difference in latency between the M4 and M5), I focused on testing two different long prompt sizes: 10,000 and 16,000 tokens.</p>
<p id="p6">In my new tests based on an updated version of one of the published MLX samples, I used <a href="https://huggingface.co/Qwen/Qwen3-8B-MLX-4bit" rel="noopener noreferrer">Qwen3-8B-4bit</a> to measure the performance of local AI on my old M4 iPad Pro and new M5 iPad Pro. As you will see in the charts below, a prompt that took the M4&nbsp;81 seconds to process was loaded by the M5 in 18 seconds &ndash; a 4.4&times; improvement in TTFT. The numbers become even more impressive with longer prompts: while <strong>it took the M4&nbsp;118 seconds</strong> (nearly two minutes!) to start generating an answer for a 16,000-token prompt, <strong>the</strong> <strong>M5 did it&hellip;in 38 seconds</strong>.</p>
<p id="p7">But enough paragraphs about numbers! Let&rsquo;s see some pretty charts instead.</p>
<h3 id="10k-prompt">10k Prompt</h3>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_combined-1763629884047.png" alt=""><p class="image-caption"></p></div>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_ttft-1763629883753.png" alt="The key stat in these comparisons."><p class="image-caption">The key stat in these comparisons.</p></div>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_tokens_per_sec-1763629883602.png" alt=""><p class="image-caption"></p></div>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_total_time-1763629883611.png" alt=""><p class="image-caption"></p></div>
<h3 id="16k-prompt">16k Prompt</h3>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_combined_16k-1763630054998.png" alt=""><p class="image-caption"></p></div>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_ttft_16k-1763630054857.png" alt=""><p class="image-caption"></p></div>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_tokens_per_sec_16k-1763630054570.png" alt=""><p class="image-caption"></p></div>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/m4_vs_m5_total_time_16k-1763630054579.png" alt=""><p class="image-caption"></p></div>
<p id="p16">As you can see, there are some improvements for token generation across the M4 and M5, but at 1.5&times; faster generation, they&rsquo;re marginal. The star of the show for the M5 is prompt processing in the prefill stage: the Neural Accelerators vastly reduce the time needed by the M5 to process long prompts and start generating answers. The fact that this is happening on a consumer-grade tablet that is thin, lightweight, and fanless is all the more impressive.</p>
<hr id="hr17"><p id="p18">In practice, this means a few things.</p>
<p id="p19">If you&rsquo;re a developer of local AI apps for iPad, I highly recommend you start integrating with MLX and consider features that will take advantage of long prompts. <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation" rel="noopener noreferrer">RAG</a> applications for cross-document search, LLM clients with &ldquo;project&rdquo; features that support system-level instructions, and local AI clients that integrate with MCP servers (MCP tools notoriously <a href="https://www.anthropic.com/engineering/code-execution-with-mcp" rel="noopener noreferrer">fill the context window of LLMs</a> with instructions and tool descriptions) are the kinds of apps that will benefit from the M5&rsquo;s faster prompt processing the most, especially at long contexts.</p>
<p id="p20">For users, although the <a href="https://www.macstories.net/linked/ipados-26-2-beta-restores-drag-and-drop-gestures-for-split-view-and-slide-over/" rel="noopener noreferrer">iPad&rsquo;s app ecosystem</a> for local AI remains largely aspirational right now and behind the curve compared to macOS, there are early signs of iPad apps that will take advantage of the power of the M5. Apps like <a href="https://apps.apple.com/us/app/locally-ai-local-ai-chat/id6741426692" rel="noopener noreferrer">Locally AI</a>, <a href="https://apps.apple.com/us/app/offlinellm-private-ai-chat/id6474508768" rel="noopener noreferrer">OfflineLLM</a>, and <a href="https://apps.apple.com/us/app/craft-write-docs-ai-editing/id1487937127" rel="noopener noreferrer">Craft</a> (which supports local, offline assistants on iOS and iPadOS) should, theoretically, be able to tap into the power of the M5 and provide considerable performance gains compared to the M4.</p>
<p id="p21">The M5 alone doesn&rsquo;t change the fact that local AI is a niche, and local AI on iPadOS is a <em>niche of a niche</em> right now. However, the power is there, and as soon as the public version of MLX receives support for neural acceleration, we may start seeing a progressive buildup of AI tablet apps that can run offline &ndash; and inherently more private &ndash; LLMs with the kind of performance that was previously exclusive to desktops.</p>
<p id="p22">With this kind of power, it&rsquo;d be a shame if no one took advantage of it. I hope some third-party iPad app developers will, and I&rsquo;ll be along for the ride.</p>
