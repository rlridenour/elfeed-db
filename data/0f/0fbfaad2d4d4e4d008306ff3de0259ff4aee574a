<p id="p1">For all the criticism (<a href="https://www.macstories.net/stories/notes-on-the-apple-intelligence-delay/" rel="noopener noreferrer">mine included</a>) surrounding Apple&rsquo;s delay of various Apple Intelligence features, I found <a href="https://stratechery.com/2025/apple-ais-platform-pivot-potential/" rel="noopener noreferrer">this different perspective by Ben Thompson</a> fascinating and worth considering:</p>
<blockquote id="blockquote2"><p>
  What that means in practical terms is that Apple just shipped the best consumer-grade AI computer ever. A Mac Studio with an M3 Ultra chip and 512GB RAM can run a 4-bit quantized version of DeepSeek R1 &mdash; a state-of-the-art open-source reasoning model &mdash; right on your desktop. It&rsquo;s not perfect &mdash; quantization reduces precision, and the memory bandwidth is a bottleneck that limits performance &mdash; but this is something you simply can&rsquo;t do with a standalone Nvidia chip, pro or consumer. The former can, of course, be interconnected, giving you superior performance, but that costs hundreds of thousands of dollars all-in; the only real alternative for home use would be a server CPU and gobs of RAM, but that&rsquo;s even slower, and you have to put it together yourself. Apple didn&rsquo;t, of course, explicitly design the M3 Ultra for R1; the architectural decisions undergirding this chip were surely made years ago. In fact, if you want to include the critical decision to pursue a unified memory architecture, then your timeline has to extend back to the late 2000s, whenever the key architectural decisions were made for Apple&rsquo;s first A4 chip, which debuted in the original iPad in 2010. Regardless, the fact of the matter is that you can make a strong case that Apple is the best consumer hardware company in AI, and this week affirmed that reality.
</p></blockquote>
<p id="p3">Anecdotally speaking, based on the people who cover AI that I follow these days, it seems there are largely two buckets of folks who are into local, on-device models: those who have set up pricey NVIDIA rigs at home for their <a href="https://en.wikipedia.org/wiki/CUDA" rel="noopener noreferrer">CUDA cores</a> (the vast minority); and &ndash; the undeniable majority &ndash; those who run a spectrum of local models on their Macs of different shapes and configurations (usually, MacBook Pros). If you have to run high-end, performance-intensive local models for academic or scientific workflows on a desktop, the M3 Ultra Mac Studio sounds like an absolute winner.</p>
<p id="p4">However, I&rsquo;d point out that &ndash; again, as far as local, on-device models are concerned &ndash; Apple is <em>not</em> shipping the best possible hardware on <em>smartphones</em>.</p>
<p id="p5">While the entire iPhone 16 lineup is stuck on 8&nbsp;GB of RAM (and we know how memory-hungry <a href="https://daringfireball.net/linked/2024/04/22/on-device-ai-craves-ram" rel="noopener noreferrer">these models can be</a>), Android phones with at least <a href="https://9to5google.com/2025/01/22/samsungs-galaxy-s25-series-omits-16gb-variant/" rel="noopener noreferrer">12&nbsp;GB</a> or <a href="https://www.theverge.com/24336534/oneplus-13-review-specs-screen-camera-battery" rel="noopener noreferrer">16&nbsp;GB of RAM</a> are becoming pretty much the norm now, especially in flagship territory. Even better in Android land, what are being advertised as &ldquo;gaming phones&rdquo; with a whopping 24&nbsp;GB of RAM (such as the <a href="https://rog.asus.com/phones/rog-phone-9-pro/" rel="noopener noreferrer">ASUS ROG Phone 9 Pro</a> or the <a href="https://eu.redmagic.gg/pages/redmagic-10-pro" rel="noopener noreferrer">RedMagic 10 Pro</a>) may actually make for compelling pocket computers to run smaller, distilled versions of DeepSeek, LLama, or Mistral with better performance than current iPhones.</p>
<p id="p6">Interestingly, I keep going back to <a href="https://www.bloomberg.com/news/articles/2025-03-07/apple-confirms-delay-of-ai-infused-personalized-siri-assistant" rel="noopener noreferrer">this quote from Mark Gurman&rsquo;s latest report</a> on Apple&rsquo;s AI challenges:</p>
<blockquote id="blockquote7"><p>
  There are also concerns internally that fixing Siri will require having more powerful AI models run on Apple&rsquo;s devices. That could strain the hardware, meaning Apple either has to reduce its set of features or make the models run more slowly on current or older devices. It would also require upping the hardware capabilities of future products to make the features run at full strength.
</p></blockquote>
<p id="p8">Given Apple&rsquo;s struggles, their preference for a hybrid on-device/<a href="https://security.apple.com/blog/private-cloud-compute/" rel="noopener noreferrer">server-based</a> AI system, and the market&rsquo;s evolution on Android, I don&rsquo;t think Apple can afford to ship 8&nbsp;GB on iPhones for much longer if they&rsquo;re serious about AI and positioning their hardware as the best consumer-grade AI computers.</p>
<p>â†’ Source: <a href='https://stratechery.com/2025/apple-ais-platform-pivot-potential/' target='_blank'>stratechery.com</a></p>