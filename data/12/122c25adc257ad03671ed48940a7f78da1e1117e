<p>Since <a href="https://lmno.lol/alvaro/chatgpt-shell-goes-multi-model">chatgpt-shell going multi-model</a>, it was only a matter of time until we added support for local/offline models. As of version 2.0.6, <a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a> has a basic <a href="https://ollama.com/">Ollama</a> implementation (llama3.2 for now).</p>
<p><code>chatgpt-shell</code> is more than a shell. Check out the <a href="https://lmno.lol/alvaro/chatgpt-shell-goes-multi-model">demos in the previous post</a>.</p>
<p><img src="https://xenodium.github.io/images/chatgpt-shell-goes-offline/who-offline.gif" alt=""></p>
<p>For anyone keen on keeping all their LLM interactions offline, Ollama seems like a great option. I'm an Ollama noobie myself and already looking forward to getting acquainted. Having an offline LLM available at my Emacs fingertips will be super convenient.</p>
<p>For the more familiar with Ollama, please give the <a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a> integration a try (it's on <a href="https://melpa.org/#/chatgpt-shell">MELPA</a>).</p>
<p>v2.0.6 has a basic/inital Ollama implementation. Please give it a good run and <a href="https://github.com/xenodium/chatgpt-shell/issues/new">report bugs</a>.</p>
<p>You can swap models via:</p>
<pre><code>M-x chatgpt-shell-swap-model
</code></pre>
<p>Help make this project sustainable. <a href="https://github.com/sponsors/xenodium">Sponsor the work</a>.</p>
