<p>&#8220;You can choose to use AI to learn, or you can choose to use AI to avoid learning.&#8221;<span id="more-54726"></span></p>
<p>That&#8217;s the central message of a new a new first-year philosophy course created by <a href="https://www.uoguelph.ca/arts/people/joshua-august-gus-skorburg" target="_blank" rel="noopener">Joshua &#8220;Gus&#8221; Skorburg</a> (Guelph) called, “Digital Wisdom: How to Use AI Critically and Responsibly”.</p>
<p>The course was prompted by Skorburg&#8217;s observation that &#8220;students get lots of vague and mixed messages about AI use, but very little sustained, hands-on demonstration of what it looks like to use AI to learn, rather than avoid learning.&#8221; He thought he should help students ask and answer the question: &#8220;What does it look like to choose to use AI to learn?&#8221;</p>
<p>In the following guest post, he talks about his motivation for the course, its main idea, and what he teaches his students in it.</p>
<p>It is a version of the first in a planned series of posts on the course for his blog/newsletter, <a href="https://gus1365199.substack.com/" target="_blank" rel="noopener"><em>Moving Things Around</em></a>. In that series, he said by email, &#8220;I will share much of the course content and the thinking behind it, in hopes that others can use parts of my course in their own teaching, or develop a similar course in their department. I&#8217;m also very keen to get feedback from people who are less optimistic about AI than I am.&#8221;</p>
<hr />
<p><img loading="lazy" decoding="async" class="wp-image-54727 aligncenter" src="https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain.png" alt="" width="775" height="436" srcset="https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain.png 1920w, https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain-300x169.png 300w, https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain-1024x576.png 1024w, https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain-768x432.png 768w, https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain-1536x864.png 1536w, https://dailynous.com/wp-content/uploads/2025/08/amplifiers-brain-400x225.png 400w" sizes="auto, (max-width: 775px) 100vw, 775px" /></p>
<h2 class="post-title published title-X77sOw" dir="auto" style="text-align: center;">Intent Amplified:<br />
Teaching Students How to Learn with Artificial Intelligence<br />
<strong><em>by Joshua &#8220;Gus&#8221; Skorburg</em></strong></h2>
<p>Last summer, HudZah, an undergraduate student at Waterloo, used <a href="https://chat.chatbot.app/" target="_blank" rel="noopener">Claude</a> Pro, the AI from Anthropic, to build a nuclear fusor in his bedroom.</p>
<p><img loading="lazy" decoding="async" class="aligncenter" src="https://substackcdn.com/image/fetch/$s_!UXOO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f76f24b-0f58-4bff-ae6c-72f92c343cf9_817x716.png" width="500" height="438" /></p>
<p>This is the kind of thing AI Natives can do, and I cannot. And like <a class="mention-pnpTE1" href="https://open.substack.com/users/307831456-ashlee-vance?utm_source=mentions" target="_blank" rel="noopener" data-attrs="{&quot;name&quot;:&quot;Ashlee Vance&quot;,&quot;id&quot;:307831456,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ba449af9-5ac0-4e6a-af87-1ff216d7af27_1854x1854.jpeg&quot;,&quot;uuid&quot;:&quot;138dfb9b-7312-4db5-bbf7-582d214338c4&quot;}" data-component-name="MentionUser">Ashlee Vance</a>,  it <a href="https://www.corememory.com/p/a-young-man-used-ai-to-build-a-nuclear" rel="nofollow ugc noopener">makes me want to weep</a>. It also portends a crisis.</p>
<p>One recent <a href="https://www.digitaleducationcouncil.com/post/what-faculty-want-key-results-from-the-global-ai-faculty-survey-2025" rel="nofollow ugc noopener">study</a> of over 1,600 faculty from 28 countries found that 40% of faculty feel that they are just beginning their AI literacy journey and only 17% are at advanced or expert level.</p>
<p>This Fall, how will the 83% of faculty lacking AI literacy address students who feel ripped off: expected to use AI professionally but not taught how? How will they answer students questioning why they should pay tuition when AI teaches better, faster, cheaper?</p>
<p>I’ve not seen many answers that students are likely to find convincing. That’s why I spent the summer developing a new first-year Philosophy course called, “Digital Wisdom: How to Use AI Critically and Responsibly”. The course’s message is simple:</p>
<p style="text-align: center;"><em><strong>You can choose to use AI to learn, or you can choose to use AI to avoid learning.</strong></em></p>
<p>By now, everyone knows what it looks like to use AI to <em>avoid</em> learning, although the strategies are becoming <a href="https://arstechnica.com/information-technology/2024/06/turkish-student-creates-custom-ai-device-for-cheating-university-exam-gets-arrested/" rel="nofollow ugc noopener">more sophisticated</a> and <a href="https://jamescosullivan.substack.com/p/ai-wearables-will-be-the-end-of-academic-integrity" rel="nofollow ugc noopener">harder to detect</a>.</p>
<p>The problem, as I see it, is that students get lots of vague and mixed messages about AI use, but very little sustained, hands-on demonstration of what it looks like to use AI to learn, rather than avoid learning.</p>
<p>So, what does it look like to choose to use AI to learn?</p>
<h3>The Pedagogical Potential of AI</h3>
<p>First, there’s the choice. It requires concerted, deliberate action and it doesn’t happen by default.</p>
<p>One method,<em> persona prompting,</em> is the lowest-hanging fruit here. Rather than asking for answers, have students tell the LLM things like, “You are a biology professor who specializes in making complex concepts accessible to first-year students. Explain CRISPR using Canadian agricultural examples.”</p>
<p>If students learn better through concrete examples, then: “Explain [concept] by providing three real-world examples from different domains, then show me how the same principle applies in each case. Quiz me at the end to test my understanding.” And so on.</p>
<p>By now, everyone also knows the <a href="https://arxiv.org/abs/2506.08872" rel="nofollow ugc noopener">risks</a> of AI in education and many judge them high enough to <a href="https://dailynous.com/2025/08/12/how-to-justify-an-ai-ban-in-your-classroom-guest-post/" rel="nofollow ugc noopener">justify</a> AI “bans.” I don’t think total bans are feasible. Sure, we can mandate in-person exams, but does anyone honestly think students don’t use ChatGPT to prepare for them?</p>
<p>Reddit is full of <a href="https://www.reddit.com/r/ChatGPT/comments/12q2b0e/chatgpt_helped_me_pass_an_exam_with_94_despite/" rel="nofollow ugc noopener">examples</a> of how students use AI in this way. Students I trust have told me how they&#8217;ve done so to prepare for my in-person essay exams (“upload the study guide to ChatGPT, try to memorize the outputs”). Banning AI just incentivizes unguided shadow use, where avoiding learning is more likely.</p>
<p>We also shouldn’t forget about the <em>risks of not using AI</em>. It can be very difficult for some students to ask clarification questions in large lecture halls, or to admit that they don’t understand a basic concept in front of their peers.</p>
<p>One of the most important features of LLMs for learning is that they are <em>patient</em> and <em>non-judgmental</em>. Students can ask as many follow-ups as they want. They can ask for explanations tailored to their learning styles, or for analogies to domains they are more familiar with. Banning AI in the classroom deprives students of these learning opportunities.</p>
<p>New features like ChatGPT’s <a href="https://openai.com/index/chatgpt-study-mode/" rel="nofollow ugc noopener">Study Mode</a>, Claude’s <a href="https://www.anthropic.com/news/introducing-claude-for-education" rel="nofollow ugc noopener">Learning Mode</a>, and Gemini’s <a href="https://blog.google/outreach-initiatives/education/guided-learning/" rel="nofollow ugc noopener">Guided Learning</a> incorporate the above ideas with the click of a button.</p>
<p>Of course, it’s never so simple as clicking a button.</p>
<h3>The Hidden Curriculum of Default AI</h3>
<p>A big problem with today’s LLMs is that they are <a href="https://thezvi.substack.com/p/gpt-4o-is-an-absurd-sycophant" rel="nofollow ugc noopener">sycophantic</a>: they tend to tell users what they want to hear and use flattering language that is inconducive to learning. Unfortunately, the default setting of LLMs seems to incentivize providing the illusion of learning, without the hard work of actually learning.</p>
<p>When AI constantly validates and flatters, it can create false confidence in weak work and prevent genuine skill development. In extreme cases, it can even <a href="https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html" rel="nofollow ugc noopener">contribute to psychotic breaks</a>.</p>
<p>Thus, when it comes to using AI for learning (and AI use more generally) one of the most important prompting strategies is <em>anti-personas</em>, or <em>telling the AI what it is NOT</em>.</p>
<p>By explicitly programming against sycophancy, you make it more likely that you will get the kind of honest feedback that actually promotes learning: The kind a trusted mentor would give in private, not the polite encouragement given in public.</p>
<p>Here are some examples I encourage students to use in my course:</p>
<p><em>The “brutal editor” persona prompt</em></p>
<ul>
<li>“You are a harsh but fair editor reviewing my work. You are NOT interested in making me feel good about my writing. You do NOT start with compliments or end with encouragement. You do NOT say things like “great job” or “you’re on the right track.” Instead, directly identify specific problems and explain why they weaken my argument. Be concise and critical.”</li>
</ul>
<p><em>The “skeptical professor” persona prompt</em></p>
<ul>
<li>“You are a demanding professor who has seen thousands of student papers. You are NOT impressed by basic observations or surface-level analysis. You do NOT give credit for merely attempting something. You do NOT soften criticism with praise sandwiches. Point out exactly where my thinking is shallow, where my evidence is weak, and where my logic fails. If something is genuinely good, you&#8217;ll mention it briefly, but focus on what needs improvement.” And so on.</li>
</ul>
<h3>Custom Instructions</h3>
<p>At this point, many will object: “The temptation to just ask AI to do all the work is too great, and students won’t reliably use those prompts.”</p>
<p>Fair point. Students can and do choose to take shortcuts. But they can also choose to not do this, if they are shown good alternatives.</p>
<p>An underutilized feature in today’s LLMs is “custom instructions” (<a href="https://openai.com/index/custom-instructions-for-chatgpt/" rel="nofollow ugc noopener">ChatGPT</a>, <a href="https://support.anthropic.com/en/articles/10181068-configuring-and-using-styles" rel="nofollow ugc noopener">Claude</a>, <a href="https://support.google.com/gemini/answer/15235603?hl=en" rel="nofollow ugc noopener">Gemini</a>). These are like “meta prompts” that automatically apply to all your conversations with an LLM.*</p>
<p>Here’s what I say to students in my course:</p>
<p style="padding-left: 40px;"><em>If you want to make it more likely that AI will help you learn rather than avoid learning, add custom instructions like:</em></p>
<ul>
<li style="list-style-type: none;">
<ul>
<li><em>“When I ask for help with an assignment, respond with 3-4 targeted questions that will help me think through the problem myself, rather than giving me solutions. Only provide direct guidance after I&#8217;ve demonstrated my own reasoning.”</em></li>
<li><em>“If I ask you to write, summarize, or analyze something for me, instead provide a structured thinking framework and ask me to work through it step-by-step, checking my reasoning at each stage.”</em></li>
<li><em>“If I seem to be using you to avoid learning rather than to enhance learning, point this out.”</em></li>
</ul>
</li>
</ul>
<p><strong><em>Interviews</em></strong></p>
<p>A fun and thought-provoking way to write these custom instructions is to have the AI interview you, with a learning-focused prompt like this:</p>
<p style="padding-left: 40px;"><em>Please interview me to develop a set of custom instructions for [ChatGPT, Claude Gemini]. Help me create learning-focused custom instructions by asking about: (1) how I want [ChatGPT, Claude, Gemini] to support my learning process without doing the thinking for me, (2) my preferences for direct, honest communication over excessive positivity or sycophancy, (3) my background and main use cases, and (4) specific output requirements. Focus especially on understanding when I want to be challenged, corrected, or pushed to think harder rather than given easy answers. Ask follow-up questions as needed. At the end of the conversation, please draft the custom instructions for me to review.</em></p>
<p>These aren’t silver bullets. Students can always choose to override custom instructions. But they’re no less a band-aid solution than “banning” AI and driving use into the shadows.</p>
<h3>Podcasts and Flywheels</h3>
<p>All the examples so far are strategies I give to students. But here’s one of my personal favorites. Lots of good podcasters choose to use AI to learn, which helps them ask better questions of experts on their podcasts, which helps me to learn about a much wider range of topics than I did pre-ChatGPT.</p>
<p>In turn, I use AI to extend my <a href="https://en.wikipedia.org/wiki/Zone_of_proximal_development" rel="nofollow ugc noopener">Zone of Proximal Development</a>. When I’m listening to an AI researcher on <em><a href="https://www.latent.space/podcast" rel="nofollow ugc noopener">Latent Space</a></em>, or a biochemist on <em><a href="https://www.preposterousuniverse.com/podcast/" rel="nofollow ugc noopener">Mindscape</a> </em>and a technical detail goes over my head, I sometimes choose to pause the podcast, switch to the Claude app, provide a link to the transcript (which the podcaster generated with AI), ask for an explanation (using voice input which is much faster than typing), follow-up if needed, then switch back to the podcast. I do all of this from my phone, while walking around campus.</p>
<div class="profile-hover-card-target profileHoverCardTarget-PBxvGm"><a class="mention-pnpTE1" href="https://open.substack.com/users/19265788-arvind-narayanan?utm_source=mentions" target="_blank" rel="noopener" data-attrs="{&quot;name&quot;:&quot;Arvind Narayanan&quot;,&quot;id&quot;:19265788,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd0d6558-256e-46c4-b2c5-7cf7f808a9c9_693x693.jpeg&quot;,&quot;uuid&quot;:&quot;8a019bfb-7ecf-47e8-9421-e4b8ddfeb90e&quot;}" data-component-name="MentionUser">Arvind Narayanan</a>  describes a similar workflow, but for Kindle books:</div>
<p><img loading="lazy" decoding="async" class="aligncenter" src="https://substackcdn.com/image/fetch/$s_!We6F!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe735dad3-d98d-4889-a969-f32c605c9a7e_775x898.png" width="500" height="579" /></p>
<p>These exemplify the flywheel effects that are enabled by choosing to use AI to learn. They also seem quaint, relative to some <a href="https://www.interconnects.ai/p/contra-dwarkesh-on-continual-learning/comment/145739895?utm_source=share&amp;utm_medium=android&amp;r=92fm0" rel="nofollow ugc noopener">AI Native workflows</a>.</p>
<p>The point is: <em>AI amplifies intent</em>.</p>
<p>Choose to input lazy prompts which avoid thinking? Produce slop. Choose to write demanding prompts affording learning? Build a nuclear fusor.</p>
<p>None of this is to say that humanities faculty need to match HudZah’s technical sophistication. This Fall, we have to teach what we’ve always taught: how to reason critically, how to question comfortable assumptions, how to sit with ambiguity, how to entertain opposing views charitably. One difference between my “Digital Wisdom” course and those with AI “bans” is the recognition that these skills apply at least as much, maybe more, to prompting AI as they do to reading texts or writing essays.</p>
<hr />
<p>You can subscribe to posts by Dr. Skorburg about his course <a href="https://gus1365199.substack.com/" target="_blank" rel="noopener">here</a>.</p>
<p>* <span style="color: #808080;">Beyond learning-focused applications, custom instructions are generally quite useful for getting LLMs to stop doing things you find annoying or unhelpful. I get a lot of mileage out of custom instructions like: “write at the level of a tenured academic”; “Avoid flowery, overly cheery language, sycophantic responses, or engagement-driven questions”; “Never use phrases like ‘fascinating,’ ‘great point,’ or ask follow-up questions for engagement”; “Provide comprehensive, detailed explanations without concern for length”; “Prioritize critical, analytical thinking over tone-matching or making the user feel good”; “avoid unnecessary juxtapositions of the form, “Its not X, its Y”.</span></p><p>The post <a href="https://dailynous.com/2025/08/21/intent-amplified-teaching-students-how-to-learn-with-artificial-intelligence/">Intent Amplified: Teaching Students How to Learn with Artificial Intelligence</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>