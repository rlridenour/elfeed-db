
<p>MG Siegler, writing at Spyglass:</p>

<blockquote>
  <p>And so you can’t help but wonder if part of the equation in this settlement wasn’t decidedly more cynical. Fresh off <a href="https://spyglass.org/signal-the-google-antitrust-nuance/#:~:text=%F0%9F%A7%A0%20Anthropic%27s%20%2413B,%5BBloomberg%20%F0%9F%94%92%5D">a new massive fundraise</a> — one in which they raised <a href="https://www.bloomberg.com/news/articles/2025-09-02/anthropic-completes-new-funding-round-at-183-billion-valuation?embedded-checkout=true&amp;ref=spyglass.org">far more</a> than they were initially targeting, I might add — Anthropic has a lot of money. More than perhaps <a href="https://spyglass.org/openai-ipo-ish/">all but one of their competitors</a> on the startup side. By settling for $1.5B, is Anthropic sort of pulling up a drawbridge, making it so that other startups can’t possibly come into their castle? I mean, am I crazy?</p>

<p>I’m not so sure I am. At $1.5B, there are only a handful of companies that could afford to pay such fines. Certainly OpenAI is one. Maybe xAI. And of course all the tech giants like Apple, Amazon, Microsoft, Google, <a href="https://spyglass.org/meta-operation-kick-puppies/">and Meta</a>. But could any other startup that has done any level of model training with such data? Probably not.</p>
</blockquote>

<p>Kind of dastardly when you think about it this way.</p>

<div>
<a  title="Permanent link to ‘A Cynical Read on Anthropic’s Book Settlement’"  href="https://daringfireball.net/linked/2025/09/06/siegler-cynical-read">&nbsp;★&nbsp;</a>
</div>

	