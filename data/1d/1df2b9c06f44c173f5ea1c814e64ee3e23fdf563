<p>As <a href="https://github.com/tanrax/org-social">Org Social</a> grows, users follow more feeds, and individual <code>social.org</code> files accumulate hundreds of posts over time. Traditional approaches that download entire feeds sequentially create two major bottlenecks:</p>
<ol>
<li><strong>Bandwidth waste</strong>: Downloading complete files when users only need recent posts</li>
<li><strong>Time inefficiency</strong>: Sequential downloads that block the user interface</li>
</ol>
<p>This article explores how <a href="https://github.com/tanrax/org-social.el">Org-social.el</a> 2.3+ (Official client) solves both problems with a sophisticated combination of concurrent queue processing and HTTP Range-based partial fetching while maintaining complete compatibility with all servers.</p>
<h2 id="blog-single-content__title--the-challenge" class="blog-single__content-title">The Challenge</h2>
<pre class="mermaid">flowchart TB
    A[User Opens Timeline] --> B[20 Feeds to Download]
    B --> C[Traditional Approach: Sequential Downloads]
    C --> D[Feed 1: 27KB 150 posts]
    D --> E[Feed 2: 15KB 80 posts]
    E --> F[Feed 3: 12KB 60 posts]
    F --> G[... 17 more feeds]
    G --> H[Total: ~300KB and ~1500 posts]
    H --> I[Filter to last 14 days]
    I --> J[Actually needed: ~10 posts=first page]

    style C fill:#ffcccc,color:black
    style H fill:#ffcccc,color:black
    style J fill:#ccffcc,color:black</pre>
<p>Downloading 300KB and processing 1500 posts to get 10 posts... It is not good!</p>
<h2 id="blog-single-content__title--optimization" class="blog-single__content-title">Optimization</h2>
<p><a href="https://github.com/tanrax/org-social.el">Org-social.el</a> implements a sophisticated three-layer approach:</p>
<h3 id="blog-single-content__title--layer-1-concurrent-queue-processing" class="blog-single__content-title">Layer 1: Concurrent Queue Processing</h3>
<p>A <strong>process queue</strong> is a data structure that manages tasks to be executed. In Org Social, each feed to download is added to the queue as a pending task. The system then processes these tasks concurrently (multiple at the same time) using a worker pool—a limited number of threads that execute downloads in parallel.</p>
<p>This smart queue system manages parallel downloads without overwhelming system resources.</p>
<pre class="mermaid">flowchart LR
    A[Feed Queue] --> B[Worker Pool Max 20 concurrent]
    B --> C[Worker 1 Feed A]
    B --> D[Worker 2 Feed B]
    B --> E[Worker 3 Feed C]
    B --> F[...]
    B --> G[Worker 20 Feed T]

    C --> H{Done?}
    D --> H
    E --> H
    G --> H

    H -->|Yes| I[Process Next Pending Feed]
    H -->|Error| J[Mark Failed, Continue]

    I --> B
    J --> I

    style B fill:#e1f5ff,color:black
    style H fill:#fff4e1,color:black
    style I fill:#ccffcc,color:black</pre>
<p>Key Features:</p>
<ul>
<li><strong>Configurable concurrency</strong>: <code>org-social-max-concurrent-downloads</code> (default: 20)</li>
<li><strong>Non-blocking threads</strong>: Each download runs in a separate thread</li>
<li><strong>Automatic recovery</strong>: Failed downloads don't block the queue</li>
<li><strong>Smart scheduling</strong>: New downloads start immediately when slots free up</li>
</ul>
<h3 id="blog-single-content__title--layer-2-http-range-based-partial-fetching" class="blog-single__content-title">Layer 2: HTTP Range-Based Partial Fetching</h3>
<p>HTTP has a built-in feature that allows downloading only specific parts of a file using the <code>Range</code> header. When a client sends a request with <code>Range: bytes=0-999</code>, the server responds with just the first 1000 bytes of the file instead of the entire content. This capability is commonly used for video streaming and resumable downloads, but it can also be used to <strong>paginate files</strong>—downloading them in chunks rather than all at once.</p>
<p>Instead of downloading entire <code>social.org</code> files, Org Social uses HTTP Range requests to fetch only what's needed: the header section and recent posts.</p>
<p>This system is not compatible with all providers. While most traditional web servers (Apache, Nginx, Caddy) support HTTP Range requests natively, some hosting platforms have limitations:</p>
<ul>
<li>
<p><strong>Cloudflare CDN</strong>: Does not provide <code>Content-Length</code> or <code>Content-Range</code> headers, making it impossible to determine file size or download specific byte ranges. The system automatically falls back to downloading the complete file and filtering client-side.</p>
</li>
<li>
<p><strong>Codeberg.org</strong>: Implements aggressive rate limiting when multiple Range requests are made in quick succession. When HTTP 429 (Too Many Requests) is detected, the system falls back to a full download without filtering to avoid being blocked.</p>
</li>
<li>
<p><strong>GitHub Raw Content</strong>: Provides proper HTTP Range support and works optimally with partial downloads.</p>
</li>
</ul>
<p>The system detects these limitations automatically and adapts its strategy to ensure 100% compatibility across all hosting platforms.</p>
<pre class="mermaid">sequenceDiagram
    participant C as Client
    participant S as Server
    participant F as social.org (27KB, 150 posts)

    Note over C,F: Step 1: Find Header
    C->>S: Range: bytes=0-999
    S->>C: First 1KB (headers)
    Note over C: Found "* Posts" at byte 800

    Note over C,F: Step 2: Get File Size
    C->>S: Range: bytes=0-0
    S->>C: Content-Range: bytes 0-0/27656
    Note over C: Total size: 27656 bytes

    Note over C,F: Step 3: Fetch Recent Posts
    C->>S: Range: bytes=26656-27655
    S->>C: Last 1KB (recent posts)

    C->>S: Range: bytes=25656-26655
    S->>C: Previous 1KB
    Note over C: Found post older than 14 days

    Note over C,F: Result: Downloaded 3KB instead of 27KB</pre>
<p><strong>Algorithm:</strong></p>
<ol>
<li><strong>Header Discovery</strong> (bytes 0 → forwards)</li>
<li>Download the first 1KB chunk (bytes 0-999)</li>
<li>If <code>* Posts</code> is not found, download the next 1KB chunk (bytes 1000-1999)</li>
<li>Continue downloading subsequent chunks until <code>* Posts</code> is found</li>
<li>
<p>Typical header size: 500-1500 bytes</p>
</li>
<li>
<p><strong>Backward Post Fetching</strong> (end → backwards)</p>
</li>
<li>Start from the end of the file (most recent posts)</li>
<li>Download 1KB chunks moving backwards</li>
<li>Parse each post's <code>:ID:</code> property (e.g., <code>:ID: 2025-10-24T10:00:00+0200</code>)</li>
<li>
<p>Stop when reaching posts older than <code>org-social-max-post-age-days</code> (default: 14 days)</p>
</li>
<li>
<p><strong>Date Filtering</strong></p>
</li>
<li>Parse post IDs (RFC 3339 timestamps)</li>
<li>Keep only posts ≥ start date</li>
<li>Discard older posts without downloading</li>
</ol>
<h3 id="blog-single-content__title--trick-for-range-support-detection" class="blog-single__content-title">Trick for Range Support Detection</h3>
<p>The system sends a test request with the <code>Range: bytes=0-0</code> header to check if the server responds with <code>Content-Range</code> or <code>Accept-Ranges: bytes</code> headers, indicating Range support.</p>
<h3 id="blog-single-content__title--edge-cases-and-fallbacks-compressed-content" class="blog-single__content-title">Edge Cases and Fallbacks: Compressed Content</h3>
<p>Servers using gzip compression (e.g., Caddy) report compressed sizes:</p>
<p><strong>Problem</strong>: HEAD request returns compressed size, but content arrives uncompressed</p>
<p><strong>Solution</strong>: Use a Range request (<code>bytes=0-0</code>) for size detection instead. The server responds with <code>Content-Range: bytes 0-0/TOTAL</code> where TOTAL is the actual uncompressed file size.</p>
<h3 id="blog-single-content__title--layer-3-ui-pagination" class="blog-single__content-title">Layer 3: UI Pagination</h3>
<p>Even after downloading only recent posts, rendering all of them at once would overwhelm Emacs. The Org Social UI uses <strong>widgets</strong> to create an interactive interface with buttons, images, and formatted text. Each widget consumes memory and processing power.</p>
<p>To keep the interface responsive, the system implements <strong>pagination</strong> that displays only <strong>10 posts per page</strong>. This means:</p>
<ul>
<li>When a user opens the timeline, only the first 10 most recent posts are rendered</li>
<li>Images, avatars, and interactive widgets are created only for these 10 visible posts</li>
<li>The remaining downloaded posts stay in memory but aren't rendered</li>
<li>Users can navigate to the next page, which then renders the next 10 posts</li>
</ul>
<pre class="mermaid">flowchart LR
    A[Downloaded Posts: 50] --> B[Page 1: Render 10 posts]
    A --> C[Page 2: 10 posts in memory]
    A --> D[Page 3: 10 posts in memory]
    A --> E[Page 4: 10 posts in memory]
    A --> F[Page 5: 10 posts in memory]

    B --> G[User sees: 10 posts with widgets & images]

    C -.->|User clicks Next| H[Render next 10 posts]

    style A fill:#e1f5ff,color:black
    style B fill:#ccffcc,color:black
    style G fill:#d5ffe1,color:black
    style H fill:#ffe1cc,color:black</pre>
<p>Emacs widgets and image rendering are resource-intensive. Rendering 50 posts with avatars and buttons could slow down the editor. By rendering only 10 at a time, the UI stays fast and responsive regardless of how many posts were downloaded.</p>
<p>This is the final optimization layer: even if your <code>social.org</code> has 10,000 posts, and you download 50 recent ones, you only render 10 on screen. The rest wait in memory until needed.</p>
<h2 id="blog-single-content__title--performance-benchmarks" class="blog-single__content-title">Performance Benchmarks</h2>
<p>The following table shows how the system scales with different feed sizes, assuming an average post size of 250 bytes and a 14-day filter (capturing approximately 20-30 recent posts):</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Total Posts</th>
<th>Full File Size</th>
<th>With Partial Fetch (14 days)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Empty feed</td>
<td>0</td>
<td>~1 KB</td>
<td>~1 KB</td>
<td>Only headers downloaded</td>
</tr>
<tr>
<td>New user</td>
<td>1</td>
<td>~1.5 KB</td>
<td>~1.5 KB</td>
<td>Single post, no optimization needed</td>
</tr>
<tr>
<td>Light user</td>
<td>10</td>
<td>~3.5 KB</td>
<td>~3.5 KB</td>
<td>All posts fit in 14-day window</td>
</tr>
<tr>
<td>Regular user</td>
<td>100</td>
<td>~26 KB</td>
<td>~8 KB</td>
<td>Headers + ~30 recent posts</td>
</tr>
<tr>
<td>Active user</td>
<td>1,000</td>
<td>~250 KB</td>
<td>~8 KB</td>
<td>Headers + ~30 recent posts</td>
</tr>
<tr>
<td>Power user</td>
<td>10,000</td>
<td>~2.5 MB</td>
<td>~8 KB</td>
<td>Headers + ~30 recent posts</td>
</tr>
</tbody>
</table>
<p><strong>Key insight</strong>: Once a feed exceeds ~100 posts, partial fetching maintains consistent download sizes (~8 KB) regardless of total feed size. A feed with 10,000 posts downloads the same amount of data as one with 1,000 posts.</p>
<h2 id="blog-single-content__title--tuning-recommendations" class="blog-single__content-title">Tuning Recommendations</h2>
<p>Users can customize two main parameters:</p>
<ul>
<li><code>org-social-max-concurrent-downloads</code>: Maximum parallel downloads (default: 20)</li>
<li><code>org-social-max-post-age-days</code>: Maximum age of posts to fetch in days (default: 14)</li>
</ul>
<p>So...</p>
<ul>
<li><strong>Fast connection + many feeds</strong>: Increase to 30 concurrent</li>
<li><strong>Slow connection</strong>: Decrease to 10 concurrent</li>
<li><strong>Large feeds + limited bandwidth</strong>: Decrease <code>max-post-age-days</code> to 7</li>
<li><strong>Small feeds</strong>: Set <code>max-post-age-days</code> to nil (no optimization needed)</li>
</ul>
<h2 id="blog-single-content__title--conclusion" class="blog-single__content-title">Conclusion</h2>
<p>The three-layer optimization approach (concurrent queue processing, HTTP Range-based partial fetching, and UI pagination) provides a significant bandwidth optimization on large feeds with date filtering and non-blocking UI. This architecture positions Org Social to scale efficiently as both individual feeds and follower counts grow, while maintaining the simplicity and decentralization that make Org Social unique.</p>
<p>Your <code>social.org</code> can have millions of lines because:</p>
<ol>
<li>Only recent posts are downloaded (14 days by default)</li>
<li>Downloads happen in parallel without blocking</li>
<li>Only 10 posts are rendered on screen at once</li>
</ol>
<p>Enjoy it!</p>
<h2 id="blog-single-content__title--technical-references" class="blog-single__content-title">Technical References</h2>
<ul>
<li><strong>HTTP Range Requests</strong>: <a href="https://datatracker.ietf.org/doc/html/rfc7233">RFC 7233</a></li>
<li><strong>RFC 3339 Timestamps</strong>: <a href="https://datatracker.ietf.org/doc/html/rfc3339">RFC 3339</a></li>
<li><strong>Org Social Specification</strong>: <a href="https://github.com/tanrax/org-social">github.com/tanrax/org-social</a></li>
</ul>