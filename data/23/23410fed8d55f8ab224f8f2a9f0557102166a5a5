<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-29-at-18-45-27-2x-1748537149987.png" alt=""><p class="image-caption"></p></div>
<p id="p2">DeepSeek <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528" rel="noopener noreferrer">released an updated version</a> of their popular R1 reasoning model (version 0528) with &ndash; <a href="https://x.com/deepseek_ai/status/1928061589107900779" rel="noopener noreferrer">according</a> to the company &ndash; increased benchmark performance, reduced hallucinations, and native support for function calling and JSON output. Early tests from Artificial Analysis <a href="https://x.com/ArtificialAnlys/status/1928071179115581671" rel="noopener noreferrer">report a nice bump in performance</a>, putting it behind OpenAI&rsquo;s o3 and o4-mini-high in their Intelligence Index benchmarks. The model is available in the official DeepSeek API, and open weights have been <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528" rel="noopener noreferrer">distributed on Hugging Face</a>. I downloaded different quantized versions of the full model on my M3 Ultra Mac Studio, and here are some notes on how it went.</p>
<p id="p3"><!--more--></p>
<p id="p4">Last night, the folks at MLX Community <a href="https://huggingface.co/mlx-community/DeepSeek-R1-0528-4bit" rel="noopener noreferrer">uploaded a 4-bit quant of the full 685B parameter model</a>, which is a ~350&nbsp;GB download that I immediately tested in <a href="https://lmstudio.ai" rel="noopener noreferrer">LM Studio</a>, which has a built-in MLX interpreter for Apple Silicon. Loading the model in LM Studio lets you set a maximum context window size of 163,840 tokens; I attempted that, but the M3 Ultra with 512&nbsp;GB of RAM couldn&rsquo;t do it. Setting the context window to a much smaller 32,000 token size loaded the model with 363&nbsp;GB of RAM used, but it was swiftly ejected from memory as soon as I tried to chat with it. Going down to a more reasonable 8,192 context window size did the trick. Using a prompt that asked R1-0528 to reformat a long text file (attached to the chat) produced these results:</p>
<ul id="ul5"><li><strong>Prompt</strong>: <em>&ldquo;This is the output of an advanced OCR process of a long PDF. Can you help me clean up cruft? Only remove repetitions, page markers, and other byproducts of OCR. Do NOT change the main body text and sections. Return the full text without truncating it. Explain what you did.&rdquo;</em></li>
<li><strong>Results</strong>: 15.74 tok/sec &bull; 1344 tokens &bull; 37.70s to first token</li>
</ul><p id="p6">And here&rsquo;s how the updated R1 thought behind the scenes:</p>
<blockquote id="blockquote7"><p>
  Alright, let me tackle this user request step by step. The user shared OCR output from a PDF and wants it cleaned up without altering the main content. They specifically mentioned removing repetitions, page markers, and OCR artifacts while preserving all meaningful text sections.</p>
<p>  Hmm, first I notice the citations contain overlapping content about iOS features like Dictation improvements and security enhancements. The key is to deduplicate without losing any unique details&mdash;for example, the &ldquo;And More&rdquo; section appears in both citations but with different endings. I&rsquo;ll need to merge those carefully.</p>
<p>  The user&rsquo;s explicit request is clear, but their tone suggests urgency about getting a clean document&mdash;probably for professional use. They emphasized keeping all substantive text, so I should avoid over-editing. The repetition of features like &ldquo;Improved speech recognition&rdquo; across citations is a clear target for consolidation.</p>
<p>  Looking closer, page markers (like &ldquo;19&rdquo;) and fragments (&ldquo;## Overview | Key Features&hellip;&rdquo;) are classic OCR artifacts. Those can go safely since they don&rsquo;t add value. The headings seem consistent, so I&rsquo;ll preserve their hierarchy while removing redundancies&mdash;like the duplicated &ldquo;### Dictation&rdquo; section.</p>
<p>  Wait, should I explain why certain deletions happened? Yes&mdash;the user asked for explanations, so noting removed page numbers/repeats will build trust. Also, they didn&rsquo;t mention formatting fixes, so I won&rsquo;t touch line breaks or spacing unless it&rsquo;s clearly an artifact (like random numbers mid-text).</p>
<p>  Biggest challenge: ensuring the merged &ldquo;And More&rdquo; section captures both citations&rsquo; unique points (cursor animation + language indicator) without duplication. I&rsquo;ll cross-check each sentence&hellip; done. Finally, a quick verification&mdash;no technical details were lost, all sections flow logically, and zero main text edits. Perfect!
</p></blockquote>
<p id="p8">The results were pretty good, and R1 reformatted the document as requested&hellip;or at least a <em>portion</em> of it, since the small context window resulted in a truncated output in that R1 couldn&rsquo;t fit the entire document in its response.</p>
<p id="p9">A few hours ago, LM Studio also <a href="https://x.com/lmstudio/status/1928092450410648000" rel="noopener noreferrer">released</a> a distilled version of R1-0528 &ndash; <a href="https://lmstudio.ai/models/deepseek/deepseek-r1-0528-qwen3-8b" rel="noopener noreferrer">DeepSeek-R1-0528-Qwen3-8B</a>. From their model page:</p>
<blockquote id="blockquote10"><p>
  we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3&nbsp;8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3&nbsp;8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.
</p></blockquote>
<p id="p11">I also attempted to load the full-blown version of R1-0528 in <a href="https://ollama.com" rel="noopener noreferrer">Ollama</a>, my favorite CLI for locally-installed models that is also compatible with <a href="https://github.com/simonw/llm" rel="noopener noreferrer">Simon Willison&rsquo;s LLM CLI</a>. When I looked earlier this morning, only &ldquo;sharded&rdquo; (i.e. multi-part) GGUF versions of the full 685B parameter model were available, specifically <a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF" rel="noopener noreferrer">these by unsloth</a>. That led me down a fascinating rabbit hole that involved learning how to download multiple GGUF open weights and merge them into a unified file that can be used with Ollama on macOS, which I&rsquo;ll document here for posterity (including my future self).</p>
<p id="p12">First, before trying to merge multiple GGUFs, I had to install the <a href="https://github.com/ggml-org/llama.cpp" rel="noopener noreferrer">llama.cpp</a> CLI, and I did so with <code>brew</code>:</p>
<p id="p13"><code>brew install llama.cpp</code></p>
<p id="p14">Since Homebrew doesn&rsquo;t automatically add llama.cpp to your system <code>PATH</code>, I manually made a note of where the dedicated <code>llama-gguf-split</code> utility was located on my Mac, which would be:</p>
<p id="p15"><code>/opt/homebrew/bin/llama-gguf-split</code></p>
<p id="p16">Then, I started downloading the 15 files required for the 8-bit version of R1-0528 I wanted to test &ndash; a 700&nbsp;GB download that took a few hours to complete. After all the files were saved in my Downloads folder, I ran:</p>
<p id="p17"><code>/opt/homebrew/bin/llama-gguf-split --merge DeepSeek-R1-0528-Q8_0-00001-of-00015.gguf outfile.gguf</code></p>
<p id="p18">After waiting several minutes, I ended up with single 713&nbsp;GB file called &lsquo;outfile.gguf&rsquo; that I renamed to &lsquo;DeepSeek-R1-0528-Q8_0&rsquo; and placed in my Home folder. That&rsquo;s the unified model file we need to use in Ollama.</p>
<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-29-at-15-09-43-2x-1748537227138.png" alt="The model file."><p class="image-caption">The model file.</p></div>
<p id="p20">Before doing that, however, I had to create a &ldquo;model file&rdquo; for Ollama to install the model &ndash; essentially, a template that tells Ollama what to load and how to use it. I wasn&rsquo;t familiar with the structure of model files, so to get started, I copied a model file from one of the existing models I already had in Ollama with:</p>
<p id="p21"><code>ollama show --modelfile deepseek-r1:1.5b &gt;&gt; Modelfile</code></p>
<p id="p22">I then grabbed the <code>Modelfile</code> file, opened it in TextEdit, and modified it with the necessary parameters and the local path of my GGUF file:</p>
<pre id="pre23"><code><span class="code-line"># Modelfile generated by "ollama show"
</span><span class="code-line"># To build a new Modelfile based on this, replace FROM with:
</span><span class="code-line"># FROM DeepSeek-R1-0528-Q8_0
</span><span class="code-line">
</span><span class="code-line">FROM /Users/viticci/GGUFs/DeepSeek-R1-0528-Q8_0.gguf
</span><span class="code-line">TEMPLATE """{{- if .System }}{{ .System }}{{ end }}
</span><span class="code-line">{{- range $i, $_ := .Messages }}
</span><span class="code-line">{{- $last := eq (len (slice $.Messages $i)) 1}}
</span><span class="code-line">{{- if eq .Role "user" }}&lt;&#65372;User&#65372;&gt;{{ .Content }}
</span><span class="code-line">{{- else if eq .Role "assistant" }}&lt;&#65372;Assistant&#65372;&gt;
</span><span class="code-line">  {{- if and $.IsThinkSet (and $last .Thinking) -}}
</span><span class="code-line">&lt;think&gt;
</span><span class="code-line">{{ .Thinking }}
</span><span class="code-line">&lt;/think&gt;
</span><span class="code-line">{{- end }}{{ .Content }}{{- if not $last }}&lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;{{- end }}
</span><span class="code-line">{{- end }}
</span><span class="code-line">{{- if and $last (ne .Role "assistant") }}&lt;&#65372;Assistant&#65372;&gt;
</span><span class="code-line">{{- if and $.IsThinkSet (not $.Think) -}}
</span><span class="code-line">&lt;think&gt;
</span><span class="code-line">
</span><span class="code-line">&lt;/think&gt;
</span><span class="code-line">
</span><span class="code-line">{{ end }}
</span><span class="code-line">{{- end -}}
</span><span class="code-line">{{- end }}"""
</span><span class="code-line">PARAMETER stop &lt;&#65372;begin&#9601;of&#9601;sentence&#65372;&gt;
</span><span class="code-line">PARAMETER stop &lt;&#65372;end&#9601;of&#9601;sentence&#65372;&gt;
</span><span class="code-line">PARAMETER stop &lt;&#65372;User&#65372;&gt;
</span><span class="code-line">PARAMETER stop &lt;&#65372;Assistant&#65372;&gt;
</span><span class="code-line">SYSTEM You are a helpful assistant.
</span><span class="code-line">PARAMETER temperature 0.6
</span></code></pre>
<p id="p24">I saved the file, then ran:</p>
<p id="p25"><code>ollama create DeepSeek-R1-0528-Q8_0 -f /Users/viticci/Desktop/Modelfile</code><br>
And waited several minutes for Ollama to gather all the components and install the local model. At the end, Ollama showed a success message:</p>
<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-29-at-15-42-01-2x-1748535066100.png" alt=""><p class="image-caption"></p></div>
<p id="p27">&hellip;but that&rsquo;s when my lucky streak ended: I finally found a local model that couldn&rsquo;t run on my maxed-out, M3 Ultra Mac Studio! As it turns out, the 8-bit, 700&nbsp;GB version of DeepSeek R1 is too big even for this machine. After trying to load the model in memory and going over 400&nbsp;GB of RAM used, Ollama consistently terminated the session:</p>
<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-29-at-15-49-43-2x-1748535119704.png" alt=""><p class="image-caption"></p></div>
<p id="p29">Fair enough &ndash; I imagined that the 8-bit version of R1 was too large for this machine to handle. I went back to the beginning, and downloaded the multi-part, <a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/tree/main/Q4_K_M" rel="noopener noreferrer">4-bit version of DeepSeek R1</a>, which is considerably smaller at &ldquo;only&rdquo; 400&nbsp;GB of storage instead of 700&nbsp;GB. After repeating all of the above steps (with different filenames for this version of the model), I installed the smaller model and ran:</p>
<p id="p30"><code>ollama run DeepSeek-R1-0528-Q4_K_M:latest --verbose</code></p>
<p id="p31">Then waited for the model to load, and it worked.</p>
<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-29-at-19-12-33-2x-1748538780669.png" alt="The 400 GB version of R1-0528 installed in Ollama."><p class="image-caption">The 400&nbsp;GB version of R1-0528 installed in Ollama.</p></div>
<p id="p33">I then asked a simple question:</p>
<blockquote id="blockquote34"><p>
  What is MacStories?
</p></blockquote>
<p id="p35">And this was R1&rsquo;s response, along with its thinking process and statistics about the response itself since I <a href="https://github.com/ollama/ollama/issues/1806" rel="noopener noreferrer">added</a> the <code>--verbose</code> flag:</p>
<pre id="pre36"><code><span class="code-line">&lt;think&gt;Okay, the user asked &ldquo;What is MacStories?&rdquo; This seems like a straightforward informational query about a tech-focused website. Hmm, judging by how basic the question is, they might be completely new to Apple ecosystem coverage or just heard the name somewhere. No complex context here &ndash; probably genuine curiosity without hidden layers. Let me mentally outline key aspects: First clarify it&rsquo;s an independent site (not affiliated with Apple), then highlight their specialization in deep-dive content across all Apple platforms. Should emphasize their unique value proposition like Federico Viticci&rsquo;s legendary iOS reviews and Club MacStories subscription model. Wait &ndash; important to mention they&rsquo;re not just news regurgitators but creators of original workflows and app showcases. The automation tutorials are particularly signature. Oh, and must include that Marco Arment connection since Underflow is historically significant. For tone: Keep it enthusiastic but precise. They deserve props for longevity (founded 2009!) in this volatile niche. User likely wants concise yet comprehensive overview &ndash; no fluff, just meaty facts about their content pillars and influence. Better double-check my memory: Yes, still based in Italy despite global reach. And Club MacStories does have that exclusive automation system they developed&hellip; what was it called again? Actions! Right. Final structure: Definition &rarr; Founder &rarr; Content types &rarr; Business model &rarr; Special sauce (workflow focus). No need for criticism here since user just asked for basics.&lt;/think&gt;
</span><span class="code-line">
</span><span class="code-line">**MacStories** is a well-respected **independent technology website and podcast network**, primarily focused on in-depth coverage of Apple&rsquo;s ecosystem, including:
</span><span class="code-line">
</span><span class="code-line">1. **Apple Hardware &amp; Software:** Detailed reviews, tutorials, and news about iPhones, iPads, Macs, Apple Watch, Apple TV, iOS/iPadOS, macOS, watchOS, tvOS. 
</span><span class="code-line">2. **Third-Party Apps:** Extensive app discovery, reviews (especially for productivity and creative tools), developer interviews, and curation of high-quality software. 
</span><span class="code-line">3. **Workflows &amp; Automation:** A major specialty is exploring how to use Apple devices effectively, particularly through automation using Shortcuts on iOS/iPadOS/Mac and other tools.
</span><span class="code-line">4. **Apple Services:** Coverage of services like iCloud+, Apple Music, Apple TV+, etc.
</span><span class="code-line">
</span><span class="code-line">**Key Distinguishing Features:**
</span><span class="code-line">
</span><span class="code-line">- **Depth &amp; Quality:** Known for exceptionally thorough reviews (especially Federico Viticci&rsquo;s multi-thousand-word iPad Pro reviews) and insightful analysis beyond basic news reporting.
</span><span class="code-line">- **Focus on Productivity &amp; Creativity:** Strong emphasis on how users can leverage technology to get work done, create content, and streamline tasks. Automation is a central theme.
</span><span class="code-line">- **Founder-Driven:** Founded in 2009 by Federico Viticci (Editor-in-Chief), who remains its most prominent voice alongside Managing Editor John Voorhees and other talented writers/editors.
</span><span class="code-line">- **Club MacStories:** A popular subscription service offering exclusive content, including:
</span><span class="code-line">
</span><span class="code-line">    - Weekly newsletters (MacStories Weekly &amp; AppStories Weekly)
</span><span class="code-line">    - In-depth articles and tutorials
</span><span class="code-line">    - Access to a powerful Shortcuts automation library (&ldquo;Actions&rdquo;)
</span><span class="code-line">    - Exclusive podcast episodes (AppStories)
</span><span class="code-line">
</span><span class="code-line">- **Podcasts:** Hosts podcasts like Connected (Federico Viticci, Myke Hurley, Stephen Hackett), MacStories Unwind, and the Club-exclusive AppStories.
</span><span class="code-line">- **Origin of Underflow:** The concept for Marco Arment&rsquo;s influential iOS app discovery platform &ldquo;Underflow&rdquo; originated from an idea first discussed by Federico Viticci on MacStories.
</span><span class="code-line">
</span><span class="code-line">**In essence, MacStories is a go-to resource for Apple enthusiasts, power users, and professionals seeking deep dives into Apple products, apps (especially productivity-focused), automation techniques, and thoughtful perspectives on how to make the most of their technology.** It&rsquo;s known for its high-quality writing, editorial independence, and passionate community.
</span></code></pre>
<p id="p37">And here are the stats:</p>
<ul id="ul38"><li>total duration: 1m46.627582375s</li>
<li>load duration: 30.969542ms</li>
<li>prompt eval count: 15 token(s)</li>
<li>prompt eval duration: 52.491389792s</li>
<li>prompt eval rate: 0.29 tokens/s</li>
<li>eval count: 810 token(s)</li>
<li>eval duration: 54.103737291s</li>
<li>eval rate: 14.97 tokens/s</li>
</ul><p id="p39">As you can see, for an offline model that runs on my computer with ~370&nbsp;GB of RAM, it&rsquo;s quite good &ndash; but there are hallucinations. While R1-0528 got several aspects of MacStories and our team correct, there are some <em>weird</em> things in the response, such as the &ldquo;Marco Arment connection&rdquo; with &ldquo;Underflow&rdquo; (what?), an &ldquo;AppStories Weekly&rdquo; newsletter that does not exist, and a section of our Shortcuts coverage apparently called &ldquo;Actions&rdquo;. None of these are true! But the rest &ndash; including when I founded MacStories, the Club, and editorial values are pretty spot-on. It&rsquo;s always very strange to see the complete chain-of-thought of these models.</p>
<p id="p40">In any case, while it was fun to learn all this and test the full version of R1-0528, I wouldn&rsquo;t recommend doing this unless you truly want to push the limits of an M3 Ultra Mac Studio with 512&nbsp;GB of RAM.</p>
<p id="p41">I&rsquo;m keen to play around with the updated R1 some more, but I&rsquo;ll wait for official versions to show up in Ollama&rsquo;s directory with different sizes and better model files compared to the one I put together this morning. Regardless, the fact that this Mac Studio can run the full version of DeepSeek R1-0528 with slightly less than 400&nbsp;GB of RAM is pretty remarkable, and once again confirms that Apple is <a href="https://www.macstories.net/linked/is-apple-shipping-the-best-ai-computers/" rel="noopener noreferrer">making the best consumer-grade computers</a> for local AI development.</p>
<hr /><h3>Access Extra Content and Perks</h3><p><p>Founded in 2015, <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">Club MacStories</a> has delivered exclusive content every week for nearly a decade.</p>
<p>What started with weekly and monthly email newsletters has blossomed into <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">a family of memberships</a> designed every MacStories fan.</p>
<p><strong><a href="https://club.macstories.net/plans/club" rel="noopener noreferrer">Club MacStories</a></strong>: Weekly and monthly newsletters via email and the web that are brimming with apps, tips, automation workflows, longform writing, early access to the <a href="https://www.macstories.net/unwind/" rel="noopener noreferrer">MacStories Unwind podcast</a>, periodic giveaways, and more;</p>
<p><strong><a href="https://club.macstories.net/plans/plus" rel="noopener noreferrer">Club MacStories+</a></strong>: Everything that Club MacStories offers, plus an active Discord community, advanced search and custom RSS features for exploring the Club&rsquo;s entire back catalog, bonus columns, and dozens of app discounts;</p>
<p><strong><a href="https://club.macstories.net/plans/premier" rel="noopener noreferrer">Club Premier</a></strong>: All of the above <em>and</em> AppStories+, an extended version of our flagship podcast that&rsquo;s delivered early, ad-free, and in high-bitrate audio.</p>
<p>Learn more <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">here</a> and from our <a href="https://club.macstories.net/faq" rel="noopener noreferrer">Club FAQs</a>.</p>
</p><a href='https://club.macstories.net/?utm_source=ms&#038;utm_medium=rss'>Join Now</a>