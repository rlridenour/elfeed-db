<p>More improvements to <code>ollama-buddy</code> <a href="https://github.com/captainflasmr/ollama-buddy">https://github.com/captainflasmr/ollama-buddy</a></p>
<figure><img src="https://emacs.dyerdwelling.family/ox-hugo/20250424085731-emacs--Ollama-Buddy-0-9-35-Grok-Gemini-Integration-Enhanced-Sessions.jpg" width="100%">
</figure>

<h2 id="0-dot-9-dot-38"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-04-29 Tue&gt; </span></span> <strong>0.9.38</strong></h2>
<p>Added model unloading functionality to free system resources</p>
<ul>
<li>Add unload capability for individual models via the model management UI</li>
<li>Create keyboard shortcut (C-c C-u) for quick unloading of all models</li>
<li>Display running model count and unload buttons in model management buffer</li>
</ul>
<p>Large language models consume significant RAM and GPU memory while loaded. Until now, there wasn&rsquo;t an easy way to reclaim these resources without restarting the Ollama server entirely. This new functionality allows you to:</p>
<ul>
<li>Free up GPU memory when you&rsquo;re done with your LLM sessions</li>
<li>Switch between resource-intensive tasks more fluidly</li>
<li>Manage multiple models more efficiently on machines with limited resources</li>
<li>Avoid having to restart the Ollama server just to clear memory</li>
</ul>
<p>There are several ways to unload models with the new functionality:</p>
<ol>
<li>
<p><strong>Unload All Models</strong>: Press <code>C-c C-u</code> to unload all running models at once (with confirmation)</p>
</li>
<li>
<p><strong>Model Management Interface</strong>: Access the model management interface with <code>C-c W</code> where you&rsquo;ll find:</p>
<ul>
<li>A counter showing how many models are currently running</li>
<li>An &ldquo;Unload All&rdquo; button to free all models at once</li>
<li>Individual &ldquo;Unload&rdquo; buttons next to each running model</li>
</ul>
</li>
<li>
<p><strong>Quick Access in Management Buffer</strong>: When in the model management buffer, simply press <code>u</code> to unload all models</p>
</li>
</ol>
<p>The unloading happens asynchronously in the background, with clear status indicators so you can see when the operation completes.</p>
<h2 id="0-dot-9-dot-37"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-04-25 Fri&gt; </span></span> <strong>0.9.37</strong></h2>
<ul>
<li>Display modified parameters in token stats</li>
</ul>
<p>Enhanced the token statistics section to include any modified parameters, providing a clearer insight into the active configurations. This update helps in debugging and understanding the runtime environment.</p>
<h2 id="0-dot-9-dot-36"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-04-25 Fri&gt; </span></span> <strong>0.9.36</strong></h2>
<p>Added Reasoning/Thinking section visibility toggle functionality</p>
<ul>
<li>Introduced the ability to hide reasoning/thinking sections during AI responses, making the chat output cleaner and more focused on final results</li>
<li>Added a new customizable variable <code>ollama-buddy-hide-reasoning</code> (default: nil) which controls visibility of reasoning sections</li>
<li>Added <code>ollama-buddy-reasoning-markers</code> to configure marker pairs that encapsulate reasoning sections (supports multiple formats like &lt;think&gt;&lt;/think&gt; or &mdash;-)</li>
<li>Added <code>ollama-buddy-toggle-reasoning-visibility</code> interactive command to switch visibility on/off</li>
<li>Added keybinding <code>C-c V</code> for toggling reasoning visibility in chat buffer</li>
<li>Added transient menu option &ldquo;V&rdquo; for toggling reasoning visibility</li>
<li>When reasoning is hidden, a status message shows which section is being processed (e.g., &ldquo;Think&hellip;&rdquo; or custom marker names)</li>
<li>Reasoning sections are automatically detected during streaming responses</li>
<li>Header line now indicates when reasoning is hidden with &ldquo;REASONING HIDDEN&rdquo; text</li>
<li>All changes preserve streaming response functionality while providing cleaner output</li>
</ul>
<p>This feature is particularly useful when working with AI models that output their &ldquo;chain of thought&rdquo; or reasoning process before providing the final answer, allowing users to focus on the end results while still having the option to see the full reasoning when needed.</p>