<p>This is the thirteenth post in the <a href="/blog/fastmail-advent-2024/">Fastmail Advent 2024</a> series. The previous post was <a href="/blog/following-the-sun/">Dec 12: Following the Sun</a>. The next post is <a href="/blog/on-call-systems/">Dec 14: On-call systems</a>.</p><p>Ten years ago on December 13, 2014, I talked about how Fastmail had moved its DNS system <a href="/blog/fastmail-dns-hosting/">from TinyDNS to PowerDNS</a>.</p><p>In 2023, we made another big move, switching all our DNS serving from PowerDNS to <a href="https://www.knot-dns.cz/" target="_blank" rel="noopener">Knot DNS</a>. This turned out to be a fairly large change and we ended up going down a couple of different paths before landing on a solid implementation.</p><h2 id="where-were-we-up-to-again" tabindex="-1">Where were we up to again?</h2><p>As a reminder where we <a href="/blog/fastmail-dns-hosting/">left off in 2014</a>, we’d moved DNS serving for our <code>ns[12].messagingengine.com</code> DNS servers from static TinyDNS files to using PowerDNS with its <a href="https://doc.powerdns.com/authoritative/backends/pipe.html" target="_blank" rel="noopener">pipe backend</a> to generate content dynamically on each DNS request via internal logic in code. Modulo some caching, this removed the latency from when you made an update to your domain’s DNS in our UI to when those changes became visible at our DNS servers.</p><p>If you own your own domain and host it at Fastmail, it’s easy to customise the DNS for it. Just go to <a href="https://app.fastmail.com/settings/domains" target="_blank" rel="noopener">Settings -&gt; Domains</a> and click <strong>Edit</strong> next to the domain.</p><p><picture><source type="image/webp" srcset="/assets/images/customise-dns-1-F6B9iDf5KA-375.webp 375w, /assets/images/customise-dns-1-F6B9iDf5KA-750.webp 750w, /assets/images/customise-dns-1-F6B9iDf5KA-1024.webp 1024w" sizes="(max-width: 425px) 375px, 750px"><img alt="Screenshot of domain settings" loading="lazy" decoding="async" src="/assets/images/customise-dns-1-F6B9iDf5KA-375.png" width="1024" height="100" srcset="/assets/images/customise-dns-1-F6B9iDf5KA-375.png 375w, /assets/images/customise-dns-1-F6B9iDf5KA-750.png 750w, /assets/images/customise-dns-1-F6B9iDf5KA-1024.png 1024w" sizes="(max-width: 425px) 375px, 750px"></picture></p><p>Then <strong>Customise DNS</strong>.</p><p><picture><source type="image/webp" srcset="/assets/images/customise-dns-2-q21fuWTaz8-375.webp 375w, /assets/images/customise-dns-2-q21fuWTaz8-750.webp 750w, /assets/images/customise-dns-2-q21fuWTaz8-1024.webp 1024w" sizes="(max-width: 425px) 375px, 750px"><img alt="Screenshot of Customise DNS link" loading="lazy" decoding="async" src="/assets/images/customise-dns-2-q21fuWTaz8-375.png" width="1024" height="80" srcset="/assets/images/customise-dns-2-q21fuWTaz8-375.png 375w, /assets/images/customise-dns-2-q21fuWTaz8-750.png 750w, /assets/images/customise-dns-2-q21fuWTaz8-1024.png 1024w" sizes="(max-width: 425px) 375px, 750px"></picture></p><p>By default we generate a number of records to make using your domain with email easy. We recommend leaving these as is, but you have full control and it’s easy to add or change additional records.</p><p><picture><source type="image/webp" srcset="/assets/images/customise-dns-3-_fxZZelxrZ-375.webp 375w, /assets/images/customise-dns-3-_fxZZelxrZ-750.webp 750w, /assets/images/customise-dns-3-_fxZZelxrZ-1024.webp 1024w" sizes="(max-width: 425px) 375px, 750px"><img alt="Screenshot of custom DNS page" loading="lazy" decoding="async" src="/assets/images/customise-dns-3-_fxZZelxrZ-375.png" width="1024" height="270" srcset="/assets/images/customise-dns-3-_fxZZelxrZ-375.png 375w, /assets/images/customise-dns-3-_fxZZelxrZ-750.png 750w, /assets/images/customise-dns-3-_fxZZelxrZ-1024.png 1024w" sizes="(max-width: 425px) 375px, 750px"></picture></p><pre><code># dig +short a.b.c.uberengineer.com TXT
&quot;txt for a.b.c&quot;
</code></pre><p>While this solved the original latency problem we had, it introduced a few others.</p><p>The pipe backend was computationally considerably more expensive. Under normal DNS load this wasn’t a problem and the system was scaled appropriately to handle it just fine. However if we got hit with excessive load mostly due to some form of <a href="https://en.wikipedia.org/wiki/Denial-of-service_attack#Distributed_DoS_attack" target="_blank" rel="noopener">DDoS</a> attack, the system could easily come under strain and start to fail. When this happens people can’t access our website reliably, or our IMAP/POP/SMTP servers, and worst of all other sites might have problems working out which servers to deliver email for Fastmail customers to.</p><p>To protect our servers from DDoS attacks, we had put them behind <a href="https://developers.cloudflare.com/dns/dns-firewall/" target="_blank" rel="noopener">Cloudflare’s DNS firewall</a> product, which is a global distributed DNS cache.</p><p>The Cloudflare DNS firewall product works great at edge caching if there’s a flood of DNS queries to a particular domain (or small set of) domain names and solved most of the DDoS flooding issues we saw. However we experienced cases where we were flooded with DNS queries of the form <code>$randomdomain.fastmail.com</code> (called a <a href="https://developers.cloudflare.com/dns/dns-firewall/random-prefix-attacks/about/" target="_blank" rel="noopener">pseudo random prefix attack</a> or random subdomain attack). If every DNS query is to a random different sub-domain, then Cloudflare has to pass all those queries straight through to us as it has nothing cached. In theory Cloudflare say they can mitigate this. Unfortunately we felt their mitigation didn’t work particularly well and still resulted in a significant overload of incoming DNS queries.</p><p>Again, this flood of queries could cause an overload of the PowerDNS pipe backend processes which caused visible DNS downtime. We couldn’t find any sensible tuning that would allow PowerDNS and our pipe backend to correctly operate under one of these sustained floods.</p><h2 id="mitigating-random-prefix-attacks" tabindex="-1">Mitigating random prefix attacks</h2><p>At the time we needed a quick solution to this problem. So we ended up splitting our DNS in two. We noted that basically all the random prefix attacks were against our system domains like fastmail.com and not user domains. Since DNS for our system domains doesn’t change much at all we basically backtracked and put our system domains onto separate nameservers that ran TinyDNS using a mostly static database of DNS records. Although this felt hacky, it worked. TinyDNS was able to absorb the higher query load in these attack situations quite well.</p><p>This however complicated our DNS setup even more. We also knew if a user domain experienced one of these attacks, it wasn’t using the TinyDNS system. Obviously it was possible an attack could take out not just the one user domain, but <em>all</em> user domains using our DNS if it overloaded the PowerDNS server. We wanted a simpler, better, and more permanent solution.</p><h2 id="replacing-all-dns-with-knot-dns-server" tabindex="-1">Replacing all DNS with Knot DNS server</h2><p>So the main observation about DNS is that in general it doesn’t actually change that often. So what we wanted was a solution where each of the 100,000’s of domains in our system represents a DNS zone that can be individually built into a static database, and each zone can be added/updated/deleted separately without having to reload/rebuild all zones.</p><p>We looked around at a few servers and went with <a href="https://www.knot-dns.cz/" target="_blank" rel="noopener">Knot DNS</a>.</p><ul> <li>It looks like a “standard” DNS server with zone files, replication, etc, so it’s easier for new staff to understand</li> <li>A single zone can be easily added/updated/deleted at runtime into its live database</li> <li>It’s a known <a href="https://www.knot-dns.cz/benchmark/" target="_blank" rel="noopener">high performance server out of the box</a></li> </ul><p>In theory then what we want isn’t actually that hard:</p><ul> <li>Whenever a domain is added/updated/deleted, we create/replace/remove a zone file for that domain</li> <li>We tell the DNS server to add/update/drop the corresponding zone</li> </ul><p>The devil turned out to be in many small details and mis-adventures.</p><h2 id="lets-not-unknot-the-knot" tabindex="-1">Lets not unknot the knot</h2><p>First things first. Although Knot as a DNS server works great, we’ve found its name to be a bit annoying. It’s a play on the fact that the most common DNS server on the internet is <a href="https://www.isc.org/bind/" target="_blank" rel="noopener">bind</a> so… knot. Unfortunately, you’ll find yourself at some point saying something <a href="https://www.youtube.com/watch?v=8H1u-zh9dmU#t=0m53" target="_blank" rel="noopener">Bernard Woolley-esque</a> like “it was not obvious that it did not work because knot was not bound to the knot ips”. Which is easier to understand when you read it compared to when you say it. We keep trying to keep discussions sane by explicitly saying “ka-not” whenever we refer to the server/software.</p><h2 id="detecting-zone-changes" tabindex="-1">Detecting zone changes</h2><p>There were two main ways we could do this:</p><ol> <li>Catch everywhere in application code we insert/update/delete records for the <code>Domains</code>, <code>CustomDNS</code> or <code>DKIMRecords</code> tables</li> <li>Use DB triggers to do the same thing</li> </ol><p>We ended up going with (2) because it felt like the right choice for rock solid data reliability.</p><p>We update the DB for domains related changes in a number of places, from code, to scripts, to cron jobs. Each of those might need DNS zones to be updated and if we miss something unexpected now or in the future, it might create subtle bugs such as “domain DNS didn’t get updated” or worse, “domain SOA serial number didn’t get bumped and so the Knot replica server didn’t get the updated domain data, so we’re serving different DNS from two different nameservers”.</p><p>These are the sorts of problems that could cause really hard to debug customer issues, that then randomly disappear when the domain gets touched in some other way and everything gets updated correctly again, making them extremely hard to track down and debug.</p><p>DNS is already hard enough for many customers to understand. We wanted to be 100% sure that our DNS servers are rock solid and the data they are generating is completely consistent with what users see in our UI.</p><p>Now actually getting the triggers working turned out to have a number of issues and unexpected edge cases, but also ended up with some nice results.</p><ol> <li>We moved the logic that bumps an SOA serial number into the trigger. Originally this was done on the “active primary” server, and we had to make sure this happened before the non-active failover primary rebuilt the zone. By doing this in the trigger, we ensure that the serial can never be out of date after a change.</li> <li>We ended up with a nice design to track changed domains.</li> </ol><p>We have a <code>KnotDomainsChanged</code> table that looks like:</p><pre><code>+--------------+--------------+------+-----+---------+-------+
| Field        | Type         | Null | Key | Default | Extra |
+--------------+--------------+------+-----+---------+-------+
| Server       | varchar(255) | NO   | PRI | NULL    |       |
| DomainId     | int          | NO   | PRI | NULL    |       |
| Domain       | varchar(255) | YES  |     | NULL    |       |
| NeedsRebuild | tinyint      | YES  |     | 1       |       |
| ErrorCount   | int          | YES  |     | 0       |       |
+--------------+--------------+------+-----+---------+-------+
</code></pre><p>There is another table <code>KnotServers</code> that has a list of all currently running Knot servers. Whenever a domain is added/updated/deleted, a trigger executes this query:</p><pre><code>    INSERT INTO KnotDomainsChanged (Server, DomainId, Domain)
    SELECT Server, BumpDomainId, BumpDomain
    FROM KnotServers
    ON DUPLICATE KEY UPDATE
      NeedsRebuild = 1,
      ErrorCount = 0;
</code></pre><p>With this, the <code>KnotDomainsChanged</code> table effectively maintains a set of changed domains for each primary Knot server to pick up and build. By using <code>(Server, DomainId)</code> as the primary key, we ensure this table can’t grow without bound even if a primary Knot server is down for a while.</p><p>The <code>NeedsRebuild</code> flag allows us to correctly rebuild a zone without a race condition. The process for keeping zones up-to-date is to effectively run the following in an infinite loop.</p><ul> <li>Fetch and iterate over all <code>KnotDomainsChanged</code> records for this Server <ul> <li>Set <code>NeedsRebuild = 0</code> for this <code>(Server, DomainId)</code> record</li> <li>If the domain exists in the <code>Domains</code> table, add/rebuild the zone into Knot</li> <li>If the domain does not exist in the <code>Domains</code> table, purge the zone from Knot</li> <li>Delete the <code>(Server, DomainId)</code> record from <code>KnotDomainsChanged</code> iff <code>NeedsRebuild = 0</code></li> </ul> </li> </ul><p>So if the domain changes while a rebuild is in progress, the trigger will set <code>NeedsRebuild = 1</code>, which means it won’t be deleted from the table after the zone build finishes, which means it’ll be picked up again the next time the sync runs again in a few seconds. This avoids the race if the domain is changed while it is in the process of being rebuilt.</p><h2 id="using-standard-dns-axfr-ixfr-for-replication" tabindex="-1">Using standard DNS AXFR/IXFR for replication</h2><p>Our initial plan was to use a more “standard” DNS setup. We would have an internal hidden primary server and a number of secondary servers that pull from the primary via <a href="https://datatracker.ietf.org/doc/html/rfc5936" target="_blank" rel="noopener">AXFR</a>/IXFR. Only the secondary servers would handle DNS queries from the world.</p><p>This turned out to have a number of annoying edge cases:</p><ol> <li>It required two completely separate Knot server setups with completely separate and quite different configurations. Although this is a more “standard” DNS management approach, it reduced some of benefit of moving to a single DNS server.</li> <li>Using AXFR for updates caused unexpected problems. AXFR connections aren’t reused, so every domain transferred required a separate TCP connection. When a lot of domains needed to be updated at once, we <a href="https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux" target="_blank" rel="noopener">ran out of TCP socket tuples because of sockets in TIMEWAIT state</a>. This caused AXFRs to the secondaries to start failing. We fixed this by enabling the kernel <code>tcp_tw_reuse</code> tunable, but it felt… hacky.</li> <li>At Fastmail, whenever we have a singleton service (e.g. Knot primary), we want to make sure that we can take the machine it’s running on down safely. To allow that we have the service run on at least two separate servers and use a failover IP to bind to the current up/active server.</li> </ol><p>Unfortunately this combined with the way Knot does catalog zones completely broke AXFR/IXFR replication. What is a <a href="https://datatracker.ietf.org/doc/rfc9432/" target="_blank" rel="noopener">catalog zone</a>? It’s a standard way to allow primary and secondary DNS servers to keep the complete list of zones actually managed in sync.</p><pre><code>   The content of a DNS zone is synchronized among its primary and
   secondary nameservers using AXFR and IXFR.  However, the list of
   zones served by the primary (called a catalog in [RFC1035]) is not
   automatically synchronized with the secondaries.  To add or remove a
   zone, the administrator of a DNS nameserver farm not only has to add
   or remove the zone from the primary, they must also add/remove
   configuration for the zone from all secondaries.  This can be both
   inconvenient and error-prone; in addition, the steps required are
   dependent on the nameserver implementation.
</code></pre><p>It’s a classic example of taking a system that already has a way of storing data (DNS records) and replicating that data (AXFR/IXFR), and reusing those mechanisms to sync something else. In this case, a specially configured catalog zone that itself contains a list of all other member zones managed by the server in a standard defined format.</p><p>The basic format of the catalog zone is that each member zone managed by the server exists in the catalog zone as the RDATA of a PTR record. Then because each PTR record needs to be a unique domain name, a unique identifier is used as a sub-domain of the catalog zone itself. e.g. <code>unique-N.catalog.zone. PTR member-domain.org.</code></p><p>The problem here is that the unique identifiers are not generated in a consistent way if you have multiple different primary servers!</p><pre><code>k1: 8815a670d8fa9032.zones.catalog.dns.internal. 0        PTR     uberengineer.com.
k2: 09b6b01fb75e81e6.zones.catalog.dns.internal. 0        PTR     uberengineer.com.
</code></pre><p>During testing k1 was the entry on our first server (e.g. active primary), the k2 on the second server (e.g. backup primary).</p><p>So when we did a failover from the current active primary server to promote the backup primary to the active primary, the catalog zone on the new active primary is completely out of sync with the catalog zone on all the downstream secondary servers. This caused a massive amount of resyncing and general Knot confusion. There didn’t seem to be an easy solution to this problem without ultimately having some singleton source of truth server, which is what we wanted to avoid.</p><h2 id="switching-to-a-single-server-type" tabindex="-1">Switching to a single server type</h2><p>After this, we decided to dump the whole Knot primary/secondary system and AXFR/IXFR replication, and instead have just a single type of Knot server. This ended up having a number of advantages.</p><ol> <li>Only one type of Knot server and one type of Knot server configuration. Less configurations to understand.</li> <li>No catalog zone. Less concepts to learn about.</li> <li>No AXFR/IXFR. AXFR/IXFR is harder to reason about and less visible to operators.</li> <li>No worry about zone serial numbers going backwards if you add -&gt; update -&gt; delete -&gt; re-add a domain which can cause AXFR/IXFR replication weirdness.</li> </ol><p>Doing this definitely felt like a better solution. Additionally it already all “just worked” because we had already built the trigger system that could keep an arbitrary number of primary servers (originally for failover) up-to-date. Now it was just keeping all our Knot instances up to date.</p><h2 id="testing-the-new-system" tabindex="-1">Testing the new system</h2><p>Since DNS is so critical and we have 100,000’s of domains, we wanted to test as carefully as possible that the new Knot system would generate the same results as the existing PowerDNS system.</p><p>The <code>Net::Pcap</code>, <code>Net::Frame</code> and <code>Net::DNS</code> modules in perl made this straight forward. We were able to write a script that captured packets with libpcap, unpacked them into DNS queries and responses, and then replayed them against the new Knot servers to compare the results. This allowed us to see with real world query data that we would get back the same responses.</p><pre><code>my $resolver = Net::DNS::Resolver-&gt;new(nameservers =&gt; [ '...existing DNS ip...' ]);
... libpcap setup ...
my $link_class;
if ($linktype == Net::Pcap::DLT_EN10MB) {
  $link_class = 'Net::Frame::Layer::ETH';
} elsif ($linktype == Net::Pcap::DLT_LINUX_SLL) {
  $link_class = 'Net::Frame::Layer::SLL';
} else {
  die &quot;unknown link layer: $linktype\n&quot;;
}
... run libpcap loop ...
sub process_packet ($user_data, $header, $packet) {
  my $p_link = $link_class-&gt;new(raw =&gt; $packet);
  $p_link-&gt;unpack;
  my $p_ip4 = Net::Frame::Layer::IPv4-&gt;new(raw =&gt; $p_link-&gt;payload);
  $p_ip4-&gt;unpack;
  my $p_udp = Net::Frame::Layer::UDP-&gt;new(raw =&gt; $p_ip4-&gt;payload);
  $p_udp-&gt;unpack;
  my $p_dns = Net::DNS::Packet-&gt;decode( \$p_udp-&gt;payload );

  if (my @a = $p_dns-&gt;answer) {
    my @q = $p_dns-&gt;question;
    my $q = $q[0];

    my $dns_q = Net::DNS::Packet-&gt;new();
    $dns_q-&gt;push(question =&gt; $q);
    my $kres = $resolver-&gt;send($dns_q);
    my @ka = $kres-&gt;answer;

... compare @a (pdns answer) vs @ka (knot answer) modulo some known differences ...
</code></pre><p>Mostly it showed that everything was working as expected, though there were a few interesting edge cases that ended up needing to be dealt with.</p><h2 id="non-terminal-nodes-and-wildcards" tabindex="-1">Non-terminal nodes and wildcards</h2><p>The biggest subtle difference we discovered was around non-terminal nodes and wildcards.</p><p>This is subtly documented in the tinydns <a href="http://cr.yp.to/djbdns/axfr-get.html" target="_blank" rel="noopener">axfr-get</a> program:</p><blockquote> <p>axfr-get does not precisely simulate BIND’s handling of <code>*.dom</code>. Under BIND, records for <code>*.dom</code> do not apply to <code>y.dom</code> or <code>anything.y.dom</code> if there is a normal record for <code>x.y.dom</code>. With axfr-get and tinydns, the records apply to <code>y.dom</code> and <code>anything.y.dom</code> except <code>x.y.dom</code>.</p> </blockquote><p>Knot DNS follows the traditional BIND and <a href="https://datatracker.ietf.org/doc/html/rfc1034#section-4.3.3" target="_blank" rel="noopener">RFC 1034</a> intepretation of wildcard records. Our PowerDNS backend was built to follow the TinyDNS model because that’s what we were migrating from at the time. This can cause subtle differences in DNS results if you have any wildcard domains.</p><p>An example. If you have a domain <code>example.com</code> setup at Fastmail with our standard DNS configuration, then we add default A records for <code>*.example.com</code>. If you add a DNS record for the subdomain <code>*.foo</code> that creates a record for <code>*.foo.example.com</code>. However that implied existence of <code>foo.example.com</code> means that the <code>*.example.com</code> record no longer exists for <code>foo.example.com</code>. At least, that’s true in the Knot/Bind DNS implementation, but not true in our PowerDNS backend implementation, so this will get you subtly different results.</p><p>In quite a few cases this won’t be a problem. We see queries like <code>_domainkey.example.com A</code>, which under PowerDNS return an IP, but won’t under knot, but that’s fine. Nothing should really be using that IP anyway, it’s probably just some gateway device somewhere doing DNS querying of passively seen domains in email headers or the like. But in some cases it might be.</p><p>We initially thought we could fix this automatically. For everyone that has a <code>foo.bar.example.com</code> subdomain without a <code>bar.example.com</code> subdomain, if there’s any wildcard <code>*.example.com</code> records, we make copies of them at <code>bar.example.com</code>. This should make everything “just work”.</p><p>The problem is, there’s actually a deeper problem that affects basically every domain and every single subdomain in a subtle way. As noted, by default we publish <code>*.example.com</code> records. However these effectively work for all sub-sub domains as well. For example I have a domain <code>uberengineer.com</code> and the standard <code>*.uberengineer.com</code> wildcard means that all sub-domains, sub-sub-domains, etc resolve:</p><pre><code># dig +short that.uberengineer.com
103.168.172.37
103.168.172.52
# dig +short this.that.uberengineer.com
103.168.172.37
103.168.172.52
</code></pre><p>Now I have a TXT record at <code>test.uberengineer.com</code>, so that hides the A records</p><pre><code># dig +short test.uberengineer.com
#
</code></pre><p>But in our PowerDNS implementation, that doesn’t hide sub-domains of <code>test.uberengineer.com</code> from the original wildcard.</p><pre><code># dig +short foo.test.uberengineer.com
103.168.172.37
103.168.172.52
</code></pre><p>But with Knot, it does:</p><pre><code># dig +short foo.test.uberengineer.com @knottest.internal
#
</code></pre><p>So basically to just fix users automatically, for every single subdomain X they have configured, if there’s a wildcard at a lower level, we’d have to explicitly create a copy of all the lower level wildcard records at <code>*.X</code> as well. This was going to be just way too much magic, especially for a rare edge case that it’s possible no one was even relying on anyway!</p><p>In the end, we analysed the DNS queries we were seeing and also all the email deliveries we saw. We can looked at the email logs on our MX servers for any RCPT TO address, and then checked if the domain that was delivered to is hosted by us, and compared if it would resolve under Knot as well. Combining the data convinced us that no one was going to be actively affected by this change.</p><h2 id="conclusion" tabindex="-1">Conclusion</h2><p>This has now been running for over a year in production and has been working extremely well. We were able to remove two existing systems and replace them with a single consistent Knot DNS based system for all our Fastmail domains and 100,000’s of user domains. The triggers that track zones that need rebuilding work reliably and consistently. The new system performs enormously better than the existing PowerDNS pipe backend based system. We can easily scale it to add additional servers if needed.</p><p>This all fits with a mantra we’ve been working with recently, “fewer better ways”. We’ve been running an email service for over 25 years and it’s easy to accumulate a plethora of different services and systems with varying levels of polish and performance. Revisiting what you’re doing and running to try and consolidate to a smaller number of systems working in better ways can reduce long term debt. This makes systems more reliable, easier to manage, and also allows new staff to understand them more quickly as well.</p>