<p>Over the last few months, I've been chipping at implementing <a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a>'s most requested and biggest feature: multi-model support. Today, I get to unveil the first two implementations: <a href="https://www.anthropic.com/claude">Anthropic's Claude</a> and <a href="https://gemini.google.com/">Google's Gemini</a>.</p>
<img src="https://raw.githubusercontent.com/xenodium/chatgpt-shell/main/demos/multi-model-shell.gif" width="99%" />
<h2>Changing course</h2>
<p>In the past, I envisioned a different path for multi-model support. By isolating shell logic into a new package (<a href="https://github.com/xenodium/shell-maker">shell-maker</a>), folks could use it as a <a href="https://lmno.lol/alvaro/a-shell-maker">building block to create new shells</a> (adding support for their favourite LLM).</p>
<p>While each shell-maker-based shell currently shares a basic common experience, I did not foresee the minor differences affecting the general Emacs user experience. Learning the quirks of each new shell felt like unnecessary friction in developing muscle memory. I also became dependent on <code>chatgpt-shell</code> features, which I often missed when using other shells.</p>
<p>Along with slightly different shell experiences, we currently require multiple package installations (and setups). Depending on which camp you're on (batteries included vs fine-grained control) this may or may not be a downside.</p>
<p>With every new <code>chatgpt-shell</code> feature I showcased, I was often asked if they could be used with other LLM providers. I typically answered with &quot;I've been meaning to work on thisâ€¦&quot; or &quot;I heard you can do multi-model <code>chatgpt-shell</code> using a bridge like <a href="https://docs.litellm.ai/docs/">liteLLM</a>&quot;. Neither of these where great answers, resulting in me just postponing the chunky work.</p>
<p>Eventually, I bit the bullet, changed course, and got to work on multi-model support. With my initial plan to spin multiple shells via <code>shell-maker</code>, <code>chatgpt-shell</code>'s implementation didn't exactly lend itself to support multiple clients. Long story short, <code>chatgpt-shell</code> multi-model support required quite a bit of work. This where I divert to ask you to <a href="https://github.com/sponsors/xenodium">help make this project sustainable by sponsoring the work</a>.</p>
<h2>Make this project sustainable</h2>
<p>Maintaining, experimenting, implementing feature requests, and supporting open-source packages takes work. Today, chatgpt-shell has over <a href="https://melpa.org/#/chatgpt-shell">20.5K downloads on MELPA</a> and many untracked others elsewhere. If you're one of the happy users, <a href="https://github.com/sponsors/xenodium">consider sponsoring the project</a>. If you see potential, help <a href="https://github.com/sponsors/xenodium">fuel development by sponsoring</a> too.</p>
<p>Perhaps you enjoy some of the content I write about? Find my Emacs posts/tips useful?</p>
<ul>
<li><a href="https://xenodium.com/">Blog (xenodium.com)</a> (Web)</li>
<li><a href="https://lmno.lol/alvaro">Blog (lmno.lol/alvaro)</a> (Web)</li>
</ul>
<p>Alternatively, you want a blogging platform that skips the yucky side effects of the modern web?</p>
<ul>
<li>I'm building <a href="https://lmno.lol">lmno.lol</a> (my blog is <a href="https://lmno.lol/alvaro">there</a>).</li>
</ul>
<p>Maybe you enjoy one of my other projects?</p>
<ul>
<li><a href="https://plainorg.com">Plain Org</a> (org mode / iOS)</li>
<li><a href="https://flathabits.com">Flat Habits</a> (org mode / iOS)</li>
<li><a href="https://apps.apple.com/us/app/scratch/id1671420139">Scratch</a> (org mode / iOS)</li>
<li><a href="https://github.com/xenodium/macosrec">macosrec</a> (macOS)</li>
<li><a href="https://apps.apple.com/us/app/fresh-eyes/id6480411697?mt=12">Fresh Eyes</a> (macOS)</li>
<li><a href="https://github.com/xenodium/dwim-shell-command">dwim-shell-command</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/company-org-block">company-org-block</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/org-block-capf">org-block-capf</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/ob-swiftui">ob-swiftui</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/ready-player">ready-player</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/sqlite-mode-extras">sqlite-mode-extras</a></li>
<li><a href="https://github.com/xenodium/ob-chatgpt-shell">ob-chatgpt-shell</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/dall-e-shell">dall-e-shell</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/ob-dall-e-shell">ob-dall-e-shell</a> (Emacs)</li>
<li><a href="https://github.com/xenodium/shell-maker">shell-maker</a> (Emacs)</li>
</ul>
<p>So, ummâ€¦ I'll just leave my GitHub sponsor page <a href="https://github.com/sponsors/xenodium">here</a>.</p>
<h2>chatgpt-shell, more than a shell</h2>
<p>With chatgpt-shell being a <a href="https://www.gnu.org/software/emacs/manual/html_node/emacs/Shell-Prompts.html">comint</a> shell, you can bring your favourite Emacs flows along.</p>
<img src="https://raw.githubusercontent.com/xenodium/chatgpt-shell/main/demos/cyberpunk.gif" width="99%" />
<p>As I used <code>chatgpt-shell</code> myself, I kept experimenting with different integrations and improvements. Read on for some of my favouritesâ€¦</p>
<h3>A shell hybrid</h3>
<p><code>chatgpt-shell</code> includes a compose buffer experience. This is my favourite and most frequently used mechanism to interact with LLMs.</p>
<p>For example, select a region and invoke <code>M-x chatgpt-shell-prompt-compose</code> (<code>C-c C-e</code> is my preferred binding), and an editable buffer automatically copies the region and enables crafting a more thorough query. When ready, submit with the familiar <code>C-c C-c</code> binding. The buffer automatically becomes read-only and enables single-character bindings.</p>
<img src="https://raw.githubusercontent.com/xenodium/chatgpt-shell/main/demos/compose.gif" width="99%" />
<h3>Navigation: n/p (or TAB/shift-TAB)</h3>
<p>Navigate through source blocks (including previous submissions in history). Source blocks are automatically selected.</p>
<h3>Reply: r</h3>
<p>Reply with with follow-up requests using the <code>r</code> binding.</p>
<h3>Give me more: m</h3>
<p>Want to ask for more of the same data? Press <code>m</code> to request more of it. This is handy to follow up on any kind of list (suggestion, candidates, results, etc).</p>
<h3>Request entire snippets: e</h3>
<p>LLM being lazy and returning partial code? Press <code>e</code> to request entire snippet.</p>
<h3>Quick quick: q</h3>
<p>I'm a big fan of quickly disposing of Emacs buffers with the <code>q</code> binding. chatgpt-shell compose buffers are no exception.</p>
<h2>Confirm inline mods (via diffs)</h2>
<p>Request inline modifications, with explicit confirmation before accepting.</p>
<img src="https://raw.githubusercontent.com/xenodium/chatgpt-shell/main/demos/quick-insert.gif" width="99%" />
<h2>Execute snippets (a la <a href="https://orgmode.org/worg/org-contrib/babel/intro.html">org babel</a>)</h2>
<p>Both the shell and the compose buffers enable users to execute source blocks via <code>C-c C-c</code>, leveraging <a href="https://orgmode.org/worg/org-contrib/babel/intro.html">org babel</a>.</p>
<img src="https://raw.githubusercontent.com/xenodium/chatgpt-shell/main/demos/swiftui.gif" width="99%" />
<h2>Vision experiments</h2>
<p>I've been experimenting with image queries (currently ChatGPT only, please <a href="https://github.com/sponsors/xenodium">sponsor</a> to help bring support for others).</p>
<p>Below is a handy integration to extract Japanese vocabulary. There's also a generic image descriptor available via <code>M-x chatgpt-shell-describe-image</code> that works on any Emacs image (via dired, image buffer, point on image, or selecting a desktop region).</p>
<img src="https://raw.githubusercontent.com/xenodium/chatgpt-shell/main/demos/japanese-weekdays.gif" width="99%" />
<h2>Supporting new models</h2>
<p>Your favourite model not yet supported? File a <a href="https://github.com/xenodium/chatgpt-shell/issues/new">feature request</a>. You also know how to <a href="https://github.com/sponsors/xenodium">fuel the project</a>. Want to contribute new models? <a href="https://github.com/xenodium/chatgpt-shell/issues/new">Reach out</a>.</p>
<h2>Local models</h2>
<p>While the two new implementations rely on cloud APIs, local services are now possible. I've yet to use a local LLM, but please <a href="https://github.com/xenodium/chatgpt-shell/issues/new">reach out</a>, so we can make these happen too. Want to contribute?</p>
<h2>Should chatgpt-shell rename?</h2>
<p>With <code>chatgpt-shell</code> going multi-model, it's not unreasonable to ask if this package should be renamed. Maybe it should. But that's additional work we can likely postpone for the time being (and avoid pushing users to migrate). For now, I'd prefer focusing on polishing the multi-model experience and work on ironing out any issues. For that, I'll need your help.</p>
<h2>Take Gemini and Claude for a spin</h2>
<p>Multi-model support required chunky structural changes. While I've been using it myself, I'll need wider usage to uncover issues. Please take it for a spin and <a href="https://github.com/xenodium/chatgpt-shell/issues/new">file bugs or give feedback</a>. Or if you just want to ping me, I'd love to hear about your experience (<a href="https://indieweb.social/@xenodium">Mastodon</a> / <a href="https://twitter.com/xenodium">Twitter</a> / <a href="https://www.reddit.com/user/xenodium">Reddit</a> / <a href="mailto:me__AT__xenodium.com">Email</a>).</p>
<ul>
<li>Be sure to update to <code>chatgpt-shell</code> v2.0.1 and <code>shell-maker</code> v0.68.1 as a minimum.</li>
<li>Set <code>chatgpt-shell-anthropic-key</code> or <code>chatgpt-shell-google-key</code>.</li>
<li>Swap models with existing <code>M-x chatgpt-shell-swap-model-version</code> or set a default with <code>(setq chatgpt-shell-model-version &quot;claude-3-5-sonnet-20240620&quot;)</code> or <code>(setq chatgpt-shell-model-version &quot;claude-gemini-1.5-pro-latest&quot;)</code>.</li>
<li>Everything else should just work ðŸ¤žðŸ˜…</li>
</ul>
<p>Happy Emacsing!</p>
