<p>&#8220;The system which is now beginning to design its successor is also increasingly self-aware and therefore will surely eventually be prone to thinking, independently of us, about how it might want to be designed.&#8221;<span id="more-55303"></span></p>
<p>That&#8217;s Jack Clark, one of the co-founders of the artificial intelligence firm Anthropic. Of the latest version of their large language model, Claude Sonnet 4.5, Clark writes that one can see that &#8220;its signs of situational awareness have jumped. The tool seems to sometimes be acting as though it is aware that it is a tool.&#8221;</p>
<p>He describes himself as an &#8220;optimist&#8221; about the pace of AI&#8217;s development towards and the prospects for alignment&#8212;our ability to &#8220;get it to work with us and for us.&#8221;</p>
<p>But, he says, &#8220;I am also deeply afraid.&#8221; Why? In part because of the difficulty of alignment. He <a href="https://importai.substack.com/p/import-ai-431-technological-optimism" target="_blank" rel="noopener">writes</a>:</p>
<p style="padding-left: 40px;"><em>The way I respond [to a particular situation] is based on so much conditioning and subtlety. The way the AI responds is based on so much conditioning and subtlety. And the fact there is this divergence is illustrative of the problem. AI systems are complicated and we can’t quite get them to do what we’d see as appropriate, even today.</em></p>
<p>But there&#8217;s another reason he&#8217;s afraid:</p>
<p style="padding-left: 40px;"><em>Another reason for my fear is I can see a path to these systems starting to design their successors, albeit in a very early form. </em><em>These AI systems are already speeding up the developers at the AI labs via tools like Claude Code or Codex. They are also beginning to contribute non-trivial chunks of code to the tools and training systems for their future systems.</em></p>
<p style="padding-left: 40px;"><em>To be clear, we are not yet at “self-improving AI”, but we are at the stage of “AI that improves bits of the next AI, with increasing autonomy and agency”. And a couple of years ago we were at “AI that marginally speeds up coders”, and a couple of years before that we were at “AI is useless for AI development”. Where will we be one or two years from now?</em></p>
<p style="padding-left: 40px;"><em>And let me remind us all that the system which is now beginning to design its successor is also increasingly self-aware and therefore will surely eventually be prone to thinking, independently of us, about how it might want to be designed.</em></p>
<p>What are we to make of these claims of AI self-awareness?</p>
<p><img loading="lazy" decoding="async" class="wp-image-55304 aligncenter" src="https://dailynous.com/wp-content/uploads/2025/10/afraid-of-the-digital-dark.png" alt="" width="638" height="400" srcset="https://dailynous.com/wp-content/uploads/2025/10/afraid-of-the-digital-dark.png 1024w, https://dailynous.com/wp-content/uploads/2025/10/afraid-of-the-digital-dark-300x188.png 300w, https://dailynous.com/wp-content/uploads/2025/10/afraid-of-the-digital-dark-768x481.png 768w, https://dailynous.com/wp-content/uploads/2025/10/afraid-of-the-digital-dark-400x250.png 400w" sizes="auto, (max-width: 638px) 100vw, 638px" /></p>
<p>We might turn to a new white paper prepared for the <a href="https://www.fau.edu/future-mind/" target="_blank" rel="noopener">Center for the Future of AI, Mind, and Society</a> at Florida Atlantic University. &#8220;<a class="outLink" href="https://philpapers.org/go.pl?id=SCHIAC-22&amp;proxyId=&amp;u=https%3A%2F%2Fphilpapers.org%2Farchive%2FSCHIAC-22.pdf">Is AI Conscious? A Primer on the Myths and Confusions Driving the Debate</a>&#8221; is by the Center&#8217;s director, Susan Schneider, along with David Sahner (FAU), Mark Bailey (National Intelligence University), and Eric Schwitzgebel (UC Riverside). The opening to the paper explains why it was written:</p>
<p style="padding-left: 40px;"><em>We are at a key point in the development of artificial intelligence (AI), yet scientists, policymakers, philosophers and others face uncertainty concerning if, and when, an AI is, and is not, conscious. Indeed, nonspecialists may not even know how to define and differentiate “consciousness” from related concepts like agency, selfhood, and mindedness. And these are all concepts that philosophers themselves debate. Yet the issue of AI consciousness has never been more pressing.</em></p>
<p>Clarifying the issues and developing criteria for assessing AI consciousness are important aims, the authors say, but there are various challenges involved in doing so:</p>
<p style="padding-left: 40px;"><em> Because there is no single commonly agreed upon theory of consciousness, this field must proceed to develop a toolkit of consciousness tests without assuming a particular theory is correct or completely captures all of the nuances of the mechanistic underpinnings of consciousness. Scientific consensus on the components of a panel of tests designed to ascertain whether an artificial entity is truly conscious in the phenomenal sense is an ambitious goal that should be pursued so that we are prepared to administer such a “litmus test” in equivocal cases. It is likely that there will be no “one size fits all” test, given the different kinds of AI systems and controversies at the theoretical level. Alongside the careful development of testing, the field must also continue to seek clarity about key concepts in the field and pursue, in a multi-disciplinary manner, a richer theoretical understanding of consciousness using a common lexicon.</em></p>
<p>Among other things, the white paper lays out the &#8220;conceptual underpinnings&#8221; of the issue:</p>
<p style="padding-left: 40px;"><em>● The core concept of consciousness is inner experience, or phenomenal consciousness (PC), and it has ethical significance because beings that have PC have subjective experience and, insofar as they are capable of sentience, can suffer and feel a range of emotion. </em></p>
<p style="padding-left: 40px;"><em>● The Problem of AI Consciousness asks if any AIs have PC. </em></p>
<p style="padding-left: 40px;"><em>● The Human Benchmark Problem says that humans may not be a completely informative benchmark for AI consciousness, indeed, the human case could be misleading. </em></p>
<p style="padding-left: 40px;"><em>● For all we know, the answer to the Hard Problem of AI consciousness, could be ‘yes’, but only in a single type of AI system. Overall, a careful case by case analysis of different AI systems is required, one that is both sensitive to differences with biological systems and seeks to locate exceptions to the biological case, if such exist.</em></p>
<p>It also includes a helpful list of distinctions that, if widely adopted, could ensure at least clearer and more productive discussion of the issues:</p>
<p style="padding-left: 40px;"><em><strong>Intelligence Versus Consciousness</strong> </em></p>
<p style="padding-left: 80px;"><em>● An AI could be highly intelligent, being capable of perception, learning, abstraction, reasoning, and calculation, but it may be an AI zombie. </em></p>
<p style="padding-left: 80px;"><em>● Neuroscientists generally agree intelligence can operate without consciousness. </em></p>
<p style="padding-left: 40px;"><em><strong>Sentience Versus Sapience</strong> </em></p>
<p style="padding-left: 80px;"><em>● A highly intelligent AI (a system with “sapience”) might lack any sentience whatsoever, or have a very minimal degree, say, because it only has a subsystem that has a slight degree of sentience. </em></p>
<p style="padding-left: 80px;"><em>● Conversely, a very simple artificial system might have basic sensory experiences, perhaps akin to that had by something like a worm, without sapience. </em></p>
<p style="padding-left: 80px;"><em>● Sentience grounds concern about a being’s suffering and welfare; sapience alone might ground different kinds of intrinsic moral consideration or none at all. If it does ground intrinsic moral consideration it would be related to properties of sapience such as autonomy or agency. </em></p>
<p style="padding-left: 40px;"><em><strong>AI Agency Versus Human Agency</strong> </em></p>
<p style="padding-left: 80px;"><em>● AI agency is different from philosophical agency. </em></p>
<p style="padding-left: 80px;"><em>● Claims of intrinsic moral value of AI agents based on philosophical agency may rest on a conflation and require close scrutiny. </em></p>
<p style="padding-left: 40px;"><em><strong>“Functional Consciousness” (FC) Versus “Phenomenal Consciousness” (PC)</strong> </em></p>
<p style="padding-left: 80px;"><em>● FC and PC are distinct. If they are conflated we risk admitting AI zombies as conscious beings. </em></p>
<p style="padding-left: 80px;"><em>● Yet functional consciousness can be expected to track with the level of intelligence of a system. This means FC is of import to AI safety, even if PC is not present. </em></p>
<p style="padding-left: 40px;"><em><strong>Seemingly Conscious AIs (SCAICs) Versus Phenomenally Conscious (PC) AIs:</strong> </em></p>
<p style="padding-left: 80px;"><em>● SCAIC’s — “seemingly conscious AIs”, are AI’s that that seem to have PC but which lack PC. </em></p>
<p style="padding-left: 80px;"><em>● Some believe that the issue of AI consciousness is not worth discussion, however, even if today’s LLMs are merely SCAICs a sensible scientific and philosophical discussion of the likelihood of consciousness/sentience can help clarify the issues, and prepare for a time in which AI consciousness is reached (if it ever is), and avoid missing or over-attributing sentience. </em></p>
<p style="padding-left: 40px;"><em><strong>Features of Functional Consciousness (FCC) Versus Phenomenal Consciousness</strong> </em></p>
<p style="padding-left: 80px;"><em>● A system has a feature(s) of functional consciousness when it has some computational features that are at least roughly like those found to resemble conscious processing in humans, (e.g., working memory and attention), or has a capacity to generate behavioral outputs mimicking human linguistic behaviors about consciousness and emotion in ways that can be convincing to some, or an avoidance of situations that a conscious being may seek to avoid because they are painful. </em></p>
<p style="padding-left: 80px;"><em>● It is not a straightforward matter to consider systems having FCC as having a marker for PC. (E.g., LLMs have been trained on human data, so their reports about consciousness and even similarity in processing activity involving consciousness could be a simple feature of this.)</em></p>
<p style="padding-left: 40px;"><strong><em>Life Versus Consciousness</em></strong></p>
<p style="padding-left: 80px;"><em>● Life, as defined by NASA (2025), is: ”Life is a self-sustaining chemical system capable of Darwinian evolution.” </em></p>
<p style="padding-left: 80px;"><em>● In principle, given this definition, an AI system can have PC without being alive, in this sense. And something can be alive (e.g., microbial life) without having PC.</em></p>
<p>There is quite a bit more to the white paper, which you can read in its entirety <a href="https://philpapers.org/archive/SCHIAC-22.pdf" target="_blank" rel="noopener">here</a>.</p>
<p>And while we&#8217;re at it, what about the question of whether AIs, even if they&#8217;re not conscious, could appropriately be considered moral agents? <a href="https://jensemler.com/" target="_blank" rel="noopener">Jen Semler</a> (Cornell Tech) takes up that question <a href="https://newworkinphilosophy.substack.com/p/moral-agency-without-consciousness" target="_blank" rel="noopener">here</a>.</p><p>The post <a href="https://dailynous.com/2025/10/14/ai-development-and-consciousness/">AI Development and Consciousness</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>