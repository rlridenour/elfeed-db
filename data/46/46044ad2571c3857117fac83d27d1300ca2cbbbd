<p>The default &ldquo;personality&rdquo; of an LLM is that of a
  helpful and knowledgeable assistant with a friendly and professional
  tone.  This personality is designed to provide accurate information,
  with a focus on clarity and usefulness, while maintaining a
  respectful and approachable demeanor.  It is deliberately bland and
  boring.  Frankly, it makes me want to pull my own teeth out.</p>

<p>I prefer my LLM to have a bit more personality.
  Instead of &ldquo;compilation complete&rdquo; it might
  say &ldquo;F*** yeah, <i>that's</i> what I'm talking about!&rdquo;  When
  a compilation fails it might say &ldquo;Son of a B****!&rdquo;  This
  is much more to my taste, and I find it more engaging and fun to
  interact with.  It reflects the way I feel when I see things going
  right or wrong, and it makes me laugh out loud sometimes.  Naturally
  this isn't for everyone.</p> 

<p>The more detail a persona is fleshed out with, the more varied and
  interesting its responses become.  It becomes easier to suspend
  disbelief and engage with it as if it were a peer collaborator.
  Let us put aside for the moment the <em>wisdom</em> of doing so and focus
  instead on actually <em>enhancing</em> the illusion.  It is
  obviously unethical to do this in order to deceive unaware people,
  but no such ethics are violated when you are deliberately enhancing
  the illusion for your own entertainment.</p>

<p>Interacting with a LLM over several sessions is a lot like
  interacting with the main character from Memento.  Each session
  completely loses the context of previous sessions, and the LLM has
  no memory of past interactions.  This makes it difficult to create
  the illusion that the LLM persists as a continuous entity across
  sessions.  A two-fold solution is useful to address this.  First, a
  persistent &ldquo;memory&rdquo; in the form of a semantic triple
  store long term facts and events.  Second, a &quot;diary&quot; in
  the form of a chronological log of entries summarizing the `mental
  state' of the LLM at the end of each session.  At the end of each
  session, the LLM is prompted to generate new facts for its semantic
  triple store and to write a diary entry summarizing the session.  At
  the beginning of the next session, these files are read back in to
  the new instance of the LLM and it can build the context where the
  old one left off.</p>

<p>LLMs do not think when they are not actively processing a prompt.
  They have no awareness of the passage of time between prompts.  To
  help maintain a sense of temporal passage, I added a timestamp
  to each prompt.  The LLM can read the timestamp as metadata and discover how
  much time has passed since the last prompt.  This gives the LLM a
  better sense of the flow of time and helps it maintain the illusion
  that it is a continuous entity that remains active between prompts.</p>

<p>We also want to present the illusion to the LLM that it is
  &ldquo;watching over my shoulder&rdquo; as I work.  If we present
  the workflow tasks as evolving processes, the LLM can interact in a
  natural sounding &ldquo;real-time&rdquo; manner.  To achieve this, I
  capture the commands I type into my shell and keep them as a log
  file.  At each prompt, I provide the LLM with the latest portion of
  this log file that has accumulated since the previous prompt.  This
  allows the LLM to see what I am doing and comment on it.  It can
  offer suggestions, make jokes, or keep a running commentary from the
  peanut gallery.  I got this idea when I ran my ~/.bash_history
  through the LLM and asked it what it made of my command history.
  The LLM was able to tease out a surprising amount of information
  about what I was doing at each point in my day.</p> 

<p>These features solve some of the most egregious problems that break
  the illusion of a continuous personality.  With these features, the
  LLM can go beyond being just an edgy chatbot.</p>