<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0150-1750583786099.png" alt="Apple's 'Use Model' action in Shortcuts."><p class="image-caption">Apple&rsquo;s &lsquo;Use Model&rsquo; action in Shortcuts.</p></div>
<p id="p2">I <a href="https://appstories.net/episodes/440" rel="noopener noreferrer">mentioned this on AppStories</a> during the week of WWDC: I think Apple&rsquo;s new &lsquo;Use Model&rsquo; action in Shortcuts for iOS/iPadOS/macOS 26, which lets you prompt either the local or cloud-based Apple Foundation models, is Apple Intelligence&rsquo;s best and most exciting new feature for power users this year. This blog post is a way for me to better explain why as well as publicly investigate some aspects of the updated Foundation models that I don&rsquo;t fully understand yet.</p>
<p id="p3"><!--more--></p>
<p id="p4">On the surface, the &lsquo;Use Model&rsquo; action is pretty simple: you can choose to prompt Apple&rsquo;s on-device model, the cloud one hosted on <a href="https://security.apple.com/blog/private-cloud-compute/" rel="noopener noreferrer">Private Cloud Compute</a>, or ChatGPT. You can type whatever you want in the prompt, and you can choose to enable a follow-up mode in the action that lets you have a chatbot-like conversation with the model.</p>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0151-1750583788715.png" alt="Having a conversation with Apple's local model from Shortcuts."><p class="image-caption">Having a conversation with Apple&rsquo;s local model from Shortcuts.</p></div>
<p id="p6">What&rsquo;s immediately notable here is that, while third-party app developers only got access to the <a href="https://developer.apple.com/documentation/foundationmodels" rel="noopener noreferrer">local Foundation model to use in their apps</a>, Shortcuts users can prompt either versions of <a href="https://www.macstories.net/linked/how-could-apple-use-open-source-ai-models/" rel="noopener noreferrer">AFM</a>. It&rsquo;s not clear if the cloud model is going to be rate-limited if you perform too many requests in a row, which is something I&rsquo;d like to clarify at some point (but I would expect so). Likewise, while the ChatGPT integration seems to integrate with the native ChatGPT extension in Settings and therefore connects to your OpenAI account, it&rsquo;s not clear which ChatGPT model is being used at the moment. Trying to ask the model about it from Shortcuts <a href="https://mastodon.macstories.net/@viticci/114701298406224603" rel="noopener noreferrer">showed</a> that ChatGPT in Shortcuts was using <a href="https://platform.openai.com/docs/models/gpt-4-turbo" rel="noopener noreferrer">GPT-4 Turbo</a>, which is a <em>very</em> old model at this point (from November 2023).</p>
<p id="p7">I want to focus on the Apple Foundation models for now. To understand how they work, let&rsquo;s first take a look at a <a href="https://machinelearning.apple.com/research/apple-foundation-models-2025-updates" rel="noopener noreferrer">recent post</a> from <a href="https://machinelearning.apple.com/" rel="noopener noreferrer">Apple&rsquo;s Machine Learning Research blog</a>. On June 9, the company shared some details on the updated Foundation models, writing:</p>
<blockquote id="blockquote8"><p>
  The models have improved tool-use and reasoning capabilities, understand image and text inputs, are faster and more efficient, and are&nbsp;designed to&nbsp;support 15 languages. Our latest foundation models are optimized to run efficiently on Apple silicon, and include a compact, approximately 3-billion-parameter model, alongside a mixture-of-experts server-based model with a novel architecture tailored for <a href="https://security.apple.com/blog/private-cloud-compute/" rel="noopener noreferrer">Private Cloud Compute</a>. These two foundation models are part of a larger family of generative models created by Apple to support our users.
</p></blockquote>
<p id="p9">Both Foundation models were originally released last year. Apple doesn&rsquo;t explicitly mention whether the &lsquo;Use Model&rsquo; action in Shortcuts is currently using the updated versions of AFM compared to last year&rsquo;s versions, but I would assume so? Again, I&rsquo;d love to know more about this. Assuming that the answer is &ldquo;yes&rdquo;, we&rsquo;re then looking at a small, ~3B model running locally on-device, and a bigger, cloud-hosted model (previously referred to as &ldquo;AFM-server&rdquo;) running on Private Cloud Compute. How big is exactly that model, though? Again, let&rsquo;s go back to the blog post:</p>
<blockquote id="blockquote10"><p>
  We found that our on-device model performs favorably against the slightly larger Qwen-2.5-3B across all languages and is competitive against the larger Qwen-3-4B and Gemma-3-4B in English. Our server-based model performs favorably against Llama-4-Scout, whose total size and active number of parameters are comparable to our server model, but is behind larger models such as Qwen-3-235B and the proprietary GPT-4o.
</p></blockquote>
<p id="p11">If <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" rel="noopener noreferrer">Llama-4-Scout</a> is &ldquo;comparable&rdquo; to AFM-server, we may assume that the updated AFM-server based on a <a href="https://medium.com/@mne/explaining-the-mixture-of-experts-moe-architecture-in-simple-terms-85de9d19ea73" rel="noopener noreferrer">MoE architecture</a> is ~17 billion in size. Apple hasn&rsquo;t officially documented this anywhere. Still, as a frame of reference, Apple is saying that their server model isn&rsquo;t as good as modern ChatGPT or the latest large <a href="https://qwenlm.github.io/blog/qwen3/" rel="noopener noreferrer">Qwen 3</a> model. But perhaps internally they do have one that <a href="https://www.macrumors.com/2025/05/19/next-gen-siri-is-on-par-with-chatgpt/" rel="noopener noreferrer">matches the current version of ChatGPT</a> in quality and performance?</p>
<p id="p12">Another aspect of the updated Apple Foundation models I&rsquo;d like to understand are their vision capabilities. On its blog, Apple writes:<br>
&#8232;</p>
<blockquote id="blockquote13"><p>
  We compared the on-device model to vision models of similar size, namely InternVL-2.5-4B, Qwen-2.5-VL-3B-Instruct, and Gemma-3-4B, and our server model to Llama-4-Scout, Qwen-2.5-VL-32B, and GPT&ndash;4o. We found that Apple&rsquo;s on-device model performs favorably against the larger InternVL and Qwen and competitively against Gemma, and our server model outperforms Qwen-2.5-VL, at less than half the inference FLOPS, but is behind Llama-4-Scout and GPT&ndash;4o.
</p></blockquote>
<p id="p14">The problem is that, right now, trying to analyze an image using the local model with the &lsquo;Use Model&rsquo; action in Shortcuts doesn&rsquo;t work. When I give it <a href="https://cdn.macstories.net/img_0077-1750549359945.jpeg" rel="noopener noreferrer">this image</a>, I get this response:<br>
&#8232;</p>
<blockquote id="blockquote15"><p>
  I&rsquo;m sorry, but I can&rsquo;t describe images directly. However, if you provide a detailed description or tell me what you see, I&rsquo;d be happy to help you analyze or interpret it!
</p></blockquote>
<p id="p16">Whereas running the same prompt with the cloud model returns a pretty good response:</p>
<blockquote id="blockquote17"><p>
  The image shows a person walking two dogs on a paved path. The person is wearing white sneakers and is holding two leashes, one pink and one purple. The dogs are walking side by side, both with light brown fur and wearing harnesses. The path is lined with grass and some small plants on the left side. The scene appears to be outdoors, possibly in a park or a residential area. The lighting suggests it might be daytime.
</p></blockquote>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0152-1750583791004.png" alt="Testing the vision capabilities of Apple's models in Shortcuts."><p class="image-caption">Testing the vision capabilities of Apple&rsquo;s models in Shortcuts.</p></div>
<p id="p19">Does this mean that Shortcuts is still using an old version of the on-device model, which didn&rsquo;t have vision capabilities? Or is it the new model from 2025, but the vision features haven&rsquo;t been enabled in Shortcuts yet? Also: why is it that, if I give the cloud model the same picture over and over, I always get the exact same response, even after several days? Is Apple caching a static copy of each uploaded file on its servers for days and associating it with a specific description to decrease latency and inference costs? Again: has this been documented anywhere? Please let me know if so.</p>
<p id="p20">The most fascinating part of the &lsquo;Use Model&rsquo; action is that it works with structured data and is able to directly parse native Shortcuts entities as well as return specific data types upon configuration. For example, you can tell the &lsquo;Use Model&rsquo; action to return a &lsquo;Dictionary&rsquo; for your prompt, and the model&rsquo;s response will be a native dictionary type in Shortcuts that you can parse with the app&rsquo;s built-in actions. Or you can choose to return text, lists, and Boolean values.</p>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0153-1750583793670.png" alt="Some of the native data formats supported by the action."><p class="image-caption">Some of the native data formats supported by the action.</p></div>
<p id="p22">But there&rsquo;s more: the &lsquo;Use Model&rsquo; action can take in Shortcuts variables, understand them, and return them in its output, going beyond the typical limitations of plain text responses from chatbots. From the official description of the &lsquo;Use Model&rsquo; action:</p>
<blockquote id="blockquote23"><p>
  A request for the model that optionally includes variables and outputs from previous actions, including calendar events, reminders, photos, and more.
</p></blockquote>
<p id="p24">The &ldquo;and more&rdquo; is doing a lot of work here, in the sense that, based on my first tests, this action seems to be able to parse <em>any</em> Shortcuts variable you give it. I put together a simple shortcut that gets all my unread items from <a href="https://apps.apple.com/us/app/goodlinks/id1474335294" rel="noopener noreferrer">GoodLinks</a> and asks the local model:</p>
<blockquote id="blockquote25"><p>
  You are an intelligent read-later assistant. Your job is to process multiple articles and return only the ones that match the topic requested by the user.</p>
<p>  When analyzing articles, consider their Title and Summary as available in those properties of the Link variable. Return a subset of items in the Link variable that ONLY match the user&rsquo;s query.</p>
<p>  User Request</p>
<p>  Can you give me links about Liquid Glass?</p>
<p>  Links</p>
<p>  Here are the links:</p>
<p>  {{Link Variable from GoodLinks}}
</p></blockquote>
<p id="p26">Once configured to return a &lsquo;Link&rsquo; type in its output, the action can parse GoodLinks entities (which contain multiple pieces of metadata about each saved article), filter the ones that don&rsquo;t match my query, and return a native list of GoodLinks items about Liquid Glass:</p>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0154-1750583797823.png" alt="Parsing GoodLinks items with Apple Intelligence."><p class="image-caption">Parsing GoodLinks items with Apple Intelligence.</p></div>
<p id="p28">This also works with Reminders, Calendar events, photos, and other third-party apps I&rsquo;ve tested. For example, the model was also able to return variables for <a href="https://apps.apple.com/us/app/news-explorer/id1032668306" rel="noopener noreferrer">News Explorer</a>, an iCloud-based RSS reader that has excellent Shortcuts integration. Interestingly, neither Apple Intelligence models were able to parse 43 unread items in my RSS feeds, citing context length issues. This is something else I&rsquo;d like to know: Apple&rsquo;s blog post suggests that AFM was trained with prompts up to 65K tokens in size, but is that actually the case in Shortcuts with the &lsquo;Use Model&rsquo; action? Regardless, ChatGPT got it done and was able to summarize all my unread items from News Explorer to give me an overview of what my unread feeds were saying:</p>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/saturday-21-jun-2025-163816-1750583801349.png" alt="AFM couldn't process my 43 unread items from RSS, but ChatGPT could, likely thanks to its bigger 128K context window."><p class="image-caption">AFM couldn&rsquo;t process my 43 unread items from RSS, but ChatGPT could, likely thanks to its bigger 128K context window.</p></div>
<p id="p30">How is this possible? We do actually have an answer to this! The &lsquo;Use Model&rsquo; action converts Shortcuts entities from apps (such as my GoodLinks articles or reminders) into a <strong>JSON representation</strong> that AFM or ChatGPT can understand and return. Everything is, of course, based on the <a href="https://developer.apple.com/documentation/appintents" rel="noopener noreferrer">App Intents framework</a> and its related <a href="https://developer.apple.com/documentation/appintents/app-entities" rel="noopener noreferrer">app entities</a> for third-party apps that expose actions to Shortcuts.</p>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/img_0156-1750583803063.jpeg" alt="How app entities are sent by Shortcuts to the model as JSON objects."><p class="image-caption">How app entities are sent by Shortcuts to the model as JSON objects.</p></div>
<p id="p32">If you&rsquo;re logged in with a ChatGPT account, you can even see the request and the raw JSON content coming from Shortcuts:</p>
<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/sunday-22-jun-2025-18-27-23-1750609648374.png" alt="A Shortcuts request with JSON from app entities in ChatGPT."><p class="image-caption">A Shortcuts request with JSON from app entities in ChatGPT.</p></div>
<p id="p34">Apple went into more detail about how this works in <a href="https://youtu.be/XRDEnbdnIOY?si=utdvuT9ml-3D-BKs" rel="noopener noreferrer">this WWDC session video</a>, which I&rsquo;m also embedding below. The relevant section starts at the 1:20 minute mark.</p>
<div class="media-wrapper" id="div35"><iframe loading="lazy" title="WWDC25: Develop for Shortcuts and Spotlight with App Intents | Apple" width="768" height="432" src="https://www.youtube.com/embed/XRDEnbdnIOY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></div>
<p id="p36">Again, I&rsquo;d love to know some additional details here: are there limits on the numbers of entities converted to JSON behind the scenes that you can pass to a model? Can this JSON-based approach easily scale to more potential Apple Intelligence model integrations in the future, since both Gemini and Claude are pretty great at dealing with JSON content and instruction-following? Is there a specific style of prompting Apple&rsquo;s Foundation models (similar to <a href="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags" rel="noopener noreferrer">Claude</a> or <a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide" rel="noopener noreferrer">GPT 4.1</a>) that we should be aware of?</p>
<p id="p37">I&rsquo;d <em>love</em> to see the system prompt that injects JSON and related instructions into the main action prompt, but I&rsquo;m guessing that&rsquo;s never going to happen since Apple doesn&rsquo;t seem to be interested in sharing system prompts for Apple Intelligence.</p>
<p id="p38">I&rsquo;m excited about the prospect of &ldquo;Apple-approved&rdquo; <a href="https://www.macstories.net/tag/hybrid-automation/" rel="noopener noreferrer">hybrid automation</a> in Shortcuts that combines the non-deterministic output of LLMs with the traditional, top-to-bottom approach of workflows in Shortcuts. I have so many ideas for how I could integrate this kind of technology with shortcuts that deal with RSS, tasks, events, articles, and more. The fact that Apple designed the &lsquo;Use Model&rsquo; action to &ldquo;just work&rdquo; thanks to JSON under the hood <em>is</em> very promising: as I&rsquo;ve shown in my example, it means that entities from Shortcuts actions don&rsquo;t have to belong to apps that have been updated for iOS 26; I&rsquo;m running the iOS 18 version of GoodLinks, and the &lsquo;Use Model&rsquo; action worked out of the box with GoodLinks entities.</p>
<p id="p39">Hopefully, as the dust settles on the first developer betas of iOS/iPadOS/macOS 26, Apple will reveal more details about their Foundation models and their integration with Shortcuts. Who would have thought, just two weeks ago, that I&rsquo;d be genuinely intrigued by something related to Apple Intelligence?</p>
<hr /><h3>Access Extra Content and Perks</h3><p><p>Founded in 2015, <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">Club MacStories</a> has delivered exclusive content every week for nearly a decade.</p>
<p>What started with weekly and monthly email newsletters has blossomed into <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">a family of memberships</a> designed every MacStories fan.</p>
<p><strong><a href="https://club.macstories.net/plans/club" rel="noopener noreferrer">Club MacStories</a></strong>: Weekly and monthly newsletters via email and the web that are brimming with apps, tips, automation workflows, longform writing, early access to the <a href="https://www.macstories.net/unwind/" rel="noopener noreferrer">MacStories Unwind podcast</a>, periodic giveaways, and more;</p>
<p><strong><a href="https://club.macstories.net/plans/plus" rel="noopener noreferrer">Club MacStories+</a></strong>: Everything that Club MacStories offers, plus an active Discord community, advanced search and custom RSS features for exploring the Club&rsquo;s entire back catalog, bonus columns, and dozens of app discounts;</p>
<p><strong><a href="https://club.macstories.net/plans/premier" rel="noopener noreferrer">Club Premier</a></strong>: All of the above <em>and</em> AppStories+, an extended version of our flagship podcast that&rsquo;s delivered early, ad-free, and in high-bitrate audio.</p>
<p>Learn more <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">here</a> and from our <a href="https://club.macstories.net/faq" rel="noopener noreferrer">Club FAQs</a>.</p>
</p><a href='https://club.macstories.net/?utm_source=ms&#038;utm_medium=rss'>Join Now</a>