<p>[Equations in this post may not look right (or appear at all) in your RSS reader. Go to <a href="https://leancrew.com/all-this/2025/07/cauchy-coup/">the original article</a> to see them rendered properly.]</p>
  <hr />
  <p>Bruce Ediger, who blogs at <a href="https://bruceediger.com/">Information Camouflage</a>, has been doing some interesting numerical experimentation recently; first in a post about <a href="https://bruceediger.com/posts/practical-techniques/">estimating  population size</a> from a sample of serial numbers (you may have seen <a href="https://www.youtube.com/watch?v=WLCwMRJBhuI">this Numberphile video</a> on the same topic), and then a couple of videos about the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a>. In <a href="https://bruceediger.com/posts/stable-distributions/">the first</a>, he looked at the sum of two random variables that are Cauchy distributed; in <a href="https://bruceediger.com/posts/bonus-normal-distribution/">the second</a>, he looked at the quotient of two normally distributed random variables, which—under certain conditions—is supposed to have a Cauchy distribution.</p>
<p>By <em>numerical experimentation</em>, I mean that Ediger’s generating sets of random numbers, doing some calculations on them, and then seeing how close his calculations are to certain theoretical results. It’s a good way to get a feel for how things work, and it’s the kind of analysis that’s really only possible because of the computational power we all have at our fingertips. This sort of playing around with thousands and thousands of numbers just wasn’t feasible when I was learning probability and statistics, and I’m jealous of today’s students.</p>
<p>What attacted my interest in Ediger’s post on the quotient of two normals was the possibility that I could do the problem analytically without too much effort—mainly because the same computational power that lets him mess around with a hundred thousand quotients of random samples also lets me do calculus and algebra without dropping terms or making other silly errors.</p>
<h2>Cauchy from the quotient of two zero-mean normals</h2>
<p>I started by making sure I could prove that the quotient of two zero-mean normals has a Cauchy distribution. The analysis in this section follows the structure outlined in Papoulis’s <a href="https://lccn.loc.gov/64022956"><em>Probability, Random Variables, and Stochastic Processes</em></a>.</p>
<p>We begin by looking at the general problem of the quotient of two random variables of any distribution. We’ll call the two random variables <em>X</em> and <em>Y</em> and their <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint probability density function</a> <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo></mrow></math>.<sup id="fnref:caps"><a href="#fn:caps" rel="footnote">1</a></sup></p>
<p>We’ll call our quotient <em>Z</em> and define it this way:</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi><mo>=</mo><mfrac><mi>X</mi><mi>Y</mi></mfrac></math>
<p>To figure out the probability density function of <em>Z</em>, we’ll first look at its <a href="https://mathworld.wolfram.com/DistributionFunction.html">cumulative distribution function</a>, <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>F</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo></mrow></math>, which is defined as the probability that <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Z</mi><mo>≤</mo><mi>z</mi></mrow></math>,</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo form="prefix" stretchy="false">(</mo><mi>Z</mi><mo>≤</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo></math>
<p>This probability will be the volume under the joint PDF of <em>X</em> and <em>Y</em> over the region where <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mi>/</mi><mi>Y</mi><mo>≤</mo><mi>z</mi></mrow></math>. That’ll be this integral:</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><munder><mo>∬</mo><mrow><mi>x</mi><mi>/</mi><mi>y</mi><mo>≤</mo><mi>z</mi></mrow></munder><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>y</mi></math>
<p>And the domain over which we take the integral will be the blue region in this graph,</p>
<p><img alt="Quotient integration domain" class="ss" src="https://leancrew.com/all-this/images2025/20250719-Quotient%20integration%20domain.png" title="Quotient integration domain" width="70%"/></p>
<p>where the region extends as far as <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo></mrow></math> extends in the upper left and lower right directions.</p>
<p>If this doesn’t seem obvious to you, don’t worry, it isn’t. But think of it this way:</p>
<ul>
<li>If <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>&gt;</mo><mn>0</mn></mrow></math>, then <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mi>/</mi><mi>y</mi></mrow></math> will be less than <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math> when <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math> is to the left of the <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>=</mo><mi>z</mi><mi>y</mi></mrow></math> line.</li>
<li>If <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>&lt;</mo><mn>0</mn></mrow></math>, then <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mi>/</mi><mi>y</mi></mrow></math> will be less than <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math> when <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math> is to the right of the <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>=</mo><mi>z</mi><mi>y</mi></mrow></math> line. Making <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math> negative flips the direction of the less-than sign.</li>
</ul>
<p>With the integration domain defined, we can write the expression for <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>F</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo></mrow></math> like this:</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>F</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>∞</mi></msubsup><msubsup><mo>∫</mo><mrow><mi>−</mi><mi>∞</mi></mrow><mrow><mi>z</mi><mi>y</mi></mrow></msubsup><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>y</mi><mo>+</mo><msubsup><mo>∫</mo><mrow><mi>−</mi><mi>∞</mi></mrow><mn>0</mn></msubsup><msubsup><mo>∫</mo><mrow><mi>z</mi><mi>y</mi></mrow><mi>∞</mi></msubsup><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>y</mi></math>
<p>To get the PDF of <em>Z</em>, we need to differentiate <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>F</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo></mrow></math> with respect to <em>z</em>. This is done through the <a href="https://en.wikipedia.org/wiki/Leibniz_integral_rule">Leibniz rule</a>. Normally that would leave us with six terms, three for each of the two integrals above. But luckily for us, four of those terms are zero, leaving</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>∞</mi></msubsup><mi>y</mi><mspace width="0.167em"></mspace><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mi>y</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>y</mi><mo>−</mo><msubsup><mo>∫</mo><mrow><mi>−</mi><mi>∞</mi></mrow><mn>0</mn></msubsup><mi>y</mi><mspace width="0.167em"></mspace><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mi>y</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>y</mi></math>
<p>Now it’s time to consider the case when <em>X</em> and <em>Y</em> are jointly normal. And to get to a Cauchy distribution for <em>Z</em>, both <em>X</em> and <em>Y</em> will have to have zero means. With that restriction, the joint density function will look something like this:</p>
<p><img alt="Joint normal surface plot" class="ss" src="https://leancrew.com/all-this/images2025/20250719-Joint%20normal%20surface%20plot.png" title="Joint normal surface plot" width="100%"/></p>
<p>To make the plot, I needed some actual numbers, so I’ve given <em>X</em> a standard deviation of 1, <em>Y</em> a standard deviation of 1.5, and the correlation coefficient a value of 0.25. The analysis doesn’t rely on these or any other particular values for the parameters.</p>
<p>We can see the joint PDF’s structure a little better with a contour plot:</p>
<p><img alt="Joint normal contour plot" class="ss" src="https://leancrew.com/all-this/images2025/20250719-Joint%20normal%20contour%20plot.png" title="Joint normal contour plot" width="80%"/></p>
<p>The key is that it has this kind of symmetry,</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>−</mi><mi>x</mi><mo>,</mo><mi>−</mi><mi>y</mi><mo form="postfix" stretchy="false">)</mo></math>
<p>and we can take advantage of that symmetry to simplify the expression for <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>f</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo></mrow></math>:</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><mn>2</mn><msubsup><mo>∫</mo><mn>0</mn><mi>∞</mi></msubsup><mi>y</mi><mspace width="0.167em"></mspace><msub><mi>f</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mi>y</mi><mo>,</mo><mi>y</mi><mo form="postfix" stretchy="false">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>y</mi></math>
<p>OK, now we have to get into the nitty gritty and actually do some integration. Which means it’s time to abandon “by hand” analysis and switch to Mathematica. Here’s the notebook that does the following:</p>
<ol>
<li>The integration to determine the form of <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>f</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo></mrow></math>. When writing stuff like this by hand, I would typically use <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>σ</mi><mi>X</mi></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>σ</mi><mi>Y</mi></msub></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ρ</mi></math> for the standard deviations of <em>X</em> and <em>Y</em> and their correlation coefficient, but to make the typing easier in Mathematica, I use <code>sx</code>, <code>sy</code>, and <code>r</code>.</li>
<li>Checks the result against Papoulis’s. It’s not uncommon for Mathematica to give answers that don’t look like what you’ll find in a book, even though they are algebraically equivalent.</li>
<li><p>Determines the shape and location factors, <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>γ</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mn>0</mn></msub></math>, that put the result into the generic form for a Cauchy distribution that we see in the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Wikipedia article</a>:</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>Z</mi></msub><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo><mfrac><mi>γ</mi><mrow><mi>π</mi><mrow><mo form="prefix" stretchy="true">[</mo><mo form="prefix" stretchy="false">(</mo><mi>z</mi><mo>−</mo><msub><mi>z</mi><mn>0</mn></msub><msup><mo form="postfix" stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><mo form="postfix" stretchy="true">]</mo></mrow></mrow></mfrac></math></li>
</ol>
<p>Here’s the notebook:</p>
<iframe height="800" src="https://www.wolframcloud.com/obj/mark80/Published/papoulis.nb?_embed=iframe" width="100%"></iframe>
<p>So yes, the quotient of two zero-mean normals has a Cauchy distribution with a shape parameter of</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mi>γ</mi><mo>=</mo><mfrac><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ρ</mi><mn>2</mn></msup></mrow></msqrt><mspace width="0.167em"></mspace><msub><mi>σ</mi><mi>X</mi></msub></mrow><msub><mi>σ</mi><mi>Y</mi></msub></mfrac></math>
<p>and a location parameter of</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mn>0</mn></msub><mo>=</mo><mfrac><mrow><mi>ρ</mi><mspace width="0.167em"></mspace><msub><mi>σ</mi><mi>X</mi></msub></mrow><msub><mi>σ</mi><mi>Y</mi></msub></mfrac></math>
<h2>Ediger’s problem</h2>
<p>That was sort of a warmup to make sure I understood how to use the <a href="http://reference.wolfram.com/language/ref/MultinormalDistribution.html"><code>MultinormalDistribution</code> function</a>. It’s time to move on to Ediger’s problem.</p>
<p>Ediger doesn’t take the quotient of two zero-mean normals. Instead, he takes the quotient of two independent (<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><mo>=</mo><mn>0</mn></mrow></math>) normals, one with zero mean and unit standard deviation, and the other with a mean of 1 and standard deviation of 3. This shouldn’t result in a Cauchy distribution, and it doesn’t. But to see how close it is, I made this Mathematica notebook:</p>
<iframe height="800" src="https://www.wolframcloud.com/obj/mark80/Published/ediger.nb?_embed=iframe" width="100%"></iframe>
<p>Comparing the actual distribution of the quotient (in blue) with Ediger’s best fit of a Cauchy distribution (orange), we see this:</p>
<p><img alt="Exact PDF vs Ediger fit" class="ss" src="https://leancrew.com/all-this/images2025/20250719-Exact%20PDF%20vs%20Ediger%20fit.png" title="Exact PDF vs Ediger fit" width="100%"/></p>
<p>Definitely not the same but pretty damned close.</p>
<h2>Finally</h2>
<p>I’m not sure most people would consider this recreational mathematics, but it’s recreational to me. You should see the things in my notebooks that <em>don’t</em> get turned into blog posts.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:caps">
<p>A habit I picked up from my thesis advisor is to use capital letters for random variables and their lower case versions for particular instances of those random variables. <a href="#fnref:caps" rev="footnote">↩</a></p>
</li>
</ol>
</div>
  
