<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/cleanshot-2025-10-30-at-20-32-38-2x-1761852777693.png" alt="Kimi K2, hosted on Groq, running in TypingMind with a custom plugin I made."><p class="image-caption">Kimi K2, hosted on Groq, running in TypingMind with a custom plugin I made.</p></div>
<p id="p2">I&rsquo;ll talk about this more in depth in Monday&rsquo;s episode of <a href="https://appstories.net/" rel="noopener noreferrer">AppStories</a> (if you&rsquo;re a <a href="https://appstories.net/plans" rel="noopener noreferrer">Plus subscriber</a>, it&rsquo;ll be out on Sunday), but I wanted to post a quick note on the site to show off what I&rsquo;ve been experimenting with this week. I started playing around with <a href="https://www.typingmind.com/" rel="noopener noreferrer">TypingMind</a>, a web-based wrapper for all kinds of LLMs (from any provider you want to use), and, in the process, I&rsquo;ve ended up recreating parts of my <a href="https://www.macstories.net/linked/claudes-chat-history-and-app-integrations-as-a-form-of-lock-in/" rel="noopener noreferrer">Claude setup with third-party apps</a>&hellip;at a much, <em>much</em> higher speed. Here, let me show you with a video:</p>
<div class="media-wrapper video-autoplay js-video-autoplay aligncenter" id="div3">
            <video data-autoplay="yes" width="900" height="960" data-is-retina="yes" preload="auto" playsinline controls><source src="https://cdn.macstories.net/timerv2-1761851858223.mp4" type="video/mp4"><source src="https://cdn.macstories.net/timerv2-1761851858223.webm" type="video/webm"><source src="https://cdn.macstories.net/timerv2-1761851858223.ogv" type="video/ogg"></source></source></source></video><div class="caption">
<p>Kimi K2 hosted on Groq on the left.</p>
</div></div>
<p id="p4"><!--more--></p>
<p id="p5">It all started because I heard great things about <a href="https://moonshotai.github.io/Kimi-K2/" rel="noopener noreferrer">Kimi K2</a> (the latest open-source model by Chinese lab Moonshot AI) and its <a href="https://www.dbreunig.com/2025/07/30/how-kimi-was-post-trained-for-tool-use.html" rel="noopener noreferrer">performance with agentic tool calls</a>. The folks at Moonshot AI specifically post-trained Kimi K2 to be <em>great</em> at calling external tools, and I found that intriguing since most of my interactions with Claude &ndash; my LLM of choice these days &ndash; revolve around calling one of its connectors in conversations. At the same time, as <a href="https://appstories.net/posts/what-else-do-we-want-from-apple-in-2025" rel="noopener noreferrer">I also shared on AppStories</a> a while back, I&rsquo;ve been fascinated by <strong>really</strong> fast inference as a feature of modern LLMs. If there&rsquo;s something I dislike about Claude, it&rsquo;s how slow it can often be at generating the first token as well as streaming a few tokens per second. A few weeks ago, I played around with <a href="https://qwen.ai/blog?id=d927d7d2e59d059045ce758ded34f98c0186d2d7&amp;from=research.research-list" rel="noopener noreferrer">Qwen3-Coder</a> running on <a href="https://www.cerebras.ai/" rel="noopener noreferrer">Cerebras</a> and found its <a href="https://www.cerebras.ai/blog/qwen3-coder-480b-is-live-on-cerebras" rel="noopener noreferrer">promise</a> of 2,000 tokens per second to mostly hold true (it was more like 1,700 tps in practice) and game-changing. If you&rsquo;re used to watching Claude&rsquo;s responses slowly stream on-screen, fast inference can be eye-opening. Even if Qwen3-Coder was not as good as Claude Sonnet 4.5 at one-shotting vibe-coded <a href="https://docs.getdrafts.com/docs/actions/scripting" rel="noopener noreferrer">Drafts actions</a>, the back and forth that Cerebras&rsquo; fast inference allowed meant that I could immediately follow up in the conversation without waiting around.</p>
<p id="p6">But back to Kimi K2. I noticed that <a href="https://groq.com/" rel="noopener noreferrer">Groq</a> (with a <strong>q</strong>, not the <em>other</em> thing) had been added to the service&rsquo;s cloud-based infrastructure <a href="https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct" rel="noopener noreferrer">running at a healthy 200 tokens per second</a>. That&rsquo;s a far cry from Cerebras, but also more than three times the speed I get even from the faster <a href="https://www.macstories.net/notes/anthropic-releases-haiku-4-5-sonnet-4-performance-twice-as-fast/" rel="noopener noreferrer">Claude Haiku 4.5</a>. Since <a href="https://console.groq.com/docs/openai" rel="noopener noreferrer">Groq offers an OpenAI-compatible endpoint</a>, which TypingMind supports, and Kimi K2 supports <a href="https://platform.openai.com/docs/guides/function-calling" rel="noopener noreferrer">function calls based on the OpenAI spec</a>, I figured I could have some fun with it.</p>
<p id="p7">I&rsquo;ve been <em>loving</em> this setup. The video above is a comparison of Kimi K2 hosted on Groq &ndash; accessed from TypingMind &ndash; versus Haiku 4.5 running in Claude for iOS. In both cases, I&rsquo;m asking the LLM to give me my list of tasks due today from <a href="http://todoist.com/" rel="noopener noreferrer">Todoist</a>. In Claude, that&rsquo;s done via the <a href="https://developer.todoist.com/api/v1/#tag/Todoist-MCP" rel="noopener noreferrer">Todoist MCP connector</a>. In TypingMind, I vibe-coded (with Sonnet 4.5, in fact) a series of Todoist plugins that are making simple REST calls to the Todoist API as functions.</p>
<p id="p8">The difference is <em>staggering</em>. Kimi K2 with my TypingMind setup takes <strong>two seconds</strong> to call Todoist and generate a response; it takes <strong>11 seconds</strong> for Haiku 4.5 to do the same. This proves a couple of things: pure tool calls based on functions that run code in a separate environment are naturally faster than the verbose, <a href="https://mitek99.medium.com/mcps-overengineered-transport-and-protocol-design-f2e70bbbca62" rel="noopener noreferrer">token-consuming nature of MCP</a>; but regardless of that, fast inference means that Kimi K2 on Groq just flies through data coming from external services at blazing speed.</p>
<p id="p9">I&rsquo;ve been so impressed by this, I&rsquo;ve been building a whole suite of plugins to work with Notion and Todoist and had a wonderful time testing everything with Kimi K2 and TypingMind. (TypingMind has an <a href="https://docs.typingmind.com/plugins" rel="noopener noreferrer">excellent visual builder for function calls</a>.) I can, in fact, confirm that Kimi K2 is <em>great</em> at calling multiple tools in a row (such as getting today&rsquo;s date, finding tasks in a project, marking some of them as complete and rescheduling others) and I&rsquo;ve been pleasantly surprised by how well it responds in English, too (if you want to learn more on how Moonshot AI did this, I recommend <a href="https://www.dbreunig.com/2025/07/31/how-kimi-rl-ed-qualitative-data-to-write-better.html" rel="noopener noreferrer">reading this</a>).</p>
<p id="p10">I&rsquo;m not in love with the fact that TypingMind doesn&rsquo;t have a native iOS app, but it&rsquo;s a <em>really</em> well built web app. I don&rsquo;t think I&rsquo;m going to abandon Claude any time soon (after all, Kimi K2 is a non-reasoning model, and I want to use one for long-running tasks), but this experiment has shown me that fast inference changed my perspective on what I want from a work-focused LLM. A faster infrastructure for Claude <a href="https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services" rel="noopener noreferrer">can&rsquo;t come soon enough</a>.</p>
