<p id="p1">In addition to the M5 iPad Pro, which <a href="https://www.macstories.net/stories/m5-ipad-pro-review/" rel="noopener noreferrer">I reviewed earlier today</a>, I also received an M5 MacBook Pro review unit from Apple last week. I <em>really</em> wanted to write a companion piece to my iPad Pro story about MLX and the M5&rsquo;s Neural Accelerators; sadly, I couldn&rsquo;t get the latest MLX branch to work on the MacBook Pro either.</p>
<p id="p2">However, <a href="https://creativestrategies.com/research/m5-apple-silicon-its-all-about-the-cache-and-tensors/" rel="noopener noreferrer">Max Weinbach at Creative Strategies did</a>, and shared some <em>impressive</em> results with the M5 and its GPU&rsquo;s Neural Accelerators:</p>
<blockquote id="blockquote3"><p>
  These dedicated neural accelerators in each core lead to that 4x speedup of compute! In compute heavy parts of LLMs, like the pre-fill stage (the processing that happens during the time to first token) this should lead to massive speed-ups in performance! The decode, generating each token, should be accelerated by the memory bandwidth improvements of the SoC.</p>
<p>  Now, I would have loved to show this off! Unfortunately, full support for the Neural Accelerators isn&rsquo;t in MLX yet. <a href="https://github.com/ml-explore/mlx/pull/2687" rel="noopener noreferrer">There is preliminary support, though!</a> There will be an update later this year with full support, but that doesn&rsquo;t mean we can&rsquo;t test now! Unfortunately, I don&rsquo;t have an M4 Mac on me (traveling at the moment) but what I was able to do was compare M5 performance before and after tensor core optimization! We&rsquo;re seeing between a 3x and 4x speedup in prefill performance!
</p></blockquote>
<p id="p4">Looking at Max&rsquo;s benchmarks with Qwen3&nbsp;8B and a ~20,000-token prompt, there is indeed a <strong>3.65x speedup</strong> in tokens/sec in the prefill stage &ndash; jumping from 158.2 tok/s to a remarkable 578.7 tok/s. This is why I&rsquo;m <em>very</em> excited about the future of MLX for local inference on M5, and why I&rsquo;m also looking forward to M5 Pro/M5 Max chipsets in future Mac models.</p>
<p>â†’ Source: <a href='https://creativestrategies.com/research/m5-apple-silicon-its-all-about-the-cache-and-tensors/' target='_blank'>creativestrategies.com</a></p>