<p>In a recent editorial, <a href="https://sites.google.com/site/dwportmore/" target="_blank" rel="noopener">Douglas Portmore</a> (Notre Dame), the editor-in-chief of <a href="https://www.journals.uchicago.edu/toc/et/current" target="_blank" rel="noopener"><em>Ethics</em></a>, sets forth some &#8220;initial guidelines&#8221; about AI use for the journal.<span id="more-55129"></span></p>
<p><img loading="lazy" decoding="async" class="wp-image-55130 aligncenter" src="https://dailynous.com/wp-content/uploads/2025/09/ethics-circuits.png" alt="" width="585" height="350" srcset="https://dailynous.com/wp-content/uploads/2025/09/ethics-circuits.png 1008w, https://dailynous.com/wp-content/uploads/2025/09/ethics-circuits-300x179.png 300w, https://dailynous.com/wp-content/uploads/2025/09/ethics-circuits-768x459.png 768w, https://dailynous.com/wp-content/uploads/2025/09/ethics-circuits-400x239.png 400w" sizes="auto, (max-width: 585px) 100vw, 585px" /></p>
<p>He says his remarks &#8220;represent only my own opinions, not necessarily those of the associate editors. But I’m hoping to have some official policies and guidelines posted on the journal’s website sometime early next year.&#8221;</p>
<p>Here is what he says about the matter so far:</p>
<p style="padding-left: 40px;"><em>First, AI tools cannot author or even coauthor a submission. Authors and coauthors must be able to take both moral and legal responsibility for their submission. As nonmoral entities, AI tools cannot, as authors must, take responsibility for the accuracy and originality of their products. It is not enough, then, for authors to take responsibility for employing AI tools; they must also take responsibility for the assertions that they make, the arguments that they give, and the sources that they acknowledge or fail to acknowledge. And, as nonlegal entities, AI tools cannot make any legal declarations, nor can they sign off on the necessary copyright agreements. Thus, AI tools don’t meet the requirements for authorship.</em></p>
<p style="padding-left: 40px;"><em>Second, if an author does use any AI tools, they must be transparent about this, disclosing which AI tools were used and how they were used. So, if an author uses an AI tool to, say, generate possible objections to their view or rebuttals to those objections, they must cite the AI tool. The point is that authors shouldn’t take credit for what’s not their own work. What’s more, if an author’s submission borrows some text generated by an AI, that text needs to appear in quotes just as it would if it had been borrowed from a person. And even if an author uses an AI tool simply to edit their writing, they should be transparent about this. I would suggest including something like the following in the note containing their various acknowledgments: “The text of this article has been edited for clarity by ChatGPT, OpenAI, April 16, 2025, <a class="extLink" href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>.” Thus, you should acknowledge the assistance of an AI tool in editing your writing just as you would the assistance of a copyeditor. And, given that </em>Ethics <em>is published by the University of Chicago Press, these citations should follow the guidelines set out in </em>The Chicago Manual of Style<em>.</em></p>
<p style="padding-left: 40px;"><em>Third, it’s important not only to avoid taking credit for work that is not your own but also to give credit where credit is due. The problem, then, with just citing an AI tool as the source of, say, some objection is that the AI tool is unlikely to be the original source of that objection. The source of that objection is more likely some author whose work has been used to train the AI. Thus, attributing an objection (or some other substantive point) to ChatGPT is just as problematic as attributing an objection to your colleague when all they did was tell you about some objection that, say, Rawls raised. So, ideally, you would track down the original source of the objection and cite that, while acknowledging the help of ChatGPT in bringing it to your attention.</em></p>
<p style="padding-left: 40px;"><em>Fourth, it should go without saying that editors and reviewers should not use AI tools to help generate their reports on, or assessments of, submissions. Doing so would violate the rights of authors and breach their own professional responsibilities. Consider that files uploaded to an AI tool such as ChatGPT are retained indefinitely by that service and are then used in training their models. So, even uploading a submitted manuscript to an AI tool violates the confidentiality of the peer review process. Besides, editors and reviewers have a professional responsibility to come up with their own independent assessments of submissions.</em></p>
<p>The full editorial is <a href="https://www.journals.uchicago.edu/doi/10.1086/736441" target="_blank" rel="noopener">here</a>.</p><p>The post <a href="https://dailynous.com/2025/09/29/editor-of-ethics-floats-ai-use-guidelines/">Editor of Ethics Floats AI-Use Guidelines</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>