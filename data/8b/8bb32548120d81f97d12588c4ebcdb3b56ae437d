<div class="media-wrapper"><img decoding="async" src="https://cdn.macstories.net/sunday-02-nov-2025-16-31-31-1762097500040.png" alt="MiniMax M2 with interleaved thinking steps and tools in TypingMind."><p class="image-caption">MiniMax M2 with interleaved thinking steps and tools in TypingMind.</p></div>
<p id="p2">In addition to <a href="https://moonshotai.github.io/Kimi-K2/" rel="noopener noreferrer">Kimi K2</a> (which <a href="https://www.macstories.net/notes/ai-experiments-fast-inference-with-groq-and-third-party-tools-with-kimi-k2-in-typingmind/" rel="noopener noreferrer">I recently wrote about here</a>) and <a href="https://docs.z.ai/guides/llm/glm-4.6" rel="noopener noreferrer">GLM-4.6</a> (which will <a href="https://cdn.macstories.net/image-1762096605387.png" rel="noopener noreferrer">become an option</a> on <a href="https://www.cerebras.ai/" rel="noopener noreferrer">Cerebras</a> in a few days, when I&rsquo;ll play around with it), one of the more interesting open-source LLM releases out of China lately is <a href="https://www.minimax.io/news/minimax-m2" rel="noopener noreferrer">MiniMax M2</a>. This <a href="https://en.wikipedia.org/wiki/Mixture_of_experts" rel="noopener noreferrer">MoE model</a> (230B parameters, 10B activated at any given time) claims to reach 90% of the performance of Sonnet 4.5&hellip;at <strong>8% the cost</strong>. You can read more about the model <a href="https://medium.com/data-science-in-your-pocket/minimax-m2-best-model-for-coding-and-agentic-2f0a80d4ef7f" rel="noopener noreferrer">here</a>; Simon Willison <a href="https://simonwillison.net/2025/Oct/29/minimax-m2/" rel="noopener noreferrer">blogged about it here</a>; you can also <a href="https://huggingface.co/collections/mlx-community/minimax-m2" rel="noopener noreferrer">test it with MLX</a> on an Apple silicon Mac.</p>
<p id="p3">What I find especially interesting about M2 is that it&rsquo;s the first model to support interleaved thinking steps in between responses and tool calls, which is something that <a href="https://www.macstories.net/stories/early-impressions-of-claude-opus-4-and-using-tools-with-extended-thinking/" rel="noopener noreferrer">Anthropic pioneered with Claude Sonnet 4</a> back in May. Here&rsquo;s Skyler Miao, head of engineering at MiniMax, <a href="https://x.com/skylermiao7/status/1984809180163948701" rel="noopener noreferrer">in a post on X</a> (unfortunately, most of the open-source AI community is only active there):</p>
<blockquote id="blockquote4"><p>
  As we work more closely with partners, we&rsquo;ve been surprised how poorly community support interleaved thinking, which is crucial for long, complex agentic tasks. Sonnet 4 introduced it 5 months ago, but adoption is still limited.</p>
<p>  We think it&rsquo;s one of the most important features for agentic models: it makes great use of test-time compute.</p>
<p>  The model can reason after each tool call, especially when tool outputs are unexpected. That&rsquo;s often the hardest part of agentic jobs: you can&rsquo;t predict what the env returns. With interleaved thinking, the model could reason after get tool outputs, and try to find out a better solution.</p>
<p>  We&rsquo;re now working with partners to enable interleaved thinking in M2 &mdash; and hopefully across all capable models.
</p></blockquote>
<p id="p5">I&rsquo;ve been using Claude as my main &ldquo;production&rdquo; LLM for the past few months and, as I&rsquo;ve shared before, I consider the fact that both Sonnet and <a href="https://www.macstories.net/notes/anthropic-releases-haiku-4-5-sonnet-4-performance-twice-as-fast/" rel="noopener noreferrer">Haiku</a> think between steps an essential aspect of their agentic nature and integration with third-party apps.</p>
<p id="p6">That being said, I have been testing MiniMax M2 on <a href="https://www.typingmind.com/" rel="noopener noreferrer">TypingMind</a> in <a href="https://www.macstories.net/notes/ai-experiments-fast-inference-with-groq-and-third-party-tools-with-kimi-k2-in-typingmind/" rel="noopener noreferrer">addition to Kimi K2</a> for the past week and it is, indeed, impressive. I plugged MiniMax M2 into TypingMind using their <a href="https://platform.minimax.io/docs/guides/text-generation" rel="noopener noreferrer">Anthropic-compatible endpoint</a>; out of the box, the model worked with interleaved thinking and the several plugins I&rsquo;ve built for myself in TypingMind using Claude. I haven&rsquo;t used M2 for any vibe-coding tasks yet, but for other research or tool-based queries (like adding notes to Notion and tasks to Todoist), M2 effectively felt like a version of Sonnet not made by Anthropic.</p>
<p id="p7">Right now, MiniMax M2 isn&rsquo;t hosted on any of the fast inference providers; I&rsquo;ve accessed it via the official MiniMax API endpoint, whose inference speed isn&rsquo;t that different from Anthropic&rsquo;s cloud. The possibility of MiniMax M2 on Cerebras or Groq is <em>extremely</em> fascinating, and I hope it&rsquo;s in the cards for the near future.</p>
