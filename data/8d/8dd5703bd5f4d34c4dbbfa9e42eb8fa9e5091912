<p>What will tell your students about whether and how they may use AI for work you assign?<span id="more-54766"></span></p>
<p>It depends on the students, right?</p>
<p>That&#8217;s the main idea behind today&#8217;s guest post by <a href="https://www.bu.edu/philo/profile/victor-kumar/" target="_blank" rel="noopener">Victor Kumar</a> (Boston University). Professor Kumar is co-author (w/Richmond Campbell) of <em><a href="https://www.amazon.com/Better-Ape-Evolution-Moral-Human/dp/0197600123?crid=2KKLM5PNXUKDN&amp;dib=eyJ2IjoiMSJ9.RnNGRAhLkrBKEElGy9lMKA.G3WvfQdgBHj1e4gDVCkgq6RWdKl6pBpKhSY_24OUlGg&amp;dib_tag=se&amp;keywords=A+Better+Ape+kumar&amp;qid=1756158041&amp;sprefix=a+better+ape+kuma%2Caps%2C92&amp;sr=8-1&amp;linkCode=ll1&amp;tag=dainou-20&amp;linkId=f708a581d9d01486469225c9f0c93b46&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" rel="noopener">A Better Ape</a> </em>(OUP, 2022). In addition to his academic work, he writes about philosophy, teaching, and society at his blog, <a href="https://openquestionsblog.substack.com/" target="_blank" rel="noopener"><em>Open Questions</em></a>.</p>
<p>&#8220;To capitalize on the benefits of AI and avoid the risks, teachers must craft two AI policies: one for classes where most of the students will exploit any available shortcuts, and another for classes where most of the students won’t,&#8221; Kumar writes.</p>
<p>He&#8217;s teaching an intro-level course and an upper-level course this term, and in what follows he shares elements of the different approaches to student AI use he is taking in them.</p>
<hr />
<p><img loading="lazy" decoding="async" class="wp-image-54769 aligncenter" src="https://dailynous.com/wp-content/uploads/2025/08/traffic-light-disambiguation-2-justin-weinberg.png" alt="" width="801" height="435" srcset="https://dailynous.com/wp-content/uploads/2025/08/traffic-light-disambiguation-2-justin-weinberg.png 1218w, https://dailynous.com/wp-content/uploads/2025/08/traffic-light-disambiguation-2-justin-weinberg-300x163.png 300w, https://dailynous.com/wp-content/uploads/2025/08/traffic-light-disambiguation-2-justin-weinberg-1024x557.png 1024w, https://dailynous.com/wp-content/uploads/2025/08/traffic-light-disambiguation-2-justin-weinberg-768x417.png 768w, https://dailynous.com/wp-content/uploads/2025/08/traffic-light-disambiguation-2-justin-weinberg-400x217.png 400w" sizes="auto, (max-width: 801px) 100vw, 801px" /></p>
<h2 style="text-align: center;">You Don’t Need an AI Policy. You Need Two.</h2>
<p style="text-align: center;"><strong>One policy to preserve the take-home essay, another to explore the frontiers of education</strong></p>
<h2 style="text-align: center;"><em><strong>by Victor Kumar</strong></em></h2>
<p>Many university professors <a href="https://openquestionsblog.substack.com/p/weapon-of-mass-plagiarism-reprise">(like me)</a> are worried that AI will short-circuit learning. By outsourcing written assignments to LLMs like ChatGPT or Claude, students avoid thinking for themselves. If we don’t curb this practice, they’ll learn nothing in our courses, and the value of a university education will collapse.</p>
<p>Yet others (<a href="https://openquestionsblog.substack.com/p/dont-outsource-your-thinking-to-chatgpt">also me</a>) argue that AI&#8212;when used properly&#8212;accelerates learning. Like other information technologies that once seemed threatening but ultimately proved invaluable, LLMs scaffold cognition and allow users to ascend to new intellectual heights. We just need to figure out how to unlock this potential in our classrooms.</p>
<p>So who’s right?</p>
<p>Both are. (I’m right twice over.) The same technology that threatens cognitive atrophy also promises to nurture intellectual growth.</p>
<p>You might search for a single AI policy that minimizes the odds of degrading student learning and maximizes the odds of enhancing it. Yet it’s impossible to devise take-home assignments that students won’t game with AI. Ask students to have an LLM generate an essay and then critique it themselves? Some will just outsource the critique as well.</p>
<p>To capitalize on the benefits of AI and avoid the risks, teachers must craft two AI policies: one for classes where most of the students will exploit any available shortcuts, and another for classes where most of the students won’t.</p>
<p>Think, paradigmatically, about non-majors in lower-level, general-education courses who may never take another course in the subject again vs. majors in upper-level courses who are enthusiastic about the subject and are considering graduate programs or related professional degrees. The distinction isn’t quite so neat; which category your students fall into isn’t fixed in stone. A good teacher may be able to coax students into the responsible camp.</p>
<p>Once you’ve sized up your class, you deploy one of two policies. For the opportunists, you eliminate the possibility of benefiting from AI. For the enthusiasts, you assume most won’t outsource, guide them toward constructive uses of AI, and invite them to discover their own.</p>
<h3>For the Opportunists</h3>
<p>Obviously, you can’t rely on opportunists to abstain voluntarily. Nor should you constantly play cop. Professors can’t reliably detect AI-generated essays anyway, which now easily earn Bs or higher.</p>
<p>Yes, you can spot obvious slop and use context clues&#8212;a fluid and knowledgeable essay from a student who stumbles through classroom discussion&#8212;but you’re going to miss most cases and, worse, falsely accuse innocent students. As Henry Shevlin <a href="https://x.com/dioscuri/status/1920195911356997697">quips</a>, to think otherwise is to fall prey to the toupee fallacy: “noticing only the bad cases, oblivious when it’s convincing.” <a href="https://www.sciencedirect.com/science/article/pii/S2666920X24000109">Studies</a> <a href="https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0305354">consistently</a> <a href="https://www.sciencedirect.com/science/article/pii/S1477388025000131">reveal</a> unreliable detection and overconfidence. Those who use LLMs for writing tasks frequently are an <a href="https://arxiv.org/abs/2501.15654">exception</a> to this rule, but only a tiny sliver of humanities instructors fall into this category.</p>
<p>What’s more, the savviest students aren’t even asking LLMs to write essays outright. They’re asking for outlines and refinements&#8212;writing the essay themselves but letting AI shoulder 90% of the intellectual load. Trying to spot such essays is a fool’s errand.</p>
<p>Some throw up their hands and conclude that the only option is to replace essays with exams. This would be a shame. Slow, patient writing is the highest form of thinking in the humanities. And there’s a superior alternative: low-stakes essays that train students to excel on exams.</p>
<p>Here’s how I’m implementing this strategy with eighty students in my Intro Ethics course this Fall.</p>
<p>As usual, I’ll assign short, weekly take-home essays (max 300 words), but this time with a twist: students will begin writing them in class before revising them at home (worth 20% of their final grade). I’ll grade them for completion only, minimizing my workload and reducing the incentive to cheat. But I’ll also give three in-class essay exams (60%), where students write short essays identical to their weekly take-homes&#8212;the idea being that by writing the essays themselves, they’ll do better on the exams.</p>
<p>Foolproof, right? No, obviously. Some students cheat not for grades but to avoid thinking. But only some; to believe otherwise is far too cynical and sells students short. Plus, I care more about the learners than the cheaters&#8212;as we all should. And the goal is to encourage some students to learn who might otherwise cheat, shifting behavior at the margins.</p>
<p>The policy isn’t perfect, but it’s better than the alternatives. Compared to switching entirely to exams, it preserves opportunities for deep thinking. Another strength is that it doesn’t matter much if some students cheat. I’m increasing participation credit from 10% to 20%, so 80% of their grade is LLM-proof anyway.</p>
<p>One challenge remains: convincing students that writing the essays will help them on the exams. Honestly, I’m not certain it will! But I’ll make my case, explaining a basic premise of our discipline&#8212;in essence, that you never understand anything as thoroughly as when you transcribe, revise, and clarify (indeed create) your thoughts. That’s all they need to do to prepare for the exams, during which I’ll hand out copies of the relevant articles, so no need to memorize or cram.</p>
<h3>For the Enthusiasts</h3>
<p>That covers the opportunists. What about the enthusiasts? The students you can trust to engage with ideas for their own sake, motivated by the pleasure of thinking? For classes where they predominate, my script flips.</p>
<p>I won’t require them to use AI (unlike some <a href="https://napinillos.substack.com/p/students-should-write-with-ai-but">daring souls</a>), but I do want to show them how it can scaffold their thinking. They don’t need me, a middle-aged professor, to teach them basic AI literacy. But helping them become good researchers and writers is precisely my job; the people who excel at these tasks in the future won’t be those who work solo or simply outsource (contrary to what some <a href="https://dailynous.com/2025/08/12/how-to-justify-an-ai-ban-in-your-classroom-guest-post/">suggest</a>) but, rather, <em>those who skillfully integrate their knowledge with AI tools</em>. I want the people filling tomorrow’s professional roles to know some philosophy. And while some students are already using AI in ways I’ve never dreamed of, most don’t yet know how to harness AI to enrich their intellects.</p>
<p>To truly grasp AI’s potential, academics must stop viewing it solely through the lens of plagiarism detection. AI is not a player piano; it’s a choose-your-own- adventure research book. At least, it can be, provided you have an intuitive sense of when it’s reliable&#8212;for common knowledge rather than obscure particulars&#8212;and double-check when there’s any doubt. Know what’s <a href="https://openquestionsblog.substack.com/p/why-arent-you-using-chatgpt">at stake:</a> “foregoing ChatGPT might save you from one error, but you’ll forfeit a hundred truths.”</p>
<p>As a researcher, you can use AI to hunt for sources, produce summaries of well-worn topics, and generate examples. Some uses invite <a href="https://dailynous.com/2025/08/19/the-ethics-of-using-ai-in-philosophical-research/">ethical scrutiny</a>. Some require caution. Take brainstorming: it’s worth consulting an LLM to spark ideas, but exhaust your own ideas first. Otherwise, as Ethan Mollick <a href="https://www.oneusefulthing.org/p/against-brain-damage">warns</a>, you risk crowding out your own creativity and surrendering intellectual agency.</p>
<p>If a book is potentially relevant to your research but you lack the time or interest to read the entire thing, you can upload it and ask the LLM where it covers your topic, then dive into the pages it flags. Unlike a book, you can fire follow-up questions at it tailored to your interests, background, and knowledge (or lack thereof). <a href="https://openquestionsblog.substack.com/p/dont-outsource-your-thinking-to-chatgpt">Elsewhere</a>, I write:</p>
<p style="padding-left: 40px;"><em>As with other cognitive-enhancing information technologies, the key is interactive engagement that upgrades cognitive functioning&#8212;rather than one-shot prompting. Each exchange should deepen your understanding, leading you to synthesize information and refine your inquiry.</em></p>
<p>You can also solicit feedback on arguments, though you mustn’t defer; you have to make the call yourself about whether the ideas are any good. AI is blind without IA&#8212;intellectual autonomy.</p>
<p>In my upper-level course this Fall, we’ll mostly do what we usually do: Socratic dialogue, sparked by their critical reactions to the readings. But we’ll also carve out class time to use AI constructively, as a model for what to do on their own.</p>
<p>We’ll craft Deep Research queries to generate literature surveys&#8212;warning them to take these with a grain of salt since the real value lies in the primary sources it uncovers. We’ll feed arguments into the system and solicit criticism, evaluating for ourselves which hit the mark and which miss. Eventually, we’ll submit drafts for writing feedback.</p>
<h3>Experiment, Observe, Share</h3>
<p>These are just starting points&#8212;I’m hardly the most qualified guide. But I can learn. I’ll seek out ideas from professors like <a href="https://dailynous.com/2025/08/21/intent-amplified-teaching-students-how-to-learn-with-artificial-intelligence/">Gus Skorburg</a>, who is developing a course that trains students “to use AI to learn rather than avoid learning.” I also expect students to generate ideas I haven’t thought of.</p>
<p>Eventually, social scientists will study the effects of different AI policies and produce hard evidence about the advantages and tradeoffs. Until then, we should think carefully and adopt whatever policies strike us as effective&#8212;even if radically different from mine&#8212;and report our experiences afterward. Share on social media, email your department listservs, or (<a href="https://openquestionsblog.substack.com/">like me</a>) start a blog.</p>
<p>Too many academics hold firm beliefs about LLM capabilities while having barely interacted with the technology. Informed decisions demand first-hand knowledge. It doesn’t take much&#8212;you can quickly gain competence by <a href="https://bigthink.com/business/principles-for-leading-with-ai/">“inviting AI to the table”</a> on whatever you happen to be working on. You could even start by running your draft AI policy past ChatGPT.</p><p>The post <a href="https://dailynous.com/2025/08/26/you-dont-need-an-ai-policy-you-need-two-guest-post/">You Don’t Need an AI Policy — You Need Two (guest post)</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>