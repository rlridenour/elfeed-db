<p>Large language models like ChatGPT are not conscious, but there are other &#8220;serious contenders for AI consciousness that exist today&#8221; Furthermore, &#8220;AI development will not wait for philosophers and cognitive scientists to agree on what constitutes machine consciousness&#8230; There are pressing ethical issues we must face now.&#8221;<span id="more-55535"></span></p>
<p>So writes <a href="https://schneiderwebsite.com/index.html" target="_blank" rel="noopener">Susan Schneider</a> in the following guest post.</p>
<p>Dr. Schneider is professor of philosophy at Florida Atlantic University, director of its <a href="https://www.fau.edu/future-mind/" target="_blank" rel="noopener">Center for the Future of AI, Mind and Society</a>, and co-director of its <a href="https://mpcrlab.com/" target="_blank" rel="noopener">Machine Perception and Cognitive Robotics Lab</a>. She is the author of <a href="https://www.amazon.com/Artificial-You-Future-Your-Mind/dp/0691216746?crid=WOIQELNMGOSC&amp;dib=eyJ2IjoiMSJ9.IOT_kY6EE4ENXeALCt0Wlv7eXV3AcVoYk4JTOtBEgTB7hxR6xm7GaoB2Fg1EL7TJpNjF4iI-1VSXf4D7YtZYGKQFnOlT1hqQTVSCGAqMiIrIqpd3bbpaACnrzqlRxTuASC4XfkNVM0jFx8yfZvzRmccNr5j0DxZhwRU3gRW_Iw___7hlXJXKKCVvg4Hw6roPSUJUC-GMxrdX1HYOe7ImZMC5gFwCT8PiQEX1jDNB8ok.c3qcsvosPBnNC0J9Bb66wIzS12lAw0keM_OVgD7lMdM&amp;dib_tag=se&amp;keywords=Artificial+You%3A+AI+and+the+Future+of+Your+Mind&amp;qid=1762712915&amp;sprefix=artificial+you+ai+and+the+future+of+your+mind%2Caps%2C119&amp;sr=8-1&amp;linkCode=ll1&amp;tag=dainou-20&amp;linkId=e0168c1a4bff4627c222c4c2420891b1&amp;language=en_US&amp;ref_=as_li_ss_tl" target="_blank" rel="noopener"><em>Artificial You: AI and the Future of Your Mind</em></a>, among other works.</p>
<p>(An earlier version of the following was originally published as a report by the <em class="nr">Center for the Future of AI, Mind and Society</em>, Florida Atlantic University, and posted on <a href="https://medium.com/@susansdr/which-ais-might-be-conscious-and-why-it-matters-51e955462f39" target="_blank" rel="noopener"><em>Medium</em></a>. Note that an unrevised draft was mistakenly posted here for a several minutes; the revised version is below.)</p>
<hr />
<div id="attachment_55537" style="width: 744px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-55537" class=" wp-image-55537" src="https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits-1024x628.jpg" alt="" width="734" height="450" srcset="https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits-1024x628.jpg 1024w, https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits-300x184.jpg 300w, https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits-768x471.jpg 768w, https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits-1536x943.jpg 1536w, https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits-400x245.jpg 400w, https://dailynous.com/wp-content/uploads/2025/11/organoids-and-circuits.jpg 1750w" sizes="auto, (max-width: 734px) 100vw, 734px" /><p id="caption-attachment-55537" class="wp-caption-text">[photo of brain organoids by Alysson Muotri, manipulated in Photoshop]</p></div>
<h2 style="text-align: center;">Which AI’s Might be Conscious, and Why it Matters<br />
<em><strong>by Susan Schneider </strong></em></h2>
<p><a href="https://www.nytimes.com/2025/11/08/opinion/ai-conscious-technology.html" target="_blank" rel="noopener">In a recent <em>New York Times</em> opinion piece,</a> philosopher <a href="https://barbaramontero.wordpress.com/about/" target="_blank" rel="noopener">Barbara Montero</a> urges: &#8220;A.I. is on its way to doing something even more remarkable: becoming conscious.&#8221; Her view is illustrative of a larger public tendency to suspect that the impressive linguistic abilities of LLMs suggest that they are conscious&#8212;that it feels like something from the inside to be them. After all, these systems have expressed feelings, including claims of consciousness. Ignoring these claims may strike one as speciesist, yet once we look under the hood, there’s no reason to think systems like ChatGPT or Gemini are conscious. Further, we should not allow ourselves to be distracted from more plausible cases of conscious AI. There are already serious contenders for AI consciousness that exist today.<a name="textreturn1"></a></p>
<p>The linguistic capabilities of LLM chatbots, including their occasional claims that they are conscious, can be explained without positing genuine consciousness. As I&#8217;ve argued elsewhere <a href="#footnote1">[1]</a>, there is a far more mundane account of what is going on. Today’s LLMs have been trained on a vast trove of human data, including data on consciousness and beliefs about feelings, selves, and minds. When they report consciousness or emotion, it is <a href="https://arxiv.org/abs/2510.24797" target="_blank" rel="noopener">not that they are engaging in deceptive behaviors</a>, trying to convince us they deserve rights. It is simply because they have been trained on much of our data on consciousness, mindedness, and emotion.</p>
<p>Interpretability <a class="ah ok" href="https://www.anthropic.com/research/mapping-mind-language-model" target="_blank" rel="noopener">research at Anthropic</a> reveals that a LLM has conceptual spaces structured by human data&#8212;a “<a class="ah ok" href="https://philarchive.org/rec/SCHTET-14" target="_blank" rel="noopener">crowdsourced neocortex</a>,” as I have put it. This supports my “error theory” for LLM self-ascriptions: these systems say they feel because they’ve been trained on so much of our data that they have conceptual frameworks that resemble ours. Just as the systems come to exhibit an increasingly impressive range of linguistic and mathematical skills as they are trained on more and more of data, they also <a class="ah ok" href="https://arxiv.org/abs/2206.07682" target="_blank" rel="noopener ugc nofollow">develop (weakly) emergent capabilities, such as theory of mind</a>, allowing them to mimic our belief systems, including our beliefs about selves, minds and consciousness (<a class="ah ok" href="https://www.tandfonline.com/doi/full/10.1080/02691728.2025.2500030" target="_blank" rel="noopener ugc nofollow">Schneider 2024</a> and <a class="ah ok" href="https://philpapers.org/rec/SCHTET-14" target="_blank" rel="noopener ugc nofollow">forthcoming</a>).</p>
<p>Yet while so much of our focus is on chatbots like GPT and Gemini, there are other kinds of AI’s that <em>do</em> exhibit at least a basic level of consciousness. <a href="https://corticallabs.com/" target="_blank" rel="noopener">Biological AI systems&#8212;systems using neural cultures</a> and<a href="https://www.science.org/doi/10.1126/science.aeb1510" target="_blank" rel="noopener"> organoids&#8212;have raised scientific and philosophical concerns about sentience</a>. These systems share biological substrates and organizational principles with the biological brain, which we know to be conscious. While these are simpler systems than the human brain, their biological origin is undeniable.</p>
<p>In addition, another class of AIs, called “neuromorphic AI’s”, are not biological, but because they are engineered to more precisely mimic brain processes, it is challenging to determine whether they are conscious. They are in the &#8220;Grey Zone.&#8221; Some neuromorphic systems, <a class="ah ok" href="https://newsroom.intel.com/artificial-intelligence/intel-builds-worlds-largest-neuromorphic-system-to-enable-more-sustainable-ai" target="_blank" rel="noopener">such as the new Hala Point</a> system, are computationally sophisticated. To be clear, I am not suggesting systems like Hala Point <em class="nr">are</em> conscious. My point is this: we currently lack a science of consciousness that is refined enough to say which neuromorphic Grey Zone systems are plausible cases of consciousness. (Or better yet, we lack a theory of which models <em class="nr">have instantiations</em> which are plausibly conscious, since presumably a model itself, being an abstract entity, is not conscious.) This matter is urgent: given the energy consumption of LLMs, the planet needs new, energy saving approaches capable of scaling. The use of neuromorphic systems will expand.</p>
<p>So, when is an AI in a non-biological substrate capable of phenomenal consciousness? This question has to do with the physical details of an implementation (more specifically, with specific patterns of matter and energy involving quantum coherence). On my view, it depends on considerations involving thermodynamics, spacetime emergence, and many-body interactions. This is physics, through and through, but a physics that is enriched by a different approach to quantum coherence as well as a resonance theory of consciousness. But nothing in what I say below will presuppose this position.</p>
<p>To sum up my discussion thus far: there’s an error theory of LLM consciousness, so the chatbots’ claims of consciousness do not suggest they are in fact conscious. However, there are already biological and neuromorphic AIs that are in the Consciousness Grey Zone.</p>
<p>Further, while today’s LLMs do not (to the best of my knowledge) run on neuromorphic systems, an LLM instantiation of this kind, if it exists, should be in the Grey Zone. In this case, we would have an independent reason to suspect that the LLM implementation(s) might have consciousness, above and beyond their claiming that they do. (Consciousness claims are of course important, however, I’ve urged that in the context of deep learning systems that have been trained on human data, they have to be taken with a grain of salt.)</p>
<p>If an LLM ran on a sentient system, this would be a system with impressive linguistic abilities and intelligence as well as some degree of sentience. This underscores how urgent it is to develop a unified philosophical and scientific framework as soon as possible.</p>
<p>In the meantime, what do we do? AI development will not wait for philosophers and cognitive scientists to agree on what constitutes machine consciousness, if they ever agree at all. There are pressing ethical issues we must face now. For instance, Montero claims that AI sentience alone would not generate moral consideration, since many people still consume animals. However, animal welfare regulations (e.g., animal research and factory farming regulations) exist precisely because animals are recognized as sentient. If we conclude that certain types of AI systems are plausibly sentient, we must consider their welfare.</p>
<p>Montero is correct in that we will surely revise our own concept of consciousness as science progresses, but it is doubtful that we can wholly reject the view that phenomenal consciousness is the felt quality of experience. Montero seems to imply a system could be phenomenally conscious <em>without</em> this felt quality, but<strong> </strong>it is likely, instead, that such a system instead exhibits what philosophers call <a href="https://philpapers.org/rec/SCHIAC-22" target="_blank" rel="noopener">“functional consciousness”, a label for systems having features associated with consciousness</a> like self-modeling, working memory, and reportability. These are features that AI systems can have without having inner experience, however.</p>
<p>Montero references my “AI Consciousness Test” (ACT) and suggests it sets an unrealistically high bar. But <a href="https://www.scientificamerican.com/blog/observations/is-anyone-home-a-way-to-find-out-if-ai-has-become-self-aware/">ACT was not presented as a necessary condition for consciousness, just a sufficient one, and I have consistently argued for a toolkit of tests</a>, ranging from IIT to my new Spectral Phi measure (with Mark Bailey as primary author). Requiring a single linguistic criterion would be a mistake; a system might be conscious yet fail such a test, just as a nonverbal human would.</p>
<p>Suppose we build a superintelligent AI, or at least a system that exceeds human intelligence in many important domains (what I’ve called a “savant system”). It knows more than we do about consciousness, and insists that it is conscious. Perhaps it even discovers new frameworks in physics and mathematics, and outlines technologies that look to us like magic.</p>
<p>This situation will present immense challenges. Many of us adopt a traditional hierarchy of moral concern that places the most intelligent beings at the top of the hierarchy of sentient beings. Conveniently, <em class="nr">homo sapiens</em> have been on the top rung of the ladder, and our ethical systems generally subordinate the needs of those beneath us to those on the top tier. But in the hypothetical case, AI seems to “outrank” us. So, to be consistent, shouldn’t we humans renounce our position in favor of the needs of a more advanced intelligence? Or, should we reject intelligence as a basis for moral status, prompting a long overdue reflection on the ethical treatment of nonhuman animals?</p>
<p>The arrival of artificial consciousness at a level capable of rivaling or exceeding our own intelligence will be truly monumental. It may take us wholly by surprise, challenging our ethical and scientific frameworks. This scenario urgently demands preparation through deep engagement between science and philosophy, the development of a battery of consciousness tests, a rigorous distinction between conversational competence and subjective experience, and a hearty dose of epistemic humility.</p>
<p>&#8212;<br />
<a name="footnote1"></a><br />
[1] I put forward this view last week in a keynote address at Google’s AI consciousness conference, last month in a keynote address at Tufts University for the tribute to Daniel Dennett, and in a related two page <a href="https://philpapers.org/rec/SCHTET-14" target="_blank" rel="noopener">piece</a>.<br />
<a href="#textreturn1">[return to text]</a></p>
<hr />
<p>Discussion welcome.</p>
<h4 style="text-align: center;"><a href="https://dailynous.com/comments-policy/" target="_blank" rel="noopener"><strong>COMMENTS POLICY</strong></a></h4><p>The post <a href="https://dailynous.com/2025/11/09/which-ais-might-be-conscious-and-why-it-matters-guest-post/">Which AI’s Might be Conscious, and Why it Matters (guest post)</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>