<p>Continuing the development of my local <code>ollama</code> LLM client called <code>ollama-buddy</code>&hellip;</p>
<p><a href="https://github.com/captainflasmr/ollama-buddy">https://github.com/captainflasmr/ollama-buddy</a></p>
<p>The basic functionality, I think, is now there (and now literally zero configuration required).  If a default model isn&rsquo;t set I just pick the first one, so LLM chat can take place immediately.</p>
<figure><img src="https://emacs.dyerdwelling.family/ox-hugo/20250311180746-emacs--Ollama-Buddy-0-7-1-Org-mode-Chat-Parameter-Control-and-JSON-Debugging.jpg" width="100%">
</figure>

<p>Now I&rsquo;m getting more into this chat client malarkey, my original idea of a very minimal chat client to interface to <code>ollama</code> is starting to skew into supporting as much of the <code>ollama</code> RESTful API as possible.  Hence in this update a more advanced approach is creeping in, including setting up various subtle model parameters and providing a debugging window to monitor incoming raw JSON (pretty printed of course).  Hopefully, these features will remain tucked away for advanced users, I’ve done my best to keep them unobtrusive (but not <strong>too</strong> hidden). The tool is still designed to be a helpful companion to interface to <code>ollama</code> through Emacs, just now with more powerful options under the hood.</p>
<p>Also a note about converting the chat buffer into org-mode.  My original intention was to keep the chat buffer as a very simple almost &ldquo;no mode&rdquo; buffer, with just text and nothing else.  However, with more consideration, I felt that converting this buffer into org-mode actually held quite a few benefits:</p>
<ul>
<li>Each prompt could be a heading, hence outlining and folding can be activated!</li>
<li>Navigation between prompts now comes for free (especially if you are using <code>org-use-speed-commands</code>)</li>
<li>The org ox export backend now allows us to export to formats of many different kinds</li>
</ul>
<p>I&rsquo;m sure there are more as this list isn&rsquo;t quite the &ldquo;quite a few benefits&rdquo; I was hoping for :(</p>
<p>I have a local keymap defined with some ollama-buddy specific keybindings, and as of yet I haven’t encountered any conflicts with commonly used <code>org-mode</code> bindings but we shall see how it goes. I think for this package it is important to have a quick chatting mechanism, and what is faster than a good keybind?</p>
<p>Finally, just a note on the pain of implementing a good prompt mechanism.  I had a few goes at it and I think I now have an acceptable robust solution.  I kept running into little annoying edge cases and I ended up having to refactor quite a bit.  My original idea for this package involved a simple “mark region and send” as at the time I had a feeling that the implementation of a good prompt mechanism would be tough - how right I was!.  Things got even trickier with the move to <code>org-mode</code>, since each prompt heading should contain meaningful content for clean exports and I had to implement a mechanism to replace prompts intelligently. For example, if the model is swapped and the previous prompt is blank, it gets replaced, though, of course, even this has its own edge cases - gives a new meaning to prompt engineering! :)</p>
<p>Anyways, listed below are my latest changes, with a little deeper dive into more &ldquo;interesting&rdquo; implementations, my next ideas are a little more advanced and are kanban&rsquo;d into my github README at <a href="https://github.com/captainflasmr/ollama-buddy">https://github.com/captainflasmr/ollama-buddy</a> for those that are interested.</p>
<h2 id="0-dot-7-dot-1"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-11 Tue&gt; </span></span> <strong>0.7.1</strong></h2>
<p>Added debug mode to display raw JSON messages in a debug buffer</p>
<ul>
<li>Created new debug buffer to show raw JSON messages from Ollama API</li>
<li>Added toggle function to enable/disable debug mode (ollama-buddy-toggle-debug-mode)</li>
<li>Modified stream filter to log and pretty-print incoming JSON messages</li>
<li>Added keybinding C-c D to toggle debug mode</li>
<li>Updated documentation in welcome message</li>
</ul>
<h2 id="0-dot-7-dot-0"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-11 Tue&gt; </span></span> <strong>0.7.0</strong></h2>
<p>Added comprehensive Ollama parameter management</p>
<ul>
<li>Added customization for all Ollama option API parameters with defaults</li>
<li>Only send modified parameters to preserve Ollama defaults</li>
<li>Display active parameters with visual indicators for modified values</li>
<li>Add keybindings and help system for parameter management</li>
<li>Remove redundant temperature controls in favor of unified parameters</li>
</ul>
<p>Introduced parameter management capabilities that give you complete control over your Ollama model&rsquo;s behavior through the options in the ollamas API.</p>
<p>Ollama&rsquo;s API supports a rich set of parameters for fine-tuning text generation, from controlling creativity with <code>temperature</code> to managing token selection with <code>top_p</code> and <code>top_k</code>. Until now, Ollama Buddy only exposed the <code>temperature</code> parameter, but this update unlocks the full potential of Ollama&rsquo;s parameter system!</p>
<h3 id="key-features">Key Features:</h3>
<ul>
<li><strong>All Parameters</strong> - set all custom options for the ollama LLM at runtime</li>
<li><strong>Smart Parameter Management</strong>: Only modified parameters are sent to Ollama, preserving the model&rsquo;s built-in defaults for optimal performance</li>
<li><strong>Visual Parameter Interface</strong>: Clear display showing which parameters are active with highlighting for modified values</li>
</ul>
<h2 id="keyboard-shortcuts">Keyboard Shortcuts</h2>
<p>Parameter management is accessible through simple keyboard shortcuts from the chat buffer:</p>
<ul>
<li><code>C-c P</code> - Edit a parameter</li>
<li><code>C-c G</code> - Display current parameters</li>
<li><code>C-c I</code> - Show parameter help</li>
<li><code>C-c K</code> - Reset parameters to defaults</li>
</ul>
<h2 id="0-dot-6-dot-1"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-10 Mon&gt; </span></span> <strong>0.6.1</strong></h2>
<p>Refactored prompt handling so each org header line should now always have a prompt for better export</p>
<ul>
<li>Added functionality to properly handle prompt text when showing/replacing prompts</li>
<li>Extracted inline lambdas in menu actions into named functions</li>
<li>Added fallback for when no default model is set</li>
</ul>
<h2 id="0-dot-6-dot-0"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-08 Sat&gt; </span></span> <strong>0.6.0</strong></h2>
<p>Chat buffer now in org-mode</p>
<ul>
<li>Enabled <code>org-mode</code> in chat buffer for better text structure</li>
<li>Implemented <code>ollama-buddy--md-to-org-convert-region</code> for Markdown to Org conversion</li>
<li>Turn org conversion on and off</li>
<li>Updated keybindings <code>C-c C-o</code> to toggle Markdown to Org conversion</li>
</ul>
<p><strong>Key Features</strong></p>
<ol>
<li>
<p>The chat buffer is now in <code>org-mode</code> which gives the buffer enhanced readability and structure. Now, conversations automatically format user prompts and AI responses with <strong>org-mode headings</strong>, making them easier to navigate.</p>
</li>
<li>
<p>Of course with org-mode you will now get the additional benefits for free, such as:</p>
<ul>
<li>outlining</li>
<li>org export</li>
<li>heading navigation</li>
<li>source code fontification</li>
</ul>
</li>
<li>
<p>Previously, responses in <strong>Ollama Buddy</strong> were displayed in markdown formatting, which wasn’t always ideal for <strong>org-mode users</strong>. Now, you can automatically convert Markdown elements, such as bold/italic text, code blocks, and lists, into proper org-mode formatting.  This gives you the flexibility to work with markdown or org-mode as needed.</p>
</li>
</ol>