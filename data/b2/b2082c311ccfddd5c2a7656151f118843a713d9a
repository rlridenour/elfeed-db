<p id="p1">I <a href="https://bsky.app/profile/viticci.macstories.net/post/3lp7m6i5tls25" rel="noopener noreferrer">received</a> a top-of-the-line Mac Studio (M3 Ultra, 512&nbsp;GB of RAM, 8&nbsp;TB of storage) on loan from Apple last week, and I thought I&rsquo;d use this opportunity to revive something I&rsquo;ve been mulling over for some time: more short-form blogging on MacStories in the form of brief &ldquo;notes&rdquo; with a dedicated <a href="https://www.macstories.net/category/notes/" rel="noopener noreferrer">Notes</a> category on the site. Expect more of these &ldquo;low-pressure&rdquo;, quick posts in the future.</p>
<p id="p2">I&rsquo;ve been sent this Mac Studio as part of my <a href="https://club.macstories.net/posts/automation-academy-how-i-built-a-smarter-web-clipper-for-obsidian-using-shortcuts-and-ai" rel="noopener noreferrer">ongoing experiments</a> with assistive AI and automation, and one of the things I plan to do over the coming weeks and months is playing around with local LLMs that tap into the power of Apple Silicon and the incredible performance headroom afforded by the M3 Ultra and this computer&rsquo;s specs. I have <em>a lot</em> to learn when it comes to local AI (my shortcuts and experiments so far have focused on cloud models and the Shortcuts app combined with the <a href="https://llm.datasette.io/en/stable/" rel="noopener noreferrer">LLM CLI</a>), but since I had to start somewhere, I downloaded <a href="https://lmstudio.ai" rel="noopener noreferrer">LM Studio</a> and <a href="https://ollama.com" rel="noopener noreferrer">Ollama</a>, installed the <a href="https://github.com/taketwo/llm-ollama" rel="noopener noreferrer">llm-ollama</a> plugin, and began experimenting with open-weights models (served from <a href="https://huggingface.co" rel="noopener noreferrer">Hugging Face</a> as well as the Ollama <a href="https://ollama.com/library" rel="noopener noreferrer">library</a>) both in the <a href="https://huggingface.co/docs/hub/en/gguf" rel="noopener noreferrer">GGUF</a> format and Apple&rsquo;s own <a href="https://huggingface.co/docs/hub/en/mlx" rel="noopener noreferrer">MLX</a> framework.</p>
<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-20-at-19-55-02-2x-1747763721300.png" alt="LM Studio."><p class="image-caption">LM Studio.</p></div>
<p id="p4">I posted some of these early tests on <a href="https://bsky.app/profile/viticci.macstories.net/post/3lpa6s2ov3s25" rel="noopener noreferrer">Bluesky</a>. I ran the massive <a href="https://qwenlm.github.io/blog/qwen3/" rel="noopener noreferrer">Qwen3-235B-A22B</a> model (a <a href="https://en.wikipedia.org/wiki/Mixture_of_experts" rel="noopener noreferrer">Mixture-of-Experts</a> model with 235 billion parameters, 22 billion of which activated at once) with both <a href="https://huggingface.co/models?search=Qwen3-235B-A22B" rel="noopener noreferrer">GGUF and MLX</a> using the beta version of the LM Studio app, and these were the results:</p>
<ul id="ul5"><li>GGUF: 16 tokens/second, ~133&nbsp;GB of RAM used</li>
<li>MLX: 24 tok/sec, ~124&nbsp;GB RAM</li>
</ul><p id="p6">As you can see from these first benchmarks (both based on the <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" rel="noopener noreferrer">4-bit quant</a> of Qwen3-235B-A22B), the Apple Silicon-optimized version of the model resulted in better performance both for token generation and memory usage. Regardless of the version, the Mac Studio absolutely didn&rsquo;t care and I could barely hear the fans going.</p>
<p id="p7">I also wanted to play around with the <a href="https://huggingface.co/blog/vlms-2025" rel="noopener noreferrer">new generation of vision models</a> (VLMs) to test modern OCR capabilities of these models. One of the tasks that has become kind of a personal AI <a href="https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals" rel="noopener noreferrer">eval</a> for me lately is taking a long screenshot of a shortcut from the Shortcuts app (using <a href="https://cleanshot.com" rel="noopener noreferrer">CleanShot</a>&rsquo;s scrolling captures) and feed it either as a full-res PNG or PDF to an LLM. As I shared before, due to image compression, the vast majority of cloud LLMs either fail to accept the image as input or compresses the image <em>so much</em> that graphical artifacts lead to severe hallucinations in the text analysis of the image. Only o4-mini-high &ndash; thanks to its more agentic capabilities and tool-calling &ndash; <a href="https://bsky.app/profile/viticci.macstories.net/post/3lmxsf3t3cc2o" rel="noopener noreferrer">was able to produce</a> a decent output; even then, that was only possible because o4-mini-high decided to slice the image in multiple parts and iterate through each one with discrete <a href="https://pypi.org/project/pytesseract/" rel="noopener noreferrer">pytesseract</a> calls. The task took almost seven minutes to run in ChatGPT.</p>
<p id="p8">This morning, I installed the 72-billion parameter version of <a href="https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5" rel="noopener noreferrer">Qwen2.5-VL</a>, gave it a full-resolution screenshot of a 40-action shortcut, and let it run with Ollama and llm-ollama. After 3.5 minutes and around 100&nbsp;GB RAM usage, I got a really good, Markdown-formatted analysis of my shortcut back from the model.</p>
<div class="media-wrapper"><img src="https://cdn.macstories.net/cleanshot-2025-05-20-at-13-17-18-2x-1747763425587.png" alt=""><p class="image-caption"></p></div>
<p id="p10">To make the experience nicer, I even <a href="https://bsky.app/profile/viticci.macstories.net/post/3lpm4hhyqz224" rel="noopener noreferrer">built</a> a small local-scanning utility that lets me pick an image from Shortcuts and runs it through Qwen2.5-VL (72B) using the &lsquo;Run Shell Script&rsquo; action on macOS. It worked beautifully on my first try. Amusingly, the smaller version of Qwen2.5-VL (32B) thought my photo of <a href="https://www.protoarc.com/collections/mice/products/em11-nl-vertical-mouse?variant=42318201716825" rel="noopener noreferrer">ergonomic mice</a> was a &ldquo;collection of seashells&rdquo;. Fair enough: there&rsquo;s a reason bigger models are heavier and costlier to run.</p>
<p id="p11">Given my struggles with OCR and document analysis with cloud-hosted models, I&rsquo;m <em>very</em> excited about the potential of local VLMs that bypass memory constraints thanks to the M3 Ultra and provide accurate results in just a few minutes without having to upload private images or PDFs anywhere. I&rsquo;ve been <a href="https://club.macstories.net/posts/automation-academy-how-i-built-a-smarter-web-clipper-for-obsidian-using-shortcuts-and-ai" rel="noopener noreferrer">writing</a> a lot about this idea of &ldquo;hybrid automation&rdquo; that combines traditional Mac scripting tools, Shortcuts, and LLMs to unlock workflows that just weren&rsquo;t possible before; I feel like the power of this Mac Studio is going to be an amazing accelerator for that.</p>
<p id="p12">Next up on my list: understanding how to run MLX models with <a href="https://github.com/ml-explore/mlx-lm" rel="noopener noreferrer">mlx-lm</a>, investigating long-context models with dual-chunk attention support (looking at you, <a href="https://qwenlm.github.io/blog/qwen2.5-1m/" rel="noopener noreferrer">Qwen 2.5</a>), and experimenting with <a href="https://blog.google/technology/developers/gemma-3/" rel="noopener noreferrer">Gemma 3</a>. Fun times ahead!</p>
<hr /><h3>Access Extra Content and Perks</h3><p><p>Founded in 2015, <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">Club MacStories</a> has delivered exclusive content every week for nearly a decade.</p>
<p>What started with weekly and monthly email newsletters has blossomed into <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">a family of memberships</a> designed every MacStories fan.</p>
<p><strong><a href="https://club.macstories.net/plans/club" rel="noopener noreferrer">Club MacStories</a></strong>: Weekly and monthly newsletters via email and the web that are brimming with apps, tips, automation workflows, longform writing, early access to the <a href="https://www.macstories.net/unwind/" rel="noopener noreferrer">MacStories Unwind podcast</a>, periodic giveaways, and more;</p>
<p><strong><a href="https://club.macstories.net/plans/plus" rel="noopener noreferrer">Club MacStories+</a></strong>: Everything that Club MacStories offers, plus an active Discord community, advanced search and custom RSS features for exploring the Club&rsquo;s entire back catalog, bonus columns, and dozens of app discounts;</p>
<p><strong><a href="https://club.macstories.net/plans/premier" rel="noopener noreferrer">Club Premier</a></strong>: All of the above <em>and</em> AppStories+, an extended version of our flagship podcast that&rsquo;s delivered early, ad-free, and in high-bitrate audio.</p>
<p>Learn more <a href="https://club.macstories.net/plans?utm_source=ms&amp;utm_medium=web-inline" rel="noopener noreferrer">here</a> and from our <a href="https://club.macstories.net/faq" rel="noopener noreferrer">Club FAQs</a>.</p>
</p><a href='https://club.macstories.net/?utm_source=ms&#038;utm_medium=rss'>Join Now</a>