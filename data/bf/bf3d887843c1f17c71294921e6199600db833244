
<p>Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar, from Apple’s Machine Learning Research team:</p>

<blockquote>
  <p>Recent generations of frontier language models have introduced
Large Reasoning Models (LRMs) that generate detailed thinking
processes before providing answers. While these models demonstrate
improved performance on reasoning benchmarks, their fundamental
capabilities, scaling properties, and limitations remain
insufficiently understood. [...] Through extensive experimentation
across diverse puzzles, we show that frontier LRMs face a complete
accuracy collapse beyond certain complexities. Moreover, they
exhibit a counterintuitive scaling limit: their reasoning effort
increases with problem complexity up to a point, then declines
despite having an adequate token budget. By comparing LRMs with
their standard LLM counterparts under equivalent inference
compute, we identify three performance regimes: (1) low-complexity
tasks where standard models surprisingly outperform LRMs, (2)
medium-complexity tasks where additional thinking in LRMs
demonstrates advantage, and (3) high-complexity tasks where both
models experience complete collapse. We found that LRMs have
limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across puzzles. We also
investigate the reasoning traces in more depth, studying the
patterns of explored solutions and analyzing the models’
computational behavior, shedding light on their strengths,
limitations, and ultimately raising crucial questions about their
true reasoning capabilities.</p>
</blockquote>

<p>The full paper is quite readable, but today was my travel day and I haven’t had time to dig in. And <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf">it’s a PDF</a> so I couldn’t read it on my phone. (Coincidence or not that this dropped on the eve of WWDC?)</p>

<p>My basic understanding after a skim is that the paper shows, or at least strongly suggests, that LRMs don’t “reason” at all. They just use vastly more complex pattern-matching than LLMs. The result is that LRMs effectively overthink on simple problems, outperform LLMs on mid-complexity puzzles, and fail in the same exact way LLMs do on high-complexity tasks and puzzles.</p>

<div>
<a  title="Permanent link to ‘Apple Researchers Publish Paper on the Limits of Reasoning Models (Showing That They’re Not Really ‘Reasoning’ at All)’"  href="https://daringfireball.net/linked/2025/06/08/apple-research-lrms">&nbsp;★&nbsp;</a>
</div>

	