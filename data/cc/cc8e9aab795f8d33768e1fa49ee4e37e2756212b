<p id="p1">Yesterday, Wayne Ma, reporting for The Information, <a href="https://www.theinformation.com/articles/apple-fumbled-siris-ai-makeover" rel="noopener noreferrer">published an outstanding story</a> detailing the internal turmoil at Apple that led to the delay of the highly anticipated Siri AI features <a href="https://www.macstories.net/linked/apple-delays-siri-personalization/" rel="noopener noreferrer">last month</a>. From the article:</p>
<blockquote id="blockquote2"><p>
  In November 2022, OpenAI released ChatGPT to a thunderous response from the tech industry and public. Within Giannandrea&rsquo;s AI team, however, senior leaders didn&rsquo;t respond with a sense of urgency, according to former engineers who were on the team at the time.</p>
<p>  The reaction was different inside Federighi&rsquo;s software engineering group. Senior leaders of the Intelligent Systems team immediately began sharing papers about LLMs and openly talking about how they could be used to improve the iPhone, said multiple former Apple employees.</p>
<p>  Excitement began to build within the software engineering group after members of the Intelligent Systems team presented demos to Federighi showcasing what could be achieved on iPhones with AI. Using OpenAI&rsquo;s models, the demos showed how AI could understand content on a user&rsquo;s phone screen and enable more conversational speech for navigating apps and performing other tasks.
</p></blockquote>
<p id="p3">Assuming the details in this report are correct, I truly can&rsquo;t imagine how one could possibly see the debut of ChatGPT two years ago and <em>not</em> feel a sense of urgency. Fortunately, other teams at Apple did, and it sounds like they&rsquo;re the folks who have now been put in charge of the next generation of Siri and AI.</p>
<p id="p4">There are plenty of other details worth reading in the full story (especially the parts about what Rockwell&rsquo;s team wanted to accomplish with Siri and AI on the Vision Pro), but one tidbit in particular stood out to me: Federighi has now given the green light to rely on third-party, open-source LLMs to build the next wave of AI features.</p>
<blockquote id="blockquote5"><p>
  Federighi has already shaken things up. In a departure from previous policy, he has instructed Siri&rsquo;s machine-learning engineers to do whatever it takes to build the best AI features, even if it means using open-source models from other companies in its software products as opposed to Apple&rsquo;s own models, according to a person familiar with the matter.
</p></blockquote>
<p id="p6">&ldquo;Using&rdquo; open-source models from other companies doesn&rsquo;t necessarily mean <em>shipping</em> consumer features in iOS powered by external LLMs. I&rsquo;ve seen some people interpret this paragraph as Apple preparing to release a local Siri powered by <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" rel="noopener noreferrer">Llama 4</a> or DeepSeek, and I think we should pay more attention to that &ldquo;<em>build</em> the best AI features&rdquo; (emphasis mine) line.</p>
<p id="p7">My read of this part is that Federighi might have instructed his team to use <a href="https://www.ibm.com/think/topics/knowledge-distillation" rel="noopener noreferrer">distillation</a> to better train Apple&rsquo;s in-house models as a way to accelerate the development of the delayed Siri features and put them back on the company&rsquo;s roadmap. Given <a href="https://9to5mac.com/2025/03/24/tim-cook-says-chinas-deepseek-ai-is-excellent-during-visit-to-country/" rel="noopener noreferrer">Tim Cook&rsquo;s public appreciation</a> for DeepSeek and this morning&rsquo;s <a href="https://www.nytimes.com/2025/04/11/technology/apple-issues-trump-tariffs.html" rel="noopener noreferrer">New York Times report</a> that the delayed features may come this fall, I wouldn&rsquo;t be shocked to learn that Federighi told Siri&rsquo;s ML team to <a href="https://medium.com/data-science-in-your-pocket/what-are-deepseek-r1-distilled-models-329629968d5d" rel="noopener noreferrer">distill DeepSeek R1&rsquo;s reasoning knowledge</a> into a new variant of their <a href="https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models" rel="noopener noreferrer">&sim;3 billion parameter foundation model</a> that runs on-device. Doing that wouldn&rsquo;t mean that iOS 19&rsquo;s Apple Intelligence would be &ldquo;powered by DeepSeek&rdquo;; it would just be a faster way for Apple to catch up without throwing away the foundational model they unveiled last year (which, <a href="https://x.com/markgurman/status/1900616400823030024" rel="noopener noreferrer">supposedly</a>, had a ~30% error rate).</p>
<p id="p8">In thinking about this possibility, I got curious and decided to check out the <a href="https://arxiv.org/html/2407.21075v1" rel="noopener noreferrer">original paper that Apple published</a> last year with details on how they trained the two versions of AFM (Apple Foundation Model): <em>AFM-server</em> and <em>AFM-on-device</em>. The latter would be the smaller, ~3 billion model that gets downloaded on-device with Apple Intelligence. I&rsquo;ll let you guess what Apple did to improve the performance of the smaller model:</p>
<blockquote id="blockquote9"><p>
  For the on-device model, we found that knowledge distillation&nbsp;(<a href="https://arxiv.org/html/2407.21075v1#bib.bib20" rel="noopener noreferrer">Hinton et&nbsp;al., 2015</a>) and structural pruning are effective ways to improve model performance and training efficiency. These two methods are complementary to each other and work in different ways. More specifically, before training AFM-on-device, we initialize it from a pruned 6.4B model (trained from scratch using the same recipe as AFM-server), using pruning masks that are learned through a method similar to what is described in&nbsp;(<a href="https://arxiv.org/html/2407.21075v1#bib.bib53" rel="noopener noreferrer">Wang et&nbsp;al., 2020</a>; <a href="https://arxiv.org/html/2407.21075v1#bib.bib57" rel="noopener noreferrer">Xia et&nbsp;al., 2023</a>).
</p></blockquote>
<p id="p10">Or, more simply:</p>
<blockquote id="blockquote11"><p>
  AFM-server core training is conducted from scratch, while AFM-on-device is distilled and pruned from a larger model.
</p></blockquote>
<p id="p12">If the distilled version of AFM-on-device that was tested until a few weeks ago produced a wrong output one third of the time, perhaps it would be a good idea to perform distillation again based on knowledge from other smarter and larger models? Say, using <a href="https://9to5mac.com/2025/03/25/apple-is-about-to-spend-1-billion-on-nvidia-servers-for-ai-analyst/" rel="noopener noreferrer">250 Nvidia GB300 NVL72 servers</a>?</p>
<p id="p13">(One last fun fact: per their paper, Apple trained AFM-server on 8192&nbsp;<a href="https://cloud.google.com/tpu/docs/v4" rel="noopener noreferrer">TPUv4</a> chips for 6.3 trillion tokens; that setup still wouldn&rsquo;t be as powerful as &ldquo;only&rdquo; 250 modern <a href="https://www.nvidia.com/en-us/data-center/gb300-nvl72/" rel="noopener noreferrer">Nvidia servers</a> today.)</p>
<p>â†’ Source: <a href='https://www.theinformation.com/articles/apple-fumbled-siris-ai-makeover' target='_blank'>theinformation.com</a></p>