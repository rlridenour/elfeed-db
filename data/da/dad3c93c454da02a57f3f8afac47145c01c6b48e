<p>More improvements to <code>ollama-buddy</code> <a href="https://github.com/captainflasmr/ollama-buddy">https://github.com/captainflasmr/ollama-buddy</a></p>
<p>The main addition is that of system prompts, which allows setting the general tone and guidance of the overall chat.  Currently the system prompt can be set at any time and turned on and off but I think to enhance my model/command for each menu item concept, I could also add a :system property to the menu alist definition to allow even tighter control of a menu action to prompt response.</p>
<figure><img src="https://emacs.dyerdwelling.family/ox-hugo/20250314132104-emacs--Ollama-Buddy-0-8-0-Added-System-Prompts-Model-Info-and-simpler-menu-model-assignment.jpg" width="100%">
</figure>

<p>Also now I have parameter functionality working for fine grained control, I could add these individual parameters for each menu command, for example the <code>temperature</code> could be very useful in this case to play around with the randomness/casualness of the response.</p>
<p>The next improvement will likely involve adding support for interacting more directly with Ollama to create and pull models. However, I&rsquo;m still unsure whether performing this within Emacs is the best approach, I could assume that all models are already set up in Ollama.</p>
<p>That said, importing a GGUF file might be a useful feature, possibly from within <code>dired</code>. Currently, this process requires multiple steps: creating a simple model file that points to the GGUF file on disk, then running the <code>ollama create</code> command to import it. Streamlining this workflow could enhance usability.</p>
<p>Then maybe on to embeddings, of which I currently have no idea, haven&rsquo;t read up on it, nuffin, but that is something to look forward to! :)</p>
<p>Anyways, here is the latest set of updates to Ollama Buddy:</p>
<h2 id="0-dot-8-dot-0"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-14 Fri&gt; </span></span> <strong>0.8.0</strong></h2>
<p>Added system prompt support</p>
<ul>
<li>Added <code>ollama-buddy--current-system-prompt</code> variable to track system prompts</li>
<li>Updated prompt area rendering to distinguish system prompts</li>
<li>Modified request payload to include system prompt when set</li>
<li>Enhanced status bar to display system prompt indicator</li>
<li>Improved help menu with system prompt keybindings</li>
</ul>
<p>So this is system prompt support in Ollama Buddy!, allowing you to set and manage system-level instructions for your AI interactions. This feature enables you to define a <strong>persistent system prompt</strong> that remains active across user queries, providing better control over conversation context.</p>
<p><strong>Key Features</strong></p>
<p>You can now designate any user prompt as a system prompt, ensuring that the AI considers it as a guiding instruction for future interactions. To set the system prompt, use:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>C-u C-c C-c
</span></span></code></pre></div><p><strong>Example:</strong></p>
<ol>
<li>Type:</li>
</ol>
<!--listend-->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Always respond in a formal tone.
</span></span></code></pre></div><ol>
<li>Press <code>C-u C-c C-c</code> This prompt is now set as the <strong>system prompt</strong> and any further chat ollama responses will adhere to the overarching guidelines defined in the prompt.</li>
</ol>
<p>If you need to clear the system prompt and revert to normal interactions, use:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>C-u C-u C-c C-c
</span></span></code></pre></div><p><strong>How It Works</strong></p>
<ul>
<li>The active <strong>system prompt</strong> is stored and sent with each user prompt.</li>
<li>A &ldquo;S&rdquo; indicator appears in the status bar when a system prompt is active.</li>
<li>The request payload now includes the system role, allowing AI to recognize persistent instructions.</li>
</ul>
<p><strong>Demo</strong></p>
<p>Set the system message to:</p>
<p>You must always respond in a single sentence.</p>
<p>Now ask the following:</p>
<p>Tell me why Emacs is so great!</p>
<p>Tell me about black holes</p>
<p>clear the system message and ask again, the responses should now be more verbose!!</p>
<figure><img src="https://emacs.dyerdwelling.family/ox-hugo/ollama-buddy-screen-recording_015.gif" width="100%">
</figure>

<h2 id="0-dot-7-dot-4"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-13 Thu&gt; </span></span> <strong>0.7.4</strong></h2>
<p>Added model info command, update keybindings</p>
<ul>
<li>Added `ollama-buddy-show-raw-model-info` to fetch and display raw JSON details
of the current model in the chat buffer.</li>
<li>Updated keybindings:
<ul>
<li>`C-c i` now triggers model info display.</li>
<li>`C-c h` mapped to help assistant.</li>
<li>Improved shortcut descriptions in quick tips section.</li>
</ul>
</li>
<li>Removed unused help assistant entry from menu.</li>
<li>Changed minibuffer-prompt key from `?i` to `?b`.</li>
</ul>
<h2 id="0-dot-7-dot-3"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-12 Wed&gt; </span></span> <strong>0.7.3</strong></h2>
<p>Added function to associate models with menu commands</p>
<ul>
<li>Added <code>ollama-buddy-add-model-to-menu-entry</code> autoload function</li>
<li>Enabled dynamic modification of command-model associations</li>
</ul>
<p>This is a helper function that allows you to associate specific models with individual menu commands.</p>
<p>Configuration to apply a model to a menu entry is now straightforward, in your Emacs init file, add something like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-elisp" data-lang="elisp"><span style="display:flex;"><span>(with-eval-after-load <span style="color:#e6db74">&#39;ollama-buddy</span>
</span></span><span style="display:flex;"><span>  (ollama-buddy-add-model-to-menu-entry <span style="color:#e6db74">&#39;dictionary-lookup</span> <span style="color:#e6db74">&#34;tinyllama:latest&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-add-model-to-menu-entry <span style="color:#e6db74">&#39;synonym</span> <span style="color:#e6db74">&#34;tinyllama:latest&#34;</span>))
</span></span></code></pre></div><p>This configures simpler tasks like dictionary lookups and synonym searches to use the more efficient TinyLlama model, while your default model will still be used for more complex operations.</p>
<h2 id="0-dot-7-dot-2"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-12 Wed&gt; </span></span> <strong>0.7.2</strong></h2>
<p>Added menu model colours back in and removed some redundant code</p>