Anthropic <a href="https://www.anthropic.com/news/updates-to-our-consumer-terms">announced today</a> that it is changing its Consumer Terms and Privacy Policy, with plans to train its AI chatbot Claude with user data.<br/>
<br/>
<img src="https://images.macrumors.com/article-new/2025/08/anthropic-data-collection.jpg" alt="" width="2000" height="1125" class="aligncenter size-full wp-image-1016178" /><br/>
New users will be able to opt out at signup. Existing users will receive a popup that allows them to opt out of Anthropic using their data for AI training purposes.<br/>
<br/>
The popup is labeled "Updates to Consumer Terms and Policies," and when it shows up, unchecking the "You can help improve Claude" toggle will disallow the use of chats. Choosing to accept the policy now will allow all new or resumed chats to be used by Anthropic. Users will need to opt in or opt out by September 28, 2025, to continue using Claude.<br/>
<br/>
Opting out can also be done by going to Claude's Settings, selecting the Privacy option, and toggling off "Help improve Claude."<br/>
<br/>
Anthropic says that the new training policy will allow it to deliver "even more capable, useful AI models" and strengthen safeguards against harmful usage like scams and abuse. The updated terms apply to all users on Claude Free, Pro, and Max plans, but not to services under commercial terms like Claude for Work or Claude for Education.<br/>
<br/>
In addition to using chat transcripts to train Claude, Anthropic is extending data retention to five years. So if you opt in to allowing Claude to be trained with your data, Anthropic will keep your information for a five year period. Deleted conversations will not be used for future model training, and for those that do not opt in to sharing data for training, Anthropic will continue keeping information for 30 days as it does now.<br/>
<br/>
Anthropic says that a "combination of tools and automated processes" will be used to filter sensitive data, with no information provided to third-parties.<br/>
<br/>
Prior to today, Anthropic did not use conversations and data from users to train or improve Claude, unless users submitted feedback.<div class="linkback">Tag: <a href="https://www.macrumors.com/guide/anthropic/">Anthropic</a></div><br/>This article, &quot;<a href="https://www.macrumors.com/2025/08/28/anthropic-claude-chat-training/">Anthropic Will Now Train Claude on Your Chats, Here&#039;s How to Opt Out</a>&quot; first appeared on <a href="https://www.macrumors.com">MacRumors.com</a><br/><br/><a href="https://forums.macrumors.com/threads/anthropic-will-now-train-claude-on-your-chats-heres-how-to-opt-out.2464202/">Discuss this article</a> in our forums<br/><br/>