<p>Philosopher-turned-software developer <a href="https://brianrabern.net/" target="_blank" rel="noopener">Brian Rabern</a> is conducting a study comparing human and artificial intelligence (AI) performance on logic problems, and he is looking for student volunteers to take a short online survey.<span id="more-54901"></span></p>
<div id="attachment_54917" style="width: 2121px" class="wp-caption alignnone"><a href="https://cubes-revisited.art/" target="_blank" rel="noopener"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-54917" class="size-full wp-image-54917" src="https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited.png" alt="" width="2111" height="1064" srcset="https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited.png 2111w, https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited-300x151.png 300w, https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited-1024x516.png 1024w, https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited-768x387.png 768w, https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited-1536x774.png 1536w, https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited-2048x1032.png 2048w, https://dailynous.com/wp-content/uploads/2025/09/weychert-variations-on-lewitts-open-cubes-edited-400x202.png 400w" sizes="auto, (max-width: 2111px) 100vw, 2111px" /></a><p id="caption-attachment-54917" class="wp-caption-text">[Edited version of Rob Weychert, &#8220;So LeWitt&#8217;s Incomplete Open Cubes Revisited (7-part variations, detail)&#8221;. Click image for more.]</p></div>He writes:</p>
<p style="padding-left: 40px;"><em>I had seen a lot of LLM benchmarks on &#8220;logic&#8221;. And as a former logic professor I was interested in what they were testing. Much of it was familiar but nothing like an exam that I would give in an intro logic class. And much of it was just propositional logic. Of course, most of these benchmarks are designed by computer scientists, and not philosophers or logicians, per se. So I wanted to design a logic benchmark that was more familiar to what I think of as a logic exam. That was the initial motivation.</em></p>
<p>The project has now evolved into a study he and a computer science student are conducting to see how good humans and AIs are at logic.</p>
<p>To instructors, he says:</p>
<p style="padding-left: 40px;"><em>If you teach logic (especially intro), and think some of your students might be interested, please get in touch (<a href="mailto:brian.rabern@gmail.com" target="_blank" rel="noopener">brian.rabern@gmail.com</a>). I can’t offer compensation or real incentive beyond the science itself, but I will certainly acknowledge faculty who help with recruitment in the final paper.</em></p>
<p>Here&#8217;s some information about what the students will be asked to do:</p>
<p style="padding-left: 40px;"><em>Students will be emailed a google form with 3 logic tasks:</em></p>
<ul>
<li style="list-style-type: none;">
<ul>
<li><em>Symbolization: translate a natural-language sentence into logical form. </em></li>
<li><em>Countermodel: given an invalid argument in formal notation, provide a countermodel. </em></li>
<li><em>Validity: read a natural-language scenario and decide which of several sentences logically follow. </em></li>
</ul>
</li>
</ul>
<p style="padding-left: 40px;"><em>The form is anonymous but has a few demographic questions.</em></p>
<p>Here&#8217;s an example:</p>
<p><img loading="lazy" decoding="async" class="wp-image-54916 aligncenter" src="https://dailynous.com/wp-content/uploads/2025/09/ex.jpg" alt="" width="808" height="500" srcset="https://dailynous.com/wp-content/uploads/2025/09/ex.jpg 2048w, https://dailynous.com/wp-content/uploads/2025/09/ex-300x186.jpg 300w, https://dailynous.com/wp-content/uploads/2025/09/ex-1024x634.jpg 1024w, https://dailynous.com/wp-content/uploads/2025/09/ex-768x476.jpg 768w, https://dailynous.com/wp-content/uploads/2025/09/ex-1536x951.jpg 1536w, https://dailynous.com/wp-content/uploads/2025/09/ex-400x248.jpg 400w" sizes="auto, (max-width: 808px) 100vw, 808px" /></p>
<p>And here&#8217;s some background and further information from Rabern:</p>
<p style="padding-left: 40px;"><em>I’ve built a system (in Python) that compositionally generates grammatical natural language sentences, each with an abstract syntax tree and a formal logical representation (in two-variable FOL without identity). There are actually two lexicons: a fragment of English and a Carroll-style nonsense lexicon. I have a database of 100k+ sentences (e.g., “A human chased a donkey that chased it.”), each paired with a Carrollian counterpart (&#8220;A borogove snicker-snacked a tove that snicker-snacked it&#8221;) and linked to the same underlying logical representation (&#8220;∃x∃y((Nx∧My)∧(Rxy∧Ryx))&#8221;). The Carrollian language is meant to isolate logical competence from world knowledge and linguistic intuition &#8212; or at least test whether that distinction matters. This controlled fragment enables automated evaluation using SMT solvers (e.g., Z3; thanks Putnam!). The system then can be used to explore the space of sentences, find valid arguments, and record plausible but invalid distractors.</em></p>
<p style="padding-left: 40px;"><em>Preliminary benchmarking shows that LLMs struggle with countermodels (which you might have expected), do best on the validity tasks, and perform better at symbolization in English than in Carroll. One natural hypothesis is that the models exploit heuristics on validity tasks rather than deploying the subskills humans typically use (symbolization + proof/model-theoretic methods) &#8212; paralleling recent work on how LLMs handle arithmetic. So we think it&#8217;d be interesting to get some human data for comparison.</em></p>
<p style="padding-left: 40px;"><em>Students who have used the Kalish &amp; Montague or Parsons texts will find the tasks familiar, though none of the exercises involve proof systems. So any students from an intro logic course that covers symbolization and countermodels would be good candidates.</em></p>
<p><a href="mailto:brian.rabern@gmail.com">Email Rabern</a> for more info.</p>
<div></div><p>The post <a href="https://dailynous.com/2025/09/09/human-vs-ai-in-logic-tasks-a-study/">Human vs. AI in Logic Tasks: a Study</a> first appeared on <a href="https://dailynous.com">Daily Nous</a>.</p>