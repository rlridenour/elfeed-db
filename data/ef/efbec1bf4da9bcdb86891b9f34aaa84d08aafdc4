<p>This week in <a href="https://github.com/captainflasmr/ollama-buddy">ollama-buddy</a> updates, I have mostly been experimenting with ChatGPT integration! Yes, it is not a local LLM, so not ollama, hence entirely subverting the whole notion and fundamental principles of this package! This I know, and I don&rsquo;t care; I&rsquo;m having fun. I use ChatGPT and would rather use it in Emacs through the now-familiar <code>ollama-buddy</code> framework, so why not? I&rsquo;m also working on Claude integration too.</p>
<p>My original principles of a no-config Emacs ollama integration still hold true, as by default, you will only see ollama models available. But with a little tweak to the configuration, with a require here and an API key there, you can now enable communication with an online AI. At the moment, I use Claude and ChatGPT, but if I can get Claude working, I might think about just adding in a basic template framework to easily slot in others. At the moment, there is a little too much internal ollama-buddy faffing to incorporate these external AIs into the main package, but I&rsquo;m sure I can figure out a way to accommodate separate elisp external AIs.</p>
<figure><img src="https://emacs.dyerdwelling.family/ox-hugo/20250325093201-emacs--Ollama-Buddy-0-9-11-Experimental-ChatGPT-Integration-Customizable-AI-Streaming-and-Texinfo-documentation.jpg" width="100%">
</figure>

<p>In other <code>ollama-buddy</code> news, I have now added support for the <code>stream</code> variable in the ollama API. By default, I had streaming on, and I guess why wouldn&rsquo;t you? It is a chat, and you would want to see &ldquo;typing&rdquo; or tokens arriving as they come in?. But to support more of the API, you can toggle it on and off, so if you want, you can sit there and wait for the response to arrive in one go and maybe it can be less distracting (and possibly more efficient?).</p>
<p>Just a note back on the topic of online AI offerings: to simplify those integrations, I just disabled streaming for the response to arrive in one shot. Mainly, I just couldn&rsquo;t figure out the ChatGPT streaming, and for an external offering, I wasn&rsquo;t quite willing to spend more time on it, and due to the speed of these online behemoths, do you really need to see each token come in as it arrives?</p>
<p>Oh, there is something else too, something I have been itching to do for a while now, and that is to write a Texinfo document so a manual can be viewed in Emacs. Of course, this being an AI-based package, I fed in my <code>ollama-buddy</code> files and got Claude to generate one for me (I have a baby and haven&rsquo;t the time!). Reading through it, I think it turned out pretty well :) It hasn&rsquo;t been made automatically available on MELPA yet, as I need to tweak the recipe, but you can install it for yourself.</p>
<p>Anyways, see below for the changelog gubbins:</p>
<h2 id="0-dot-9-dot-11"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-24 Mon&gt; </span></span> <strong>0.9.11</strong></h2>
<p>Added the ability to toggle streaming on and off</p>
<ul>
<li>Added customization option to enable/disable streaming mode</li>
<li>Implemented toggle function with keybindings (C-c x) and transient menu option</li>
<li>Added streaming status indicator in the modeline</li>
</ul>
<p>The latest update introduces the ability to toggle between two response modes:</p>
<ul>
<li><strong>Streaming mode (default)</strong>: Responses appear token by token in real-time, giving you immediate feedback as the AI generates content.</li>
<li><strong>Non-streaming mode</strong>: Responses only appear after they&rsquo;re fully generated, showing a &ldquo;Loading response&hellip;&rdquo; placeholder in the meantime.</li>
</ul>
<p>While watching AI responses stream in real-time is often helpful, there are situations where you might prefer to see the complete response at once:</p>
<ul>
<li>When working on large displays where the cursor jumping around during streaming is distracting</li>
<li>When you want to focus on your work without the distraction of incoming tokens until the full response is ready</li>
</ul>
<p>The streaming toggle can be accessed in several ways:</p>
<ol>
<li>Use the keyboard shortcut <code>C-c x</code></li>
<li>Press <code>x</code> in the transient menu</li>
<li>Set the default behaviour through customization:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-elisp" data-lang="elisp"><span style="display:flex;"><span>      (setq ollama-buddy-streaming-enabled <span style="color:#66d9ef">nil</span>) <span style="color:#75715e">;; Disable streaming by default</span>
</span></span></code></pre></div></li>
</ol>
<p>The current streaming status is visible in the modeline indicator, where an &ldquo;X&rdquo; appears when streaming is disabled.</p>
<h2 id="0-dot-9-dot-10"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-22 Sat&gt; </span></span> <strong>0.9.10</strong></h2>
<p>Added experimental OpenAI support!</p>
<p>Yes, that&rsquo;s right, I said I never would do it, and of course, this package is still very much <code>ollama</code>-centric, but I thought I would just sneak in some rudimentary ChatGPT support, just for fun!</p>
<p>It is a very simple implementation, I haven&rsquo;t managed to get streaming working, so Emacs will just show &ldquo;Loading Response&hellip;&rdquo; as it waits for the response to arrive. It is asynchronous, however, so you can go off on your Emacs day while it loads (although being ChatGPT, you would think the response would be quite fast!)</p>
<p>By default, OpenAI/ChatGPT will not be enabled, so anyone wanting to use just a local LLM through <code>ollama</code> can continue as before. However, you can now sneak in some experimental ChatGPT support by adding the following to your Emacs config as part of the <code>ollama-buddy</code> set up.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-elisp" data-lang="elisp"><span style="display:flex;"><span>(require <span style="color:#e6db74">&#39;ollama-buddy-openai</span> <span style="color:#66d9ef">nil</span> <span style="color:#66d9ef">t</span>)
</span></span><span style="display:flex;"><span>(setq ollama-buddy-openai-api-key <span style="color:#e6db74">&#34;&lt;big long key&gt;&#34;</span>)
</span></span></code></pre></div><p>and you can set the default model to ChatGPT too!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-elisp" data-lang="elisp"><span style="display:flex;"><span>(setq ollama-buddy-default-model <span style="color:#e6db74">&#34;GPT gpt-4o&#34;</span>)
</span></span></code></pre></div><p>With this enabled, chat will present a list of ChatGPT models to choose from. The custom menu should also now work with chat, so from anywhere in Emacs, you can push predefined prompts to the <code>ollama</code> buddy chat buffer now supporting ChatGPT.</p>
<p>There is more integration required to fully incorporate ChatGPT into the <code>ollama</code> buddy system, like token rates and history, etc. But not bad for a first effort, methinks!</p>
<p>Here is my current config, now mixing ChatGPT with <code>ollama</code> models:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-elisp" data-lang="elisp"><span style="display:flex;"><span>(use-package ollama-buddy
</span></span><span style="display:flex;"><span>  :bind
</span></span><span style="display:flex;"><span>  (<span style="color:#e6db74">&#34;C-c o&#34;</span> <span style="color:#f92672">.</span> ollama-buddy-menu)
</span></span><span style="display:flex;"><span>  (<span style="color:#e6db74">&#34;C-c O&#34;</span> <span style="color:#f92672">.</span> ollama-buddy-transient-menu-wrapper)
</span></span><span style="display:flex;"><span>  :custom
</span></span><span style="display:flex;"><span>  (ollama-buddy-openai-api-key <span style="color:#e6db74">&#34;&lt;very long key&gt;&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-default-model <span style="color:#e6db74">&#34;GPT gpt-4o&#34;</span>)
</span></span><span style="display:flex;"><span>  :config
</span></span><span style="display:flex;"><span>  (require <span style="color:#e6db74">&#39;ollama-buddy-openai</span> <span style="color:#66d9ef">nil</span> <span style="color:#66d9ef">t</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;refactor-code</span> :model <span style="color:#e6db74">&#34;qwen2.5-coder:7b&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;git-commit</span> :model <span style="color:#e6db74">&#34;qwen2.5-coder:3b&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;describe-code</span> :model <span style="color:#e6db74">&#34;qwen2.5-coder:3b&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;dictionary-lookup</span> :model <span style="color:#e6db74">&#34;llama3.2:3b&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;synonym</span> :model <span style="color:#e6db74">&#34;llama3.2:3b&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;proofread</span> :model <span style="color:#e6db74">&#34;GPT gpt-4o&#34;</span>)
</span></span><span style="display:flex;"><span>  (ollama-buddy-update-menu-entry
</span></span><span style="display:flex;"><span>   <span style="color:#e6db74">&#39;custom-prompt</span> :model <span style="color:#e6db74">&#34;deepseek-r1:7b&#34;</span>))
</span></span></code></pre></div><h2 id="0-dot-9-dot-9"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-22 Sat&gt; </span></span> <strong>0.9.9</strong></h2>
<p>Added texinfo documentation for future automatic installation through MELPA and created an Emacs manual.</p>
<p>If you want to see what the manual would look like, just download the docs directory from github, cd into it, and run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>make
</span></span><span style="display:flex;"><span>sudo make install-docs
</span></span></code></pre></div><p>Then calling up <code>info</code> <code>C-h i</code> and ollama buddy will be present in the Emacs menu, or just select <code>m</code> and search for <code>Ollama Buddy</code></p>
<p>For those interested in the manual, I have converted it into html format, which is accessible here:</p>
<p><a href="https://emacs.dyerdwelling.family/tags/ollama-buddy/">/tags/ollama-buddy/</a></p>
<p>It has been converted using the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>makeinfo --html --no-split ollama-buddy.texi -o ollama-buddy.html
</span></span><span style="display:flex;"><span>pandoc -f html -t org -o ollama-buddy.org ollama-buddy.html
</span></span></code></pre></div><h2 id="0-dot-9-dot-9"><span class="timestamp-wrapper"><span class="timestamp">&lt;2025-03-20 Thu&gt; </span></span> <strong>0.9.9</strong></h2>
<p>Intro message with model management options (select, pull, delete) and option for recommended models to pull</p>
<ul>
<li>Enhance model management and selection features</li>
<li>Display models available for download but not yet pulled</li>
</ul>