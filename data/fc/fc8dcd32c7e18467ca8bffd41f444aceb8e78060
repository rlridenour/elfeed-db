
<p>OpenAI:</p>

<blockquote>
  <p>We’re releasing gpt-oss-120b and gpt-oss-20b — two
state-of-the-art open-weight language models that deliver strong
real-world performance at low cost. Available under the flexible
Apache 2.0 license, these models outperform similarly sized open
models on reasoning tasks, demonstrate strong tool use
capabilities, and are optimized for efficient deployment on
consumer hardware. They were trained using a mix of reinforcement
learning and techniques informed by OpenAI’s most advanced
internal models, including o3 and other frontier systems.</p>

<p>The gpt-oss-120b model achieves near-parity with OpenAI o4-mini on
core reasoning benchmarks, while running efficiently on a single
80 GB GPU. The gpt-oss-20b model delivers similar results to
OpenAI o3‑mini on common benchmarks and can run on edge devices
with just 16 GB of memory, making it ideal for on-device use
cases, local inference, or rapid iteration without costly
infrastructure. Both models also perform strongly on tool use,
few-shot function calling, CoT reasoning (as seen in results on
the Tau-Bench agentic evaluation suite) and HealthBench (even
outperforming proprietary models like OpenAI o1 and GPT‑4o).</p>
</blockquote>

<p><a href="https://simonwillison.net/2025/Aug/5/gpt-oss/">Simon Willison</a>:</p>

<blockquote>
  <p>The long promised <a href="https://simonwillison.net/2025/Aug/5/gpt-oss/">OpenAI open weight models are here</a>, and
they are <em>very</em> impressive. [...]</p>

<p>o4-mini and o3-mini are <em>really good</em> proprietary models — I was
not expecting the open weights releases to be anywhere near that
class, especially given their small sizes. That gpt-oss-20b model
should run quite comfortably on a Mac laptop with 32GB of RAM.</p>
</blockquote>

<div>
<a  title="Permanent link to ‘OpenAI: ‘Introducing gpt-oss’’"  href="https://daringfireball.net/linked/2025/08/06/gpt-oss">&nbsp;★&nbsp;</a>
</div>

	