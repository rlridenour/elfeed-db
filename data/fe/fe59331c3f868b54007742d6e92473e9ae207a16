<p id="p1"><a href="https://www.bloomberg.com/news/articles/2025-11-05/apple-plans-to-use-1-2-trillion-parameter-google-gemini-model-to-power-new-siri" rel="noopener noreferrer">Quite the scoop from Mark Gurman</a> yesterday on what Apple is planning for major Siri improvements in 2026:</p>
<blockquote id="blockquote2"><p>
  Apple Inc. is planning to pay about $1 billion a year for an ultrapowerful 1.2 trillion parameter artificial intelligence model developed by Alphabet Inc.&rsquo;s Google that would help run its long-promised overhaul of the Siri voice assistant, according to people with knowledge of the matter.
</p></blockquote>
<p id="p3">There is a lot to unpack here and I have <em>a lot</em> of questions.</p>
<p id="p4"><!--more--></p>
<p id="p5">First, let&rsquo;s backtrack a little. Gurman <a href="https://www.bloomberg.com/news/articles/2025-06-12/apple-targets-spring-2026-for-release-of-delayed-siri-ai-upgrade" rel="noopener noreferrer">previously reported</a> that Apple was targeting iOS 26.4 in spring 2026 for the delayed Siri features <a href="https://www.macstories.net/stories/notes-on-the-apple-intelligence-delay/" rel="noopener noreferrer">from earlier this year</a>, which matches Apple&rsquo;s promise of &ldquo;<a href="https://www.macrumors.com/2025/10/30/tim-cook-say-siri-revamp-on-track-for-next-year/" rel="noopener noreferrer">next year</a>&rdquo; as well. Those features were about on-screen awareness, in-app actions, and personal context based on App Intents, which Apple announced at WWDC 2024 and never shipped (in the meantime, third-party developers have been updating their apps with <a href="https://developer.apple.com/documentation/appintents" rel="noopener noreferrer">App Intents</a> in preparation for Apple Intelligence).</p>
<p id="p6">In September, however, <a href="https://www.bloomberg.com/news/articles/2025-09-03/apple-plans-ai-search-engine-for-siri-to-rival-openai-google-siri-talks-advance" rel="noopener noreferrer">Gurman also reported</a> that Apple was looking to launch a &ldquo;world knowledge&rdquo; version of Siri infused with web search capabilities to &ldquo;rival OpenAI and Perplexity&rdquo;. He wrote:</p>
<blockquote id="blockquote7"><p>
  The company is working on a new system &ndash; dubbed internally as World Knowledge Answers &ndash; that will be integrated into the Siri voice assistant, according to people with knowledge of the matter. Apple has discussed also eventually adding the technology to its Safari web browser and Spotlight, which is used to search from the iPhone home screen.
</p></blockquote>
<p id="p8">And:</p>
<blockquote id="blockquote9"><p>
  Apple&rsquo;s new search experience will include an interface that makes use of text, photos, video and local points of interest, according to the people. It also will offer an AI-powered summarization system designed to make results more quickly digestible and more accurate than what&rsquo;s offered by the current Siri.
</p></blockquote>
<p id="p10">In that September report, Gurman also said that those features were on track for a spring 2026 launch, therefore joining the other previously delayed Apple Intelligence features in the rumored Siri overhaul.</p>
<p id="p11">Back to yesterday&rsquo;s report:</p>
<blockquote id="blockquote12"><p>
  Under the arrangement, Google&rsquo;s Gemini model will handle Siri&rsquo;s summarizer and planner functions &ndash; the components that help the voice assistant synthesize information and decide how to execute complex tasks. Some Siri features will continue to use Apple&rsquo;s in-house models.
</p></blockquote>
<p id="p13">I have no idea what Gurman means by &ldquo;planner&rdquo; and &ldquo;complex tasks&rdquo;. Surely we&rsquo;re not looking at a reasoning model that thinks longer after a user query to plan each step of an answer, right? Apple appears to be testing a traditional &ldquo;LLM Siri&rdquo; chatbot app, but <a href="https://www.bloomberg.com/news/articles/2025-09-26/apple-builds-a-chatgpt-like-app-to-help-test-the-revamped-siri" rel="noopener noreferrer">it sounds like</a> it&rsquo;s for internal testing only and won&rsquo;t be released anytime soon. That is, in my opinion, a mistake: people clearly like chatbots, especially the ones that work.</p>
<p id="p14">Let me try and make sense of everything we&rsquo;ve heard about this rumored Siri overhaul and Google partnership. For starters, this custom Gemini model hosted on <a href="https://security.apple.com/blog/private-cloud-compute/" rel="noopener noreferrer">Private Cloud Compute</a> is not going to have 1.2T parameters activated all at once; just like the <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf" rel="noopener noreferrer">Gemini 2.5 family of models</a>, Google and Apple have likely implemented a <a href="https://huggingface.co/blog/moe" rel="noopener noreferrer">mixture-of-experts architecture</a> that activates only a subset of parameters to guarantee faster performance during inference. This is the direction <a href="https://www.cerebras.ai/blog/moe-guide-why-moe" rel="noopener noreferrer">the industry has been going</a> for the past year, including some fascinating research by Perplexity on running large models with MoE that <a href="https://research.perplexity.ai/articles/enabling-trillion-parameter-models-on-aws-efa" rel="noopener noreferrer">was published this week</a>. For context, <a href="https://moonshotai.github.io/Kimi-K2/" rel="noopener noreferrer">Kimi K2</a> (the model I recently <a href="https://www.macstories.net/notes/ai-experiments-fast-inference-with-groq-and-third-party-tools-with-kimi-k2-in-typingmind/" rel="noopener noreferrer">covered here</a>) is a 1T parameter model with <a href="https://console.groq.com/docs/model/moonshotai/kimi-k2-instruct" rel="noopener noreferrer">384 experts</a>; only 32B parameters are activated at inference, with 8 experts selected per token. Put simply: without a MoE architecture, this Apple/Google model would be too costly for Apple to run at scale, and it would be slow for users.</p>
<p id="p15">That said, a 1.2T model is a <em>big</em> model, which squares nicely with the idea that Apple is building an &ldquo;answer engine&rdquo; inside Siri. At that size, the model would have a lot of knowledge built-in, and since it&rsquo;d be Gemini, it would be able to reliably call Google search to look up more specific questions or gather additional context via web search. The picture is a little more clear then: Apple found a partner &ndash; with whom <a href="https://www.macstories.net/news/googles-antitrust-loss-why-apple-doesnt-just-build-a-search-engine-and-what-comes-next/" rel="noopener noreferrer">they have an existing deal</a> &ndash; that can provide a large, custom model with state-of-the-art performance, pre-trained knowledge, and direct integration with web search. The last item is key here, I think. If Apple wanted to build an answer engine, Anthropic couldn&rsquo;t be a candidate since they don&rsquo;t have their own search index (they <a href="https://simonwillison.net/2025/Mar/21/anthropic-use-brave/" rel="noopener noreferrer">rely on Brave Search</a> instead); Perplexity <a href="https://www.perplexity.ai/hub/blog/introducing-the-perplexity-search-api" rel="noopener noreferrer">has one</a>, but the company doesn&rsquo;t train models; Mistral is like a worse version of Claude, and doesn&rsquo;t have its own search index either. A Gemini model, without the Gemini brand, at 1.2 trillion parameters, running on Private Cloud Compute and under Apple&rsquo;s control at $1 billion/year sounds like a <em>crazy good</em> deal to me. The one that a certain <a href="https://www.ferrari.com/en-EN/corporate/board-directors" rel="noopener noreferrer">Ferrari board member</a> would make.</p>
<p id="p16">But back to the model: which one is it? Gemini 3.0 is launching (<a href="https://x.com/legit_api/status/1986066955191665043?s=46" rel="noopener noreferrer">very</a>?) soon, and one would assume that, given the spring 2026 timing, a &ldquo;Gemini 3.0 Flash&rdquo; would exist by then. I could also see a scenario in which we&rsquo;ll never officially know which version of Gemini this model is based on, <em>if</em> we&rsquo;ll hear anything from Apple about this partnership in the first place. But let&rsquo;s assume that it&rsquo;s going to be Gemini 3.0 Flash, with the ability to provide answers, look up web results, summarize them, and support multiple modalities with text and images: what about the other delayed Siri features? Is Gemini going to be the model that will steer Siri toward one App Intent instead of another when trying to make sense of different <a href="https://developer.apple.com/documentation/appintents/app-intent-domains" rel="noopener noreferrer">app domains</a> in the semantic index that Apple announced in 2024? Is that architecture still in place? At the time, Apple&rsquo;s vague wording suggested that a mix of local and cloud models would be in charge of parsing an on-device semantic index, depending on the complexity of the task. Is that what Gurman&rsquo;s &ldquo;planner function&rdquo; is about?</p>
<p id="p17">I&rsquo;d also point out another advantage for implementing Gemini in Private Cloud Compute: it&rsquo;s natively multimodal, and it&rsquo;s very good at dealing with image recognition, OCR, and audio transcription (which I <a href="https://www.macstories.net/mac/llm-youtube-transcripts-with-claude-and-gemini-in-shortcuts/" rel="noopener noreferrer">started using</a> months ago, and I recently implemented as a <a href="https://claude.com/blog/skills" rel="noopener noreferrer">skill in Claude</a>). The vision capabilities of Gemini could probably come in handy for the on-screen awareness features that Siri was supposed to receive before Apple delayed them. Keep in mind that whenever you use Visual Intelligence and ask a question, the image <em>has</em> to go through ChatGPT; a Gemini-powered Siri would be able to answer that without the ChatGPT extension.</p>
<p id="p18">Lastly, here&rsquo;s a fascinating tidbit from the story: according to Gurman, the Apple Intelligence Foundation model running in the cloud today is a 150B one.</p>
<blockquote id="blockquote19"><p>
  The custom Gemini system represents a major advance from the 150 billion parameter model used today for the cloud-based version of Apple Intelligence. The move would vastly expand the system&rsquo;s power and its ability to process complex data and understand context.
</p></blockquote>
<p id="p20">I&rsquo;ve <a href="https://www.macstories.net/notes/i-have-many-questions-about-apples-updated-foundation-models-and-the-great-use-model-action-in-shortcuts/" rel="noopener noreferrer">long wondered</a> about the size of that model, and it&rsquo;s nice to hear a number finally, albeit an unofficial one.</p>
<p id="p21">If everything Gurman has reported so far is accurate &ndash; and I have no reason to believe it isn&rsquo;t &ndash; I still have many questions about the details of this future Siri, but at least I think I get what Apple is preparing. In iOS 26.4, Apple may unveil a much smarter Siri, powered by Gemini behind the scenes, that can answer a lot of questions on its own, search the web, summarize text and understand images, and maybe even power the long-delayed app integrations of Apple Intelligence. If these rumors are correct, it sounds like <a href="https://www.bloomberg.com/news/articles/2025-03-20/apple-vision-pro-chief-mike-rockwell-named-siri-head-giannandrea-keeps-ai-role" rel="noopener noreferrer">Siri&rsquo;s new leadership</a> is making all the right moves.</p>
